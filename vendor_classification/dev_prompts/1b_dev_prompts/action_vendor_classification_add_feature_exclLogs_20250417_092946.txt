<goal or issue to address>

Okay, let's refine the user experience for reclassification based on your ideas, aiming for clarity and simplicity without overengineering.
Here's a breakdown of an improved workflow and the necessary considerations:
Core Idea: Treat reclassification as a special type of job (REVIEW job) that takes input from a previous CLASSIFICATION job and user hints. The results of this REVIEW job can then be optionally merged back into the original CLASSIFICATION job's results.
Refined Workflow & UI/UX:
Initiating Reclassification (From Original Job Results):
On the JobStatus screen for a completed CLASSIFICATION job, the JobResultsTable (or ReviewResultsTable if viewing an already merged job) will show the results.
Add a "Flag for Review" button/checkbox to each row (as implemented in JobResultsTable/ReviewResultsTable).
When items are flagged, a "Submit Flags for Re-classification" button appears (as implemented).
Clicking this button triggers the POST /{original_job_id}/reclassify endpoint, which creates a new Job record with job_type = REVIEW and queues the reclassify_flagged_vendors_task.
UX: Provide immediate feedback that the review job has started (e.g., a success message with the new review job ID, potentially redirecting the user to the new job's status page).
Viewing Reclassification Results (Review Job Status Screen):
The user navigates to the status page for the REVIEW job (e.g., via Job History or the link provided after submission).
The JobStatus component displays the details of this REVIEW job.
When the REVIEW job status is COMPLETED:
The ReviewResultsTable component is displayed.
This table should clearly show:
Vendor Name
The Hint provided by the user for that vendor.
The Original Classification details (L1-L5 ID/Name, Status, Confidence, Source). Use distinct styling (e.g., background color like bg-blue-50).
The New Classification details obtained using the hint (L1-L5 ID/Name, Status, Confidence, Source='Review'). Use distinct styling (e.g., background color like bg-green-50).
Simplification: Remove the proposed row-level "Accept" or "Provide New Hints" options here. The user already provided hints to get this result. If they disagree with the new result, they can flag it again from this screen (or the original job screen after merging) to start another review cycle. This avoids nested complexity.
Merging Results:
On the completed REVIEW job's status screen, add a button: "Merge Results into Original Job".
This button should only be visible/enabled when the REVIEW job status is COMPLETED.
Action: Clicking this button triggers a new backend endpoint.
Backend Merge Endpoint:
Endpoint: POST /api/v1/jobs/{review_job_id}/merge
Logic:
Fetch the REVIEW job (review_job_id). Verify it's COMPLETED and job_type == REVIEW.
Get the parent_job_id from the REVIEW job.
Fetch the original CLASSIFICATION job (parent_job_id). Verify it exists.
Load the detailed_results from both jobs (List[ReviewResultItem] for review, List[JobResultItem] for original). Handle potential missing results gracefully.
Create a map of the original job's results keyed by vendor_name for efficient updates.
Iterate through the REVIEW job's results (List[ReviewResultItem]).
For each review_item, get the vendor_name and the new_result (which is a dict matching JobResultItem).
If the vendor_name exists in the original results map, replace the entry in the map with the new_result dict. Ensure the classification_source in the new_result is appropriately marked (e.g., 'Review').
Convert the updated map back into a List[JobResultItem].
Update the original CLASSIFICATION job's detailed_results field with this new list.
Crucially: Trigger the regeneration of the downloadable Excel file for the original CLASSIFICATION job using the updated detailed_results. Update the output_file_name on the original job record.
Add a flag/timestamp to the REVIEW job's stats indicating it has been merged (e.g., "merged_at": datetime.now().isoformat()) to prevent accidental double-merging and inform the UI.
Commit the changes to both job records (original job's results/output file, review job's stats).
Response: Success message, maybe the ID of the updated original job.
Downloading Results:
The primary download action should always be associated with the original CLASSIFICATION job (GET /api/v1/jobs/{original_job_id}/download).
After the merge operation, downloading the results for the original job will automatically provide the spreadsheet with the updated, reclassified data, matching the original column structure (JobResultItem schema).
The REVIEW job itself does not need a download button, as its purpose is intermediary.
</goal or issue to address>


<output instruction>
1) Explain if this is already complete, or what is missing
2) list all the files that need to be created or changed. For each file, describe what needs to be done.

   Use the format: <file path='relative/path/to/file.ext'>
```[language]
[EXPLANATION OF WHAT NEEDS TO CHANGE ]
```
</file>
 do NOT give us code yet, only description of action to take
</output instruction>


<Tree of Included Files>
- app/api/jobs.py
- app/models/job.py
- app/schemas/job.py
- app/schemas/review.py
- app/services/file_service.py
- app/tasks/classification_tasks.py
- app/tasks/reclassification_logic.py
- app/tasks/reclassification_prompts.py
- frontend/vue_frontend/src/components/JobHistory.vue
- frontend/vue_frontend/src/components/JobResultsTable.vue
- frontend/vue_frontend/src/components/JobStatus.vue
- frontend/vue_frontend/src/components/ReviewResultsTable.vue
- frontend/vue_frontend/src/services/api.ts
- frontend/vue_frontend/src/stores/job.ts
</Tree of Included Files>


<Concatenated Source Code>

<file path='app/api/jobs.py'>

# <file path='app/api/jobs.py'>
# app/api/jobs.py
from fastapi import APIRouter, Depends, HTTPException, Query, status, Path # Added Path
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Union # <<< ADDED Union
from datetime import datetime
import logging # Import logging
import uuid # <<< ADDED for generating review job IDs
import os # <<< ADDED for path joining

from core.database import get_db
from api.auth import get_current_user
from models.user import User
# --- CORRECTED IMPORT: Add ProcessingStage ---
from models.job import Job, JobStatus, JobType, ProcessingStage
# --- END CORRECTED IMPORT ---
# --- UPDATED: Import specific schemas ---
from schemas.job import JobResponse, JobResultItem, JobResultsResponse
from schemas.review import ReclassifyPayload, ReclassifyResponse, ReviewResultItem # <<< ADDED Review Schemas
# --- END UPDATED ---
from core.logging_config import get_logger
from core.log_context import set_log_context
from core.config import settings # Need settings for file path construction
# --- CORRECTED IMPORT PATH ---
from tasks.classification_tasks import reclassify_flagged_vendors_task
# --- END CORRECTED IMPORT PATH ---


logger = get_logger("vendor_classification.api.jobs")
logger.debug("Successfully imported Dict from typing for jobs API.")

router = APIRouter()

@router.get("/", response_model=List[JobResponse])
async def list_jobs(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    status_filter: Optional[JobStatus] = Query(None, alias="status", description="Filter jobs by status"),
    start_date: Optional[datetime] = Query(None, description="Filter jobs created on or after this date (ISO format)"),
    end_date: Optional[datetime] = Query(None, description="Filter jobs created on or before this date (ISO format)"),
    job_type_filter: Optional[JobType] = Query(None, alias="type", description="Filter jobs by type (CLASSIFICATION or REVIEW)"), # <<< ADDED Filter
    skip: int = Query(0, ge=0, description="Number of jobs to skip for pagination"),
    limit: int = Query(100, ge=1, le=500, description="Maximum number of jobs to return"),
):
    """
    List jobs for the current user. Admins can see all jobs (optional enhancement).
    Supports filtering by status, date range, type, and pagination.
    """
    set_log_context({"username": current_user.username})
    logger.info("Fetching job history", extra={
        "status_filter": status_filter,
        "job_type_filter": job_type_filter, # <<< ADDED Log
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "skip": skip,
        "limit": limit,
    })

    query = db.query(Job)

    # Filter by user (Admins could potentially see all - add logic here if needed)
    # For now, all users only see their own jobs
    # if not current_user.is_superuser: # Example admin check
    query = query.filter(Job.created_by == current_user.username)

    # Apply filters
    if status_filter:
        query = query.filter(Job.status == status_filter.value)
    if job_type_filter: # <<< ADDED Filter
        query = query.filter(Job.job_type == job_type_filter.value)
    if start_date:
        query = query.filter(Job.created_at >= start_date)
    if end_date:
        # Add a day to end_date to make it inclusive of the whole day if time is not specified
        # Or adjust based on desired behavior (e.g., end_date < end_date + timedelta(days=1))
        query = query.filter(Job.created_at <= end_date)

    # Order by creation date (newest first)
    query = query.order_by(Job.created_at.desc())

    # Apply pagination
    jobs = query.offset(skip).limit(limit).all()

    logger.info(f"Retrieved {len(jobs)} jobs from history.")

    # Convert Job models to JobResponse schemas
    # Pydantic v2 handles this automatically with from_attributes=True
    return jobs

@router.get("/{job_id}", response_model=JobResponse)
async def read_job(
    # Use Path to ensure job_id is correctly extracted from the URL path
    job_id: str = Path(..., title="The ID of the job to get"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve details for a specific job by its ID.
    Ensures the current user owns the job (or is an admin - future enhancement).
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching details for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # --- Authorization Check ---
    # Ensure the user requesting the job is the one who created it
    # (Or add admin override logic here if needed)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access this job")
    # --- End Authorization Check ---

    # LOGGING: Log the job details being returned, especially target_level and job_type
    logger.info(f"Returning details for job ID: {job_id}", extra={"job_status": job.status, "target_level": job.target_level, "job_type": job.job_type})
    return job # Pydantic will validate against JobResponse

# Use Dict for flexibility, or create a specific StatsResponse schema later if needed
@router.get("/{job_id}/stats", response_model=Dict)
async def read_job_stats(
    job_id: str = Path(..., title="The ID of the job to get stats for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve processing statistics for a specific job.
    For REVIEW jobs, this might contain the input hints.
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching statistics for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for stats", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check (same as read_job)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access stats for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access stats for this job")

    # LOGGING: Log the raw stats being returned from the database
    logger.info(f"Returning statistics for job ID: {job_id}", extra={"job_type": job.job_type})
    logger.debug(f"Raw stats from DB for job {job_id}: {job.stats}") # Log the actual stats dict

    # The stats are stored as JSON in the Job model
    return job.stats if job.stats else {}


# --- UPDATED: Endpoint for Detailed Results ---
# Now returns JobResultsResponse which includes job_type and Union of result types
@router.get("/{job_id}/results", response_model=JobResultsResponse)
async def read_job_results(
    job_id: str = Path(..., title="The ID of the job to get detailed results for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve the detailed classification results for a specific completed job.
    Returns a structure containing the job_id, job_type, and a list of results
    (either JobResultItem or ReviewResultItem depending on the job_type).
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching detailed results for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for results", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access results for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access results for this job")

    # Check if job is completed and has results
    if job.status != JobStatus.COMPLETED.value:
        logger.warning(f"Detailed results requested but job not completed",
                       extra={"job_id": job_id, "status": job.status, "job_type": job.job_type})
        # Return empty list in the correct response structure
        return JobResultsResponse(job_id=job_id, job_type=JobType(job.job_type), results=[]) # Cast job_type to enum

    if not job.detailed_results:
        logger.warning(f"Job {job_id} is completed but has no detailed results stored.", extra={"job_id": job_id, "job_type": job.job_type})
        return JobResultsResponse(job_id=job_id, job_type=JobType(job.job_type), results=[]) # Cast job_type to enum

    # The detailed_results field should contain a list of dicts matching the expected schema.
    # Pydantic will validate this structure upon return based on the response_model.
    # We trust the background task stored the correct structure based on job_type.
    results_count = len(job.detailed_results)
    logger.info(f"Returning {results_count} detailed result items for job ID: {job_id}", extra={"job_type": job.job_type})

    # Pydantic should automatically validate based on the Union in JobResultsResponse
    return JobResultsResponse(job_id=job_id, job_type=JobType(job.job_type), results=job.detailed_results) # Cast job_type to enum
# --- END UPDATED ---


@router.get("/{job_id}/download")
async def download_job_results(
    job_id: str = Path(..., title="The ID of the job to download results for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Downloads the output Excel file for a completed job.
    Note: Currently only generates Excel for CLASSIFICATION jobs.
    """
    from fastapi.responses import FileResponse # Import here
    # import os # Already imported above

    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Request to download results for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for download", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted download for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to download results for this job")

    # --- Check if download is applicable ---
    if job.job_type == JobType.REVIEW.value:
         logger.warning(f"Download requested for a REVIEW job ({job_id}), which doesn't generate an Excel file.", extra={"job_type": job.job_type})
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Download is not available for review jobs.")

    if job.status != JobStatus.COMPLETED.value or not job.output_file_name:
        logger.warning(f"Download requested but job not completed or output file missing",
                       extra={"job_id": job_id, "status": job.status, "output_file": job.output_file_name})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Job not completed or output file not available.")
    # --- End Check ---

    # Construct the full path to the output file
    output_dir = os.path.join(settings.OUTPUT_DATA_DIR, job_id)
    file_path = os.path.join(output_dir, job.output_file_name)

    if not os.path.exists(file_path):
         logger.error(f"Output file record exists in DB but file not found on disk",
                      extra={"job_id": job_id, "expected_path": file_path})
         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Output file not found.")

    logger.info(f"Streaming output file for download",
                extra={"job_id": job_id, "file_path": file_path})
    return FileResponse(
        path=file_path,
        filename=job.output_file_name, # Suggest filename to browser
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

# --- ADDED: Reclassify Endpoint ---
@router.post("/{original_job_id}/reclassify", response_model=ReclassifyResponse, status_code=status.HTTP_202_ACCEPTED)
async def reclassify_job_items(
    original_job_id: str = Path(..., description="The ID of the original classification job"),
    payload: ReclassifyPayload = ..., # Use the Pydantic model for the request body
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Initiates a re-classification task for specific vendors from an original job,
    using user-provided hints.
    """
    set_log_context({"username": current_user.username, "original_job_id": original_job_id})
    logger.info(f"Received reclassification request for job {original_job_id}", extra={"item_count": len(payload.items)})

    # 1. Find the original job
    original_job = db.query(Job).filter(Job.id == original_job_id).first()
    if not original_job:
        logger.warning(f"Original job {original_job_id} not found for reclassification.")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Original job not found")

    # 2. Authorization check (user owns the original job)
    if original_job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted reclassification for job '{original_job_id}' owned by '{original_job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to reclassify items for this job")

    # 3. Basic validation (ensure original job was classification, maybe check status?)
    if original_job.job_type != JobType.CLASSIFICATION.value:
         logger.warning(f"Attempted to reclassify based on a non-CLASSIFICATION job.", extra={"original_job_type": original_job.job_type})
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Reclassification can only be initiated from an original CLASSIFICATION job.")
    if original_job.status != JobStatus.COMPLETED.value:
         logger.warning(f"Attempted to reclassify based on a non-completed job.", extra={"original_job_status": original_job.status})
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Reclassification can only be initiated from a COMPLETED job.")

    if not payload.items:
        logger.warning("Reclassification request received with no items to process.")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No items provided for reclassification.")

    # 4. Create the new Review Job record
    review_job_id = f"review_{uuid.uuid4().hex[:12]}"
    review_job = Job(
        id=review_job_id,
        company_name=original_job.company_name, # Inherit company name
        input_file_name=f"Review of {original_job.input_file_name}", # Indicate source
        output_file_name=None, # Review jobs don't produce downloads (for now)
        status=JobStatus.PENDING.value,
        # --- FIX: Use a valid ProcessingStage ---
        current_stage=ProcessingStage.RECLASSIFICATION.value, # Set initial stage to RECLASSIFICATION
        # --- END FIX ---
        progress=0.0,
        created_by=current_user.username,
        target_level=original_job.target_level, # Inherit target level
        job_type=JobType.REVIEW.value, # Mark as REVIEW type
        parent_job_id=original_job_id, # Link back to the original job
        stats={"reclassify_input": [item.model_dump() for item in payload.items]}, # Store input hints/vendors
        detailed_results=None, # Will be populated by the task
        notification_email=original_job.notification_email # Optionally inherit email
    )

    try:
        db.add(review_job)
        db.commit()
        db.refresh(review_job)
        logger.info(f"Created new REVIEW job record", extra={"review_job_id": review_job_id, "parent_job_id": original_job_id})
    except Exception as e:
        db.rollback()
        logger.error("Failed to create REVIEW job record in database", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to initiate reclassification job.")

    # 5. Queue the Celery task
    try:
        logger.info(f"Queuing reclassification task for review job {review_job_id}")
        reclassify_flagged_vendors_task.delay(review_job_id=review_job_id)
        logger.info(f"Reclassification task successfully queued.")
    except Exception as e:
        logger.error(f"Failed to queue Celery reclassification task for review job {review_job_id}", exc_info=True)
        # Attempt to mark the created review job as failed
        review_job.fail(f"Failed to queue background task: {str(e)}")
        db.commit()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to queue reclassification task.")

    # 6. Return the review job ID
    return ReclassifyResponse(review_job_id=review_job_id)
# --- END ADDED ---
</file>

<file path='app/models/job.py'>
# <file path='app/models/job.py'>
# --- file path='app/models/job.py' ---
from sqlalchemy import Column, String, Float, DateTime, Enum as SQLEnum, JSON, Text, Integer, ForeignKey # <<< ADDED ForeignKey
from sqlalchemy.sql import func
from sqlalchemy.orm import Session # <<< ADDED IMPORT FOR TYPE HINTING
from enum import Enum as PyEnum
from datetime import datetime
from typing import Optional, Dict, Any, List # <<< ADDED List

from core.database import Base

class JobStatus(str, PyEnum):
    """Job status enum."""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingStage(str, PyEnum):
    """Processing stage enum."""
    INGESTION = "ingestion"
    NORMALIZATION = "normalization"
    CLASSIFICATION_L1 = "classification_level_1"
    CLASSIFICATION_L2 = "classification_level_2"
    CLASSIFICATION_L3 = "classification_level_3"
    CLASSIFICATION_L4 = "classification_level_4"
    CLASSIFICATION_L5 = "classification_level_5" # ADDED L5 Stage
    SEARCH = "search_unknown_vendors" # This stage now covers search AND recursive post-search classification
    RECLASSIFICATION = "reclassification" # <<< ADDED Reclassification Stage
    RESULT_GENERATION = "result_generation"

# --- ADDED: Job Type Enum ---
class JobType(str, PyEnum):
    """Type of job."""
    CLASSIFICATION = "CLASSIFICATION"
    REVIEW = "REVIEW"
# --- END ADDED ---


class Job(Base):
    """Job model for tracking classification jobs."""

    __tablename__ = "jobs"

    id = Column(String, primary_key=True, index=True)
    company_name = Column(String, nullable=False)
    input_file_name = Column(String, nullable=False)
    output_file_name = Column(String, nullable=True)
    status = Column(String, default=JobStatus.PENDING.value)
    current_stage = Column(String, default=ProcessingStage.INGESTION.value)
    progress = Column(Float, default=0.0)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now(), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    notification_email = Column(String, nullable=True)
    error_message = Column(Text, nullable=True)
    stats = Column(JSON, default={}) # Structure defined by ProcessingStats model OR used for review inputs
    created_by = Column(String, nullable=False)
    target_level = Column(Integer, nullable=False, default=5) # Store the desired classification depth (1-5)

    # --- ADDED: Job Type and Parent Link ---
    job_type = Column(String, default=JobType.CLASSIFICATION.value, nullable=False)
    parent_job_id = Column(String, ForeignKey("jobs.id"), nullable=True, index=True) # Link to original job for reviews
    # --- END ADDED ---

    # Stores List[JobResultItem] for CLASSIFICATION jobs
    # Stores List[ReviewResultItem] for REVIEW jobs
    detailed_results = Column(JSON, nullable=True)


    def update_progress(self, progress: float, stage: ProcessingStage, db_session: Optional[Session] = None): # Type hint now valid
        """Update job progress and stage, optionally committing."""
        self.progress = progress
        self.current_stage = stage.value
        self.updated_at = datetime.now()
        # Optionally commit immediately if session provided
        if db_session:
            try:
                db_session.commit()
            except Exception as e:
                from core.logging_config import get_logger # Local import for safety
                logger = get_logger("vendor_classification.job_model")
                logger.error(f"Failed to commit progress update for job {self.id}", exc_info=True)
                db_session.rollback()


    # --- UPDATED: complete method signature ---
    # detailed_results type hint updated to handle both list types
    def complete(self, output_file_name: Optional[str], stats: Dict[str, Any], detailed_results: Optional[List[Dict[str, Any]]] = None):
    # --- END UPDATED ---
        """Mark job as completed."""
        self.status = JobStatus.COMPLETED.value
        self.progress = 1.0
        # Ensure stage reflects completion (Result Generation for CLASSIFICATION, RECLASSIFICATION for REVIEW)
        self.current_stage = ProcessingStage.RESULT_GENERATION.value if self.job_type == JobType.CLASSIFICATION.value else ProcessingStage.RECLASSIFICATION.value
        self.output_file_name = output_file_name # Can be None for review jobs if no file is generated
        self.completed_at = datetime.now()
        self.stats = stats
        # --- UPDATED: Save detailed results ---
        self.detailed_results = detailed_results
        # --- END UPDATED ---
        self.updated_at = self.completed_at # Align updated_at with completed_at

    def fail(self, error_message: str):
        """Mark job as failed."""
        self.status = JobStatus.FAILED.value
        # Optionally set progress to 1.0 or leave as is upon failure
        # self.progress = 1.0
        self.error_message = error_message
        self.updated_at = datetime.now()
        # Ensure completed_at is Null if it failed
        self.completed_at = None
        # --- UPDATED: Ensure detailed_results is Null if it failed ---
        self.detailed_results = None
        # --- END UPDATED ---
</file>

<file path='app/schemas/job.py'>
# <file path='app/schemas/job.py'>
# app/schemas/job.py
from pydantic import BaseModel, Field, EmailStr
from typing import Optional, Dict, Any, List, Union # <<< ADDED Union
from datetime import datetime
from enum import Enum as PyEnum

from models.job import JobStatus, ProcessingStage, JobType # Import enums from model
from .review import ReviewResultItem # Import the review result schema

# --- UPDATED: Schema for a single detailed result item (for CLASSIFICATION jobs) ---
class JobResultItem(BaseModel):
    vendor_name: str = Field(..., description="Original vendor name")
    level1_id: Optional[str] = Field(None, description="Level 1 Category ID")
    level1_name: Optional[str] = Field(None, description="Level 1 Category Name")
    level2_id: Optional[str] = Field(None, description="Level 2 Category ID")
    level2_name: Optional[str] = Field(None, description="Level 2 Category Name")
    level3_id: Optional[str] = Field(None, description="Level 3 Category ID")
    level3_name: Optional[str] = Field(None, description="Level 3 Category Name")
    level4_id: Optional[str] = Field(None, description="Level 4 Category ID")
    level4_name: Optional[str] = Field(None, description="Level 4 Category Name")
    level5_id: Optional[str] = Field(None, description="Level 5 Category ID")
    level5_name: Optional[str] = Field(None, description="Level 5 Category Name")
    final_confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="Confidence score of the final classification level achieved (0.0 if not possible)")
    final_status: str = Field(..., description="Overall status ('Classified', 'Not Possible', 'Error')")
    classification_source: Optional[str] = Field(None, description="Source of the final classification ('Initial', 'Search')")
    classification_notes_or_reason: Optional[str] = Field(None, description="LLM notes or reason for failure/low confidence")
    achieved_level: Optional[int] = Field(None, ge=0, le=5, description="Deepest level successfully classified (0 if none)")

    class Config:
        from_attributes = True # For potential future ORM mapping if results move to separate table
# --- END UPDATED ---


class JobBase(BaseModel):
    company_name: str = Field(..., example="Example Corp")
    target_level: int = Field(default=5, ge=1, le=5, example=5) # Add target_level here
    notification_email: Optional[EmailStr] = Field(None, example="user@example.com")

class JobCreate(JobBase):
    # Fields required specifically on creation, if any (handled by JobBase for now)
    pass

class JobResponse(JobBase):
    id: str = Field(..., example="job_abc123")
    input_file_name: str = Field(..., example="vendors.xlsx")
    output_file_name: Optional[str] = Field(None, example="results_job_abc123.xlsx")
    status: JobStatus = Field(..., example=JobStatus.PROCESSING)
    current_stage: ProcessingStage = Field(..., example=ProcessingStage.CLASSIFICATION_L2)
    progress: float = Field(..., example=0.75)
    created_at: datetime = Field(...)
    updated_at: datetime = Field(...)
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = Field(None, example="Failed during search phase.")
    stats: Dict[str, Any] = Field(default={}, example={"total_vendors": 100, "unique_vendors": 95})
    created_by: str = Field(..., example="user@example.com")

    # --- ADDED: Job Type and Parent Link ---
    job_type: JobType = Field(..., example=JobType.CLASSIFICATION)
    parent_job_id: Optional[str] = Field(None, example="job_xyz789")
    # --- END ADDED ---

    # NOTE: We don't include detailed_results here by default to keep this response smaller.
    # It will be fetched via a separate endpoint if needed.

    class Config:
        from_attributes = True # Enable ORM mode for automatic mapping from Job model
        use_enum_values = True # Ensure enum values (strings) are used in the response


# --- ADDED: Schema for the detailed results endpoint response ---
# This allows the endpoint to return either type of result list based on job type
class JobResultsResponse(BaseModel):
    job_id: str
    job_type: JobType
    results: Union[List[JobResultItem], List[ReviewResultItem]] = Field(..., description="List of detailed results, structure depends on job_type")
# --- END ADDED ---
</file>

<file path='app/schemas/review.py'>
# app/schemas/review.py
from pydantic import BaseModel, Field
from typing import List, Dict, Any

# Schema for items in the reclassify request payload
class ReclassifyRequestItem(BaseModel):
    vendor_name: str = Field(..., description="The exact vendor name to reclassify")
    hint: str = Field(..., description="User-provided hint for reclassification")

# Schema for the reclassify request payload
class ReclassifyPayload(BaseModel):
    items: List[ReclassifyRequestItem] = Field(..., description="List of vendors and hints to reclassify")

# Schema for the reclassify response
class ReclassifyResponse(BaseModel):
    review_job_id: str = Field(..., description="The ID of the newly created review job")
    message: str = Field(default="Re-classification job started.", description="Status message")

# Schema for a single item in the detailed_results of a REVIEW job
# It stores the original result (as a dict) and the new result (as a dict)
class ReviewResultItem(BaseModel):
    vendor_name: str = Field(..., description="Original vendor name")
    hint: str = Field(..., description="Hint provided by the user for this reclassification")
    # Store the full original result structure (which should match JobResultItem)
    original_result: Dict[str, Any] = Field(..., description="The original classification result for this vendor")
    # Store the full new result structure (which should also match JobResultItem)
    new_result: Dict[str, Any] = Field(..., description="The new classification result after applying the hint")

    class Config:
        from_attributes = True # For potential future ORM mapping if results move to separate table
</file>

<file path='app/services/file_service.py'>
# <file path='app/services/file_service.py'>
# app/services/file_service.py
import os
import pandas as pd
from fastapi import UploadFile
import shutil
from typing import List, Dict, Any, Optional, Set
import uuid
import logging
from datetime import datetime
import io # Added for reading UploadFile in memory

from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import get_logger
from core.log_context import set_log_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call

# Configure logger
logger = get_logger("vendor_classification.file_service")

# --- Define expected column names (case-insensitive matching) ---
VENDOR_NAME_COL = 'vendor_name'
# --- ADDED: List of all optional columns for easier checking ---
OPTIONAL_COLS_LOWER = {
    'optional_example_good_serviced_purchased',
    'vendor_address',
    'vendor_website',
    'internal_category',
    'parent_company',
    'spend_category'
}
# --- END ADDED ---

# --- Keep original definitions for reference in read_vendor_file ---
OPTIONAL_EXAMPLE_COL = 'optional_example_good_serviced_purchased'
OPTIONAL_ADDRESS_COL = 'vendor_address'
OPTIONAL_WEBSITE_COL = 'vendor_website'
OPTIONAL_INTERNAL_CAT_COL = 'internal_category'
OPTIONAL_PARENT_CO_COL = 'parent_company'
OPTIONAL_SPEND_CAT_COL = 'spend_category'
# --- End Define expected column names ---


# --- ADDED: Function to Validate File Header ---
@log_function_call(logger, include_args=False) # Keep args=False for UploadFile
def validate_file_header(file: UploadFile) -> Dict[str, Any]:
    """
    Reads only the header of an UploadFile (Excel) to validate its structure.

    Checks for:
    1. Readability as an Excel file.
    2. Presence of the mandatory 'vendor_name' column (case-insensitive).

    Returns a dictionary with validation status and detected columns.
    """
    log_extra = {"uploaded_filename": file.filename} # Renamed from 'filename'
    logger.info("Starting file header validation", extra=log_extra)

    result = {
        "is_valid": False,
        "message": "Validation not completed.",
        "detected_columns": [],
        "missing_mandatory_columns": []
    }

    try:
        # Read only the header row (or first few rows) to get columns
        # Using BytesIO to read from the UploadFile's stream in memory
        # Important: We read the stream here. If this same UploadFile object
        # needs to be read again later (e.g., in the main upload endpoint
        # without re-uploading), its stream position needs to be reset (await file.seek(0)).
        # However, the typical flow is validate -> frontend -> upload, which are separate requests.
        file_content = file.file.read()
        file.file.seek(0) # Reset stream position in case it's needed elsewhere (though unlikely in this flow)

        # --- FIX: Remove 'extra' from LogTimer call ---
        # The LogTimer class does not accept the 'extra' argument based on the TypeError.
        # Context logging should still capture the filename via log_extra used in logger calls.
        with LogTimer(logger, "Header read (pandas)"):
        # --- END FIX ---
            # nrows=0 reads only the header, nrows=1 reads header + first data row etc.
            # Using nrows=0 is sufficient and fastest for just column names.
            df_header = pd.read_excel(io.BytesIO(file_content), header=0, nrows=0)

        detected_columns_raw = list(df_header.columns)
        # Convert all column names to string for safety
        detected_columns = [str(col) for col in detected_columns_raw]
        result["detected_columns"] = detected_columns
        log_extra["detected_columns"] = detected_columns # Add detected columns to log_extra
        logger.debug(f"Detected columns: {detected_columns}", extra=log_extra)

        # --- Perform Validation ---
        normalized_detected_columns = {col.strip().lower(): col for col in detected_columns if isinstance(col, str)}

        # Check for mandatory column
        if VENDOR_NAME_COL not in normalized_detected_columns:
            result["is_valid"] = False
            result["message"] = f"Validation Failed: Mandatory column '{VENDOR_NAME_COL}' is missing (case-insensitive)."
            result["missing_mandatory_columns"] = [VENDOR_NAME_COL]
            logger.warning(f"Mandatory column '{VENDOR_NAME_COL}' missing.", extra=log_extra)
        else:
            result["is_valid"] = True
            result["message"] = f"Validation Successful: Found mandatory column '{normalized_detected_columns[VENDOR_NAME_COL]}'."
            # Optionally list found optional columns
            found_optional = [
                normalized_detected_columns[opt_col]
                for opt_col in OPTIONAL_COLS_LOWER
                if opt_col in normalized_detected_columns
            ]
            if found_optional:
                result["message"] += f" Found optional columns: {', '.join(found_optional)}."
            else:
                 result["message"] += " No optional context columns detected."
            logger.info("Mandatory column found.", extra=log_extra)

    except ValueError as e:
        # More specific error for pandas read errors
        logger.warning(f"Pandas ValueError during header read: {e}", extra=log_extra)
        result["message"] = f"File Read Error: Could not parse Excel header. Ensure it's a valid .xlsx or .xls file. Details: {str(e)[:100]}"
        raise ValueError(result["message"]) # Re-raise to be caught by API endpoint
    except Exception as e:
        logger.error(f"Unexpected error during header validation", exc_info=True, extra=log_extra)
        result["message"] = f"Internal Server Error: An unexpected error occurred during file validation. Details: {str(e)[:100]}"
        # Don't raise generic Exception here, let the endpoint handle it
        # Set is_valid to false as a precaution
        result["is_valid"] = False # Ensure invalid state on unexpected error

    return result
# --- END ADDED: Function to Validate File Header ---


@log_function_call(logger, include_args=False) # Keep args=False for UploadFile
def save_upload_file(file: UploadFile, job_id: str) -> str:
    """
    Save uploaded file to the input directory.
    """
    job_dir = os.path.join(settings.INPUT_DATA_DIR, job_id)
    log_extra = {"job_id": job_id} # Base log extra for this function
    try:
        os.makedirs(job_dir, exist_ok=True)
        logger.debug(f"Ensured job directory exists", extra={**log_extra, "directory": job_dir})
    except OSError as e:
        logger.error(f"Failed to create job directory", exc_info=True, extra={**log_extra, "directory": job_dir})
        raise IOError(f"Could not create directory for job {job_id}: {e}")

    safe_filename = os.path.basename(file.filename or f"upload_{job_id}.tmp")
    if not safe_filename:
         safe_filename = f"upload_{job_id}.tmp"

    file_path = os.path.join(job_dir, safe_filename)
    save_log_extra = {**log_extra, "path": file_path, "original_filename": file.filename}
    logger.info("Attempting to save file", extra=save_log_extra)

    # --- FIX: Remove 'extra' from LogTimer call ---
    with LogTimer(logger, "File saving"): # Removed extra=save_log_extra
    # --- END FIX ---
        try:
            # Ensure stream is at the beginning before copying
            # This is important if the stream was read previously (e.g., by validation
            # IF the same file object instance was somehow reused, which is not the case here)
            # await file.seek(0) # Use await for async file interface if needed, otherwise just file.seek(0)
            # For standard FastAPI UploadFile, file.file is a SpooledTemporaryFile (sync interface)
            file.file.seek(0)
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
        except Exception as e:
            logger.error("Failed to save uploaded file content", exc_info=True, extra=save_log_extra)
            if os.path.exists(file_path):
                 try: os.remove(file_path)
                 except OSError: logger.warning("Could not remove partially written file on error.", extra=save_log_extra)
            raise IOError(f"Could not save uploaded file content: {e}")
        finally:
            # Close the underlying file handle of UploadFile
            if hasattr(file, 'close') and callable(file.close):
                try:
                    file.close()
                    logger.debug("Closed UploadFile stream after saving.", extra=save_log_extra)
                except Exception as close_err:
                    logger.warning(f"Error closing UploadFile stream: {close_err}", exc_info=False, extra=save_log_extra)


    try:
        file_size = os.path.getsize(file_path)
        logger.info(f"File saved successfully",
                   extra={**save_log_extra, "size_bytes": file_size})
    except OSError as e:
        logger.warning(f"Could not get size of saved file", exc_info=False, extra={**save_log_extra, "error": str(e)})
        file_size = -1

    return file_path


@log_function_call(logger)
def read_vendor_file(file_path: str) -> List[Dict[str, Any]]:
    """
    Read vendor data from Excel file, looking for mandatory 'vendor_name'
    and several optional context columns (case-insensitively).
    """
    log_extra = {"file_path": file_path}
    logger.info(f"Reading Excel file for vendor data", extra=log_extra)

    if not os.path.exists(file_path):
         logger.error(f"Input file not found at path", extra=log_extra)
         raise FileNotFoundError(f"Input file not found at path: {file_path}")

    # --- FIX: Remove 'extra' from LogTimer call ---
    with LogTimer(logger, "Excel file reading", include_in_stats=True): # Removed extra=log_extra
    # --- END FIX ---
        try:
            # Read the entire file now
            df = pd.read_excel(file_path, header=0)
            detected_columns = list(df.columns)
            logger.debug(f"Successfully read Excel file. Columns detected: {detected_columns}", extra=log_extra)
        except Exception as e:
            logger.error(f"Error reading Excel file with pandas", exc_info=True, extra=log_extra)
            raise ValueError(f"Could not parse the Excel file. Please ensure it is a valid .xlsx or .xls file. Error details: {str(e)}")

    # --- Find columns case-insensitively ---
    column_map: Dict[str, Optional[str]] = {
        'vendor_name': None,
        'example': None,
        'address': None,
        'website': None,
        'internal_cat': None,
        'parent_co': None,
        'spend_cat': None
    }
    # Convert all detected columns to string for reliable matching
    normalized_detected_columns = {str(col).strip().lower(): str(col) for col in detected_columns if isinstance(col, (str, int, float))} # Allow numeric cols but treat as str

    # Find vendor_name (mandatory)
    if VENDOR_NAME_COL in normalized_detected_columns:
        column_map['vendor_name'] = normalized_detected_columns[VENDOR_NAME_COL]
        logger.info(f"Found mandatory column '{VENDOR_NAME_COL}' as: '{column_map['vendor_name']}'", extra=log_extra)
    else:
        logger.error(f"Required column '{VENDOR_NAME_COL}' not found in file.",
                    extra={**log_extra, "available_columns": detected_columns})
        # This error should ideally be caught by the pre-validation step now
        raise ValueError(f"Input Excel file must contain a column named '{VENDOR_NAME_COL}' (case-insensitive). Found columns: {', '.join(map(str, detected_columns))}")

    # Find optional columns (using original constants here)
    optional_cols = {
        'example': OPTIONAL_EXAMPLE_COL,
        'address': OPTIONAL_ADDRESS_COL,
        'website': OPTIONAL_WEBSITE_COL,
        'internal_cat': OPTIONAL_INTERNAL_CAT_COL,
        'parent_co': OPTIONAL_PARENT_CO_COL,
        'spend_cat': OPTIONAL_SPEND_CAT_COL
    }
    for key, col_name in optional_cols.items():
        # Check against the lowercased OPTIONAL_COLS_LOWER set for consistency
        if col_name.lower() in normalized_detected_columns:
            column_map[key] = normalized_detected_columns[col_name.lower()]
            logger.info(f"Found optional column '{col_name}' as: '{column_map[key]}'", extra=log_extra)
        else:
            logger.info(f"Optional column '{col_name}' not found.", extra=log_extra)
    # --- End Find columns ---

    # --- Extract data into list of dictionaries ---
    vendors_data: List[Dict[str, Any]] = []
    processed_count = 0
    skipped_count = 0

    try:
        for index, row in df.iterrows():
            vendor_name_raw = row.get(column_map['vendor_name'])
            vendor_name = str(vendor_name_raw).strip() if pd.notna(vendor_name_raw) and str(vendor_name_raw).strip() else None

            if not vendor_name or vendor_name.lower() in ['nan', 'none', 'null']:
                skipped_count += 1
                continue

            vendor_entry: Dict[str, Any] = {'vendor_name': vendor_name}

            # Add optional fields if found
            for key, mapped_col in column_map.items():
                if key != 'vendor_name' and mapped_col: # Check if optional column was found
                    raw_value = row.get(mapped_col)
                    # Ensure value is treated as string, handle potential NaN/None from Pandas
                    value = str(raw_value).strip() if pd.notna(raw_value) and str(raw_value).strip() else None
                    if value and value.lower() not in ['nan', 'none', 'null', '#n/a']: # Additional check for common excel non-values
                        output_key = key # Default key
                        # Map internal key back to original (or desired output) key name
                        if key == 'example': output_key = 'example' # Keep as 'example' if needed, or map to full name
                        elif key == 'address': output_key = 'vendor_address'
                        elif key == 'website': output_key = 'vendor_website'
                        elif key == 'internal_cat': output_key = 'internal_category'
                        elif key == 'parent_co': output_key = 'parent_company'
                        elif key == 'spend_cat': output_key = 'spend_category'
                        vendor_entry[output_key] = value

            vendors_data.append(vendor_entry)
            processed_count += 1

        logger.info(f"Extracted data for {processed_count} vendors. Skipped {skipped_count} rows due to missing/invalid vendor name.", extra=log_extra)
        if not vendors_data:
             logger.warning(f"No valid vendor data found in the file after processing rows.", extra=log_extra)

    except KeyError as e:
        logger.error(f"Internal Error: KeyError accessing column '{e}' after it was seemingly mapped.",
                     extra={**log_extra, "column_map": column_map, "available_columns": detected_columns})
        raise ValueError(f"Internal error accessing column '{e}'.")
    except Exception as e:
        logger.error(f"Error extracting or processing data from file rows", exc_info=True, extra=log_extra)
        raise ValueError(f"Could not extract vendor data. Please check data format. Error: {e}")

    return vendors_data
    # --- End Extract data ---


@log_function_call(logger)
def normalize_vendor_data(vendors_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Normalize vendor names within the list of dictionaries by converting
    to title case and stripping whitespace. Filters out entries with
    empty names after normalization. Preserves other fields.
    """
    start_count = len(vendors_data)
    log_extra = {"original_count": start_count}
    logger.info(f"Normalizing vendor names for {start_count} entries...", extra=log_extra)

    normalized_vendors_data = []
    empty_removed_count = 0

    # --- FIX: Remove 'extra' from LogTimer call ---
    with LogTimer(logger, "Vendor name normalization", include_in_stats=True): # Removed extra=log_extra
    # --- END FIX ---
        for entry in vendors_data:
            original_name = entry.get('vendor_name')
            if isinstance(original_name, str):
                normalized_name = original_name.strip().title()
                if normalized_name:
                    normalized_entry = entry.copy()
                    normalized_entry['vendor_name'] = normalized_name
                    normalized_vendors_data.append(normalized_entry)
                else:
                    empty_removed_count += 1
                    # Avoid logging potentially large 'entry' in warning
                    logger.warning("Skipping vendor entry due to empty name after normalization", extra={"original_name": original_name})
            else:
                # Avoid logging potentially large 'entry' in warning
                logger.warning("Skipping vendor entry due to missing or non-string name during normalization", extra={"original_name": original_name})
                empty_removed_count += 1

    final_count = len(normalized_vendors_data)
    logger.info(f"Vendor names normalized.",
               extra={
                   "original_count": start_count,
                   "normalized_count": final_count,
                   "empty_or_skipped": empty_removed_count
               })

    return normalized_vendors_data


@log_function_call(logger)
def generate_output_file(
    original_vendor_data: List[Dict[str, Any]],
    classification_results: Dict[str, Dict],
    job_id: str
) -> str:
    """
    Generate output Excel file with classification results (up to Level 5), mapping back to
    original vendor data including optional fields read from the input.
    """
    log_extra = {"job_id": job_id, "original_vendor_count": len(original_vendor_data)}
    logger.info(f"Generating output file for {len(original_vendor_data)} original vendor entries",
               extra=log_extra)

    output_data = []

    # --- FIX: Remove 'extra' from LogTimer call ---
    with LogTimer(logger, "Mapping results to original vendors"): # Removed extra=log_extra
    # --- END FIX ---
        for original_entry in original_vendor_data:
            original_vendor_name = original_entry.get('vendor_name', '')
            # Use the normalized name for lookup if normalization happened before this step
            # Assuming classification_results keys are based on normalized names if normalize_vendor_data was called
            lookup_name = original_vendor_name # Adjust if needed based on workflow
            result = classification_results.get(lookup_name, {})

            final_level1_id = ""; final_level1_name = ""
            final_level2_id = ""; final_level2_name = ""
            final_level3_id = ""; final_level3_name = ""
            final_level4_id = ""; final_level4_name = ""
            final_level5_id = ""; final_level5_name = ""
            final_confidence = 0.0
            classification_not_possible_flag = True
            final_notes = ""
            reason = "Classification not possible"
            classification_source = "Initial"

            highest_successful_level = 0
            for level in range(5, 0, -1):
                level_key = f"level{level}"
                level_res = result.get(level_key)
                if level_res and isinstance(level_res, dict) and not level_res.get("classification_not_possible", True):
                    highest_successful_level = level
                    break

            if highest_successful_level > 0:
                classification_not_possible_flag = False
                reason = None # Clear default reason if successful
                final_confidence = result[f"level{highest_successful_level}"].get("confidence", 0.0)
                final_notes = result[f"level{highest_successful_level}"].get("notes", "")

                for level in range(1, highest_successful_level + 1):
                    level_res = result.get(f"level{level}", {})
                    if level == 1: final_level1_id = level_res.get("category_id", ""); final_level1_name = level_res.get("category_name", "")
                    elif level == 2: final_level2_id = level_res.get("category_id", ""); final_level2_name = level_res.get("category_name", "")
                    elif level == 3: final_level3_id = level_res.get("category_id", ""); final_level3_name = level_res.get("category_name", "")
                    elif level == 4: final_level4_id = level_res.get("category_id", ""); final_level4_name = level_res.get("category_name", "")
                    elif level == 5: final_level5_id = level_res.get("category_id", ""); final_level5_name = level_res.get("category_name", "")

                if result.get("classified_via_search"):
                    classification_source = "Search"
                    # Prepend search info to notes if notes exist, otherwise just use search info
                    search_note = "Classified via search."
                    final_notes = f"{search_note} {final_notes}" if final_notes else search_note

            else: # No level was successfully classified
                classification_not_possible_flag = True
                final_confidence = 0.0
                failure_reason_found = False
                # Look for explicit failure reasons from highest level down
                for level in range(5, 0, -1):
                     level_res = result.get(f"level{level}")
                     if level_res and isinstance(level_res, dict) and level_res.get("classification_not_possible", False):
                          reason = level_res.get("classification_not_possible_reason", f"Classification failed at Level {level}")
                          final_notes = level_res.get("notes", "") # Capture notes even on failure
                          failure_reason_found = True
                          break
                # If no explicit reason found, check search results for failure info
                if not failure_reason_found:
                     if result.get("search_attempted"):
                          search_l1_result = result.get("search_results", {}).get("classification_l1", {})
                          if search_l1_result and search_l1_result.get("classification_not_possible", False):
                               reason = search_l1_result.get("classification_not_possible_reason", "Search did not yield classification")
                               final_notes = search_l1_result.get("notes", "")
                          elif result.get("search_results", {}).get("error"):
                               reason = f"Search error: {result['search_results']['error']}"
                          else:
                               # Default reason if search attempted but no specific failure reason found
                               reason = "Classification failed after search attempt."

            # Retrieve original optional fields from the input data
            original_address = original_entry.get('vendor_address')
            original_website = original_entry.get('vendor_website')
            original_internal_cat = original_entry.get('internal_category')
            original_parent_co = original_entry.get('parent_company')
            original_spend_cat = original_entry.get('spend_category')
            original_example = original_entry.get('example') # Assuming 'example' key was used in vendors_data

            search_sources_urls = ""
            search_data = result.get("search_results", {})
            if search_data and isinstance(search_data.get("sources"), list):
                 search_sources_urls = ", ".join(
                     source.get("url", "") for source in search_data["sources"] if isinstance(source, dict) and source.get("url")
                 )

            row = {
                # Use original vendor name from input file for output consistency
                "vendor_name": original_entry.get('vendor_name', ''), # Use the name directly from original_entry
                "vendor_address": original_address or "",
                "vendor_website": original_website or "",
                "internal_category": original_internal_cat or "",
                "parent_company": original_parent_co or "",
                "spend_category": original_spend_cat or "",
                 # Use the full optional column name as expected in output
                "Optional_example_good_serviced_purchased": original_example or "",
                "level1_category_id": final_level1_id, "level1_category_name": final_level1_name,
                "level2_category_id": final_level2_id, "level2_category_name": final_level2_name,
                "level3_category_id": final_level3_id, "level3_category_name": final_level3_name,
                "level4_category_id": final_level4_id, "level4_category_name": final_level4_name,
                "level5_category_id": final_level5_id, "level5_category_name": final_level5_name,
                "final_confidence": final_confidence,
                "classification_not_possible": classification_not_possible_flag,
                # Combine reason and notes for clarity if classification failed
                "classification_notes_or_reason": reason if classification_not_possible_flag else (final_notes or ""),
                "classification_source": classification_source,
                "sources": search_sources_urls
            }
            output_data.append(row)

    output_columns = [
        "vendor_name", "vendor_address", "vendor_website", "internal_category", "parent_company", "spend_category",
        "Optional_example_good_serviced_purchased", # Match the exact optional column name
        "level1_category_id", "level1_category_name", "level2_category_id", "level2_category_name",
        "level3_category_id", "level3_category_name", "level4_category_id", "level4_category_name",
        "level5_category_id", "level5_category_name",
        "final_confidence", "classification_not_possible", "classification_notes_or_reason",
        "classification_source", "sources"
    ]
    if not output_data:
        logger.warning("No data rows generated for the output file.", extra=log_extra)
        df = pd.DataFrame(columns=output_columns)
    else:
        # --- FIX: Remove 'extra' from LogTimer call ---
        with LogTimer(logger, "Creating DataFrame for output"): # Removed extra=log_extra
        # --- END FIX ---
            df = pd.DataFrame(output_data, columns=output_columns)

    output_dir = os.path.join(settings.OUTPUT_DATA_DIR, job_id)
    try:
        os.makedirs(output_dir, exist_ok=True)
        logger.debug(f"Ensured output directory exists", extra={**log_extra, "directory": output_dir})
    except OSError as e:
        logger.error(f"Failed to create output directory", exc_info=True, extra={**log_extra, "directory": output_dir})
        raise IOError(f"Could not create output directory for job {job_id}: {e}")

    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file_name = f"classification_results_{job_id[:8]}_{timestamp_str}.xlsx"
    output_path = os.path.join(output_dir, output_file_name)

    output_log_extra = {**log_extra, "output_path": output_path, "output_filename": output_file_name}
    logger.info("Attempting to write final results to Excel file.", extra=output_log_extra)
    # --- FIX: Remove 'extra' from LogTimer call ---
    with LogTimer(logger, "Writing Excel file"): # Removed extra=output_log_extra
    # --- END FIX ---
        try:
            df.to_excel(output_path, index=False, engine='xlsxwriter')
        except Exception as e:
            logger.error("Failed to write output Excel file", exc_info=True, extra=output_log_extra)
            raise IOError(f"Could not write output file: {e}")

    try:
        file_size = os.path.getsize(output_path)
        logger.info(f"Output file generated successfully",
                   extra={**output_log_extra, "size_bytes": file_size})
    except OSError as e:
         logger.warning(f"Could not get size of generated output file", exc_info=False, extra={**output_log_extra, "error": str(e)})

    return output_file_name
</file>

<file path='app/tasks/classification_tasks.py'>
# <file path='app/tasks/classification_tasks.py'>
# app/tasks/classification_tasks.py
import os
import asyncio
import logging
from datetime import datetime
from celery import shared_task
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from typing import Dict, Any, List, Optional # <<< ADDED List, Optional

from core.database import SessionLocal
from core.config import settings
from core.logging_config import get_logger
# Import context functions from the new module
from core.log_context import set_correlation_id, set_job_id, set_log_context, clear_all_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_duration

from models.job import Job, JobStatus, ProcessingStage, JobType # <<< ADDED JobType
from services.file_service import read_vendor_file, normalize_vendor_data, generate_output_file
from services.llm_service import LLMService
from services.search_service import SearchService
from utils.taxonomy_loader import load_taxonomy

# Import the refactored logic
from .classification_logic import process_vendors
# Import the schema for type hinting
from schemas.job import JobResultItem
# Import review schemas/logic if needed (likely handled by separate task)
from schemas.review import ReviewResultItem


# Configure logger
logger = get_logger("vendor_classification.tasks")
# --- ADDED: Log confirmation ---
logger.debug("Successfully imported Dict and Any from typing for classification tasks.")
# --- END ADDED ---


# --- UPDATED: Helper function to process results for DB storage ---
def _prepare_detailed_results_for_storage(
    results_dict: Dict[str, Dict],
    target_level: int # Keep target_level for reference if needed, but we store all levels now
) -> List[Dict[str, Any]]:
    """
    Processes the complex results dictionary (containing level1, level2... sub-dicts)
    into a flat list of dictionaries, where each dictionary represents a vendor
    and contains fields for all L1-L5 classifications, plus final status details.
    Matches the JobResultItem schema.
    THIS IS FOR **CLASSIFICATION** JOBS. Review jobs store results differently.
    """
    processed_list = []
    logger.info(f"Preparing detailed results for CLASSIFICATION job storage. Processing {len(results_dict)} vendors.")

    for vendor_name, vendor_results in results_dict.items():
        # Initialize the flat structure for this vendor
        flat_result: Dict[str, Any] = {
            "vendor_name": vendor_name,
            "level1_id": None, "level1_name": None,
            "level2_id": None, "level2_name": None,
            "level3_id": None, "level3_name": None,
            "level4_id": None, "level4_name": None,
            "level5_id": None, "level5_name": None,
            "final_confidence": None,
            "final_status": "Not Possible", # Default status
            "classification_source": "Initial", # Default source
            "classification_notes_or_reason": None,
            "achieved_level": 0 # Default achieved level
        }

        deepest_successful_level = 0
        final_level_data = None
        final_source = "Initial" # Track the source of the final decision point
        final_notes_or_reason = None

        # Iterate through levels 1 to 5 to populate the flat structure
        for level in range(1, 6):
            level_key = f"level{level}"
            level_data = vendor_results.get(level_key)

            if level_data and isinstance(level_data, dict):
                # Populate the corresponding fields in flat_result
                flat_result[f"level{level}_id"] = level_data.get("category_id")
                flat_result[f"level{level}_name"] = level_data.get("category_name")

                # Track the deepest successful classification
                if not level_data.get("classification_not_possible", True):
                    deepest_successful_level = level
                    final_level_data = level_data # Store data of the deepest successful level
                    # Update source based on the source recorded *at that level*
                    final_source = level_data.get("classification_source", final_source)
                    final_notes_or_reason = level_data.get("notes") # Get notes from successful level
                elif deepest_successful_level == 0: # If no level succeeded yet, track potential failure reasons/notes from L1
                    if level == 1:
                        final_notes_or_reason = level_data.get("classification_not_possible_reason") or level_data.get("notes")
                        # Update source based on L1 source if it exists
                        final_source = level_data.get("classification_source", final_source)

            # If a level wasn't processed (e.g., stopped early), its fields remain None

        # Determine final status, confidence, and notes based on the deepest successful level
        if final_level_data:
            flat_result["final_status"] = "Classified"
            flat_result["final_confidence"] = final_level_data.get("confidence")
            flat_result["achieved_level"] = deepest_successful_level
            flat_result["classification_notes_or_reason"] = final_notes_or_reason # Use notes from final level
        else:
            # No level was successfully classified
            flat_result["final_status"] = "Not Possible"
            flat_result["final_confidence"] = 0.0
            flat_result["achieved_level"] = 0
            # Use the reason/notes captured from L1 failure or search failure
            flat_result["classification_notes_or_reason"] = final_notes_or_reason

        # Set the final determined source
        flat_result["classification_source"] = final_source

        # Handle potential ERROR states explicitly (e.g., if L1 failed with ERROR)
        l1_data = vendor_results.get("level1")
        if l1_data and l1_data.get("category_id") == "ERROR":
            flat_result["final_status"] = "Error"
            flat_result["classification_notes_or_reason"] = l1_data.get("classification_not_possible_reason") or "Processing error occurred"
            # Override source if error occurred
            final_source = l1_data.get("classification_source", "Initial")
            flat_result["classification_source"] = final_source


        # Validate against Pydantic model (optional, but good practice)
        try:
            JobResultItem.model_validate(flat_result)
            processed_list.append(flat_result)
        except Exception as validation_err:
            logger.error(f"Validation failed for prepared result of vendor '{vendor_name}'",
                         exc_info=True, extra={"result_data": flat_result})
            # Optionally append a placeholder error entry or skip
            # For now, let's skip invalid entries
            continue

    logger.info(f"Finished preparing {len(processed_list)} detailed result items for CLASSIFICATION job storage.")
    return processed_list
# --- END UPDATED ---


@shared_task(bind=True)
# --- UPDATED: Added target_level parameter ---
def process_vendor_file(self, job_id: str, file_path: str, target_level: int):
# --- END UPDATED ---
    """
    Celery task entry point for processing a vendor file (CLASSIFICATION job type).
    Orchestrates the overall process by calling the main async helper.

    Args:
        job_id: Job ID
        file_path: Path to vendor file
        target_level: The desired maximum classification level (1-5)
    """
    task_id = self.request.id if self.request and self.request.id else "UnknownTaskID"
    logger.info(f"***** process_vendor_file TASK RECEIVED (CLASSIFICATION) *****",
                extra={
                    "celery_task_id": task_id,
                    "job_id_arg": job_id,
                    "file_path_arg": file_path,
                    "target_level_arg": target_level # Log received target level
                })

    set_correlation_id(job_id) # Set correlation ID early
    set_job_id(job_id)
    set_log_context({"target_level": target_level, "job_type": JobType.CLASSIFICATION.value}) # Add target level and type to context
    logger.info(f"Starting vendor file processing task (inside function)",
                extra={"job_id": job_id, "file_path": file_path, "target_level": target_level})

    # Validate target_level
    if not 1 <= target_level <= 5:
        logger.error(f"Invalid target_level received: {target_level}. Must be between 1 and 5.")
        # Fail the job immediately if level is invalid
        db_fail = SessionLocal()
        try:
            job_fail = db_fail.query(Job).filter(Job.id == job_id).first()
            if job_fail:
                job_fail.fail(f"Invalid target level specified: {target_level}. Must be 1-5.")
                db_fail.commit()
        except Exception as db_err:
            logger.error("Failed to mark job as failed due to invalid target level", exc_info=db_err)
            db_fail.rollback()
        finally:
            db_fail.close()
        clear_all_context() # Clear context before returning
        return # Stop task execution

    # Initialize loop within the task context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    logger.debug(f"Created and set new asyncio event loop for job {job_id}")

    db = SessionLocal()
    job = None # Initialize job to None

    try:
        job = db.query(Job).filter(Job.id == job_id).first()
        if job:
            # Verify the target level matches the job record (optional sanity check)
            if job.target_level != target_level:
                logger.warning(f"Task received target_level {target_level} but job record has {job.target_level}. Using task value: {target_level}.")
                # Optionally update job record here if desired, or just proceed with task value

            # Ensure job type is CLASSIFICATION
            if job.job_type != JobType.CLASSIFICATION.value:
                 logger.error(f"process_vendor_file task called for a non-CLASSIFICATION job.", extra={"job_id": job_id, "job_type": job.job_type})
                 raise ValueError(f"Job {job_id} is not a CLASSIFICATION job.")


            set_log_context({
                "company_name": job.company_name,
                "creator": job.created_by,
                "file_name": job.input_file_name
                # target_level and job_type already set above
            })
            logger.info(f"Processing file for company",
                        extra={"company": job.company_name})
        else:
            logger.error("Job not found in database at start of task!", extra={"job_id": job_id})
            loop.close() # Close loop if job not found
            db.close() # Close db session
            clear_all_context() # Clear context before returning
            return # Exit task if job doesn't exist

        logger.info(f"About to run async processing for job {job_id}")
        with LogTimer(logger, "Complete file processing", level=logging.INFO, include_in_stats=True):
            # Run the async function within the loop created for this task
            # --- UPDATED: Pass target_level to async helper ---
            loop.run_until_complete(_process_vendor_file_async(job_id, file_path, db, target_level))
            # --- END UPDATED ---

        logger.info(f"Vendor file processing completed successfully (async part finished)")

    except Exception as e:
        logger.error(f"Error processing vendor file task (in main try block)", exc_info=True, extra={"job_id": job_id})
        try:
            # Re-query the job within this exception handler if it wasn't fetched initially or became None
            db_error_session = SessionLocal()
            try:
                job_in_error = db_error_session.query(Job).filter(Job.id == job_id).first()
                if job_in_error:
                    if job_in_error.status != JobStatus.COMPLETED.value:
                        err_msg = f"Task failed: {type(e).__name__}: {str(e)}"
                        job_in_error.fail(err_msg[:2000]) # Limit error message length
                        db_error_session.commit()
                        logger.info(f"Job status updated to failed due to task error",
                                    extra={"error": str(e)})
                    else:
                        logger.warning(f"Task error occurred after job was marked completed, status not changed.",
                                        extra={"error": str(e)})
                else:
                    logger.error("Job not found when trying to mark as failed.", extra={"job_id": job_id})
            except Exception as db_error:
                logger.error(f"Error updating job status during task failure handling", exc_info=True,
                            extra={"original_error": str(e), "db_error": str(db_error)})
                db_error_session.rollback()
            finally:
                    db_error_session.close()
        except Exception as final_db_error:
                logger.critical(f"CRITICAL: Failed even to handle database update in task error handler.", exc_info=final_db_error)

    finally:
        if db: # Close the main session used by the async function
            db.close()
            logger.debug(f"Main database session closed for task.")
        if loop and not loop.is_closed():
            loop.close()
            logger.debug(f"Event loop closed for task.")
        clear_all_context()
        logger.info(f"***** process_vendor_file TASK FINISHED (CLASSIFICATION) *****", extra={"job_id": job_id})


# --- UPDATED: Added target_level parameter ---
async def _process_vendor_file_async(job_id: str, file_path: str, db: Session, target_level: int):
# --- END UPDATED ---
    """
    Asynchronous part of the vendor file processing (CLASSIFICATION job type).
    Sets up services, initializes stats, calls the core processing logic,
    and handles final result generation and job status updates.
    """
    logger.info(f"[_process_vendor_file_async] Starting async processing for job {job_id} to target level {target_level}")

    llm_service = LLMService()
    search_service = SearchService()

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.error(f"[_process_vendor_file_async] Job not found in database", extra={"job_id": job_id})
        return

    # --- Initialize stats (Updated for L5) ---
    start_time = datetime.now()
    # --- MODIFIED: Type hints added ---
    stats: Dict[str, Any] = {
        "job_id": job.id,
        "company_name": job.company_name,
        "target_level": target_level, # Store target level in stats
        "start_time": start_time.isoformat(),
        "end_time": None,
        "processing_duration_seconds": None,
        "total_vendors": 0,
        "unique_vendors": 0,
        "successfully_classified_l4": 0, # Keep L4 count for reference
        "successfully_classified_l5": 0, # Count successful classifications reaching L5 (if target >= 5)
        "classification_not_possible_initial": 0, # Count initially unclassifiable before search
        "invalid_category_errors": 0, # Track validation errors
        "search_attempts": 0, # Count how many vendors needed search
        "search_successful_classifications_l1": 0, # Count successful L1 classifications *after* search
        "search_successful_classifications_l5": 0, # Count successful L5 classifications *after* search (if target >= 5)
        "api_usage": {
            "openrouter_calls": 0,
            "openrouter_prompt_tokens": 0,
            "openrouter_completion_tokens": 0,
            "openrouter_total_tokens": 0,
            "tavily_search_calls": 0,
            "cost_estimate_usd": 0.0
        }
    }
    # --- END MODIFIED ---
    # --- End Initialize stats ---

    # --- Initialize results dictionary ---
    # This will be populated by process_vendors
    results_dict: Dict[str, Dict] = {}
    # --- UPDATED: This will hold the processed results for DB storage (List[JobResultItem]) ---
    detailed_results_for_db: Optional[List[Dict[str, Any]]] = None
    # --- END UPDATED ---
    # --- End Initialize results dictionary ---

    try:
        job.status = JobStatus.PROCESSING.value
        job.current_stage = ProcessingStage.INGESTION.value
        job.progress = 0.05
        logger.info(f"[_process_vendor_file_async] Committing initial status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"status": job.status, "stage": job.current_stage, "progress": job.progress})

        logger.info(f"Reading vendor file")
        with log_duration(logger, "Reading vendor file"):
            vendors_data = read_vendor_file(file_path)
        logger.info(f"Vendor file read successfully",
                    extra={"vendor_count": len(vendors_data)})

        job.current_stage = ProcessingStage.NORMALIZATION.value
        job.progress = 0.1
        logger.info(f"[_process_vendor_file_async] Committing status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        logger.info(f"Normalizing vendor data")
        with log_duration(logger, "Normalizing vendor data"):
            normalized_vendors_data = normalize_vendor_data(vendors_data)
        logger.info(f"Vendor data normalized",
                    extra={"normalized_count": len(normalized_vendors_data)})

        logger.info(f"Identifying unique vendors")
        # --- MODIFIED: Type hints added ---
        unique_vendors_map: Dict[str, Dict[str, Any]] = {}
        # --- END MODIFIED ---
        for entry in normalized_vendors_data:
            name = entry.get('vendor_name')
            if name and name not in unique_vendors_map:
                unique_vendors_map[name] = entry
        logger.info(f"Unique vendors identified",
                    extra={"unique_count": len(unique_vendors_map)})

        stats["total_vendors"] = len(normalized_vendors_data)
        stats["unique_vendors"] = len(unique_vendors_map)

        logger.info(f"Loading taxonomy")
        with log_duration(logger, "Loading taxonomy"):
            taxonomy = load_taxonomy() # Can raise exceptions
        logger.info(f"Taxonomy loaded",
                    extra={"taxonomy_version": taxonomy.version})

        # Initialize the results dict structure before passing to process_vendors
        results_dict = {vendor_name: {} for vendor_name in unique_vendors_map.keys()}

        logger.info(f"Starting vendor classification process by calling classification_logic.process_vendors up to Level {target_level}")
        # --- Call the refactored logic, passing target_level ---
        # process_vendors will populate the results_dict in place
        await process_vendors(
            unique_vendors_map=unique_vendors_map,
            taxonomy=taxonomy,
            results=results_dict, # Pass the dict to be populated
            stats=stats,
            job=job,
            db=db,
            llm_service=llm_service,
            search_service=search_service,
            target_level=target_level # Pass the target level
        )
        # --- End call to refactored logic ---
        logger.info(f"Vendor classification process completed (returned from classification_logic.process_vendors)")

        logger.info("Starting result generation phase.")

        job.current_stage = ProcessingStage.RESULT_GENERATION.value
        job.progress = 0.98 # Progress after all classification/search
        logger.info(f"[_process_vendor_file_async] Committing status update before result generation: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        output_file_name = None # Initialize

        # --- Process results for DB Storage ---
        try:
            logger.info("Processing detailed results for database storage.")
            with log_duration(logger, "Processing detailed results"):
                 # --- UPDATED: Call the preparation function ---
                 detailed_results_for_db = _prepare_detailed_results_for_storage(results_dict, target_level)
                 # --- END UPDATED ---
            logger.info(f"Processed {len(detailed_results_for_db)} items for detailed results storage.")
        except Exception as proc_err:
            logger.error("Failed during detailed results processing for DB", exc_info=True)
            # Continue to generate Excel, but log the error. The job won't store detailed results.
            detailed_results_for_db = None # Ensure it's None if processing failed
        # --- End Process results for DB Storage ---

        # --- Generate Excel File ---
        try:
                logger.info(f"Generating output file")
                with log_duration(logger, "Generating output file"):
                    # Pass the original complex results_dict to generate_output_file
                    # generate_output_file needs to be updated if its logic depends on the old flattened structure
                    # For now, assume it can handle the complex results_dict or adapt it internally
                    output_file_name = generate_output_file(normalized_vendors_data, results_dict, job_id)
                logger.info(f"Output file generated", extra={"output_file": output_file_name})
        except Exception as gen_err:
                logger.error("Failed during output file generation", exc_info=True)
                job.fail(f"Failed to generate output file: {str(gen_err)}")
                db.commit()
                return # Stop processing
        # --- End Generate Excel File ---

        # --- Finalize stats ---
        end_time = datetime.now()
        processing_duration = (end_time - datetime.fromisoformat(stats["start_time"])).total_seconds()
        stats["end_time"] = end_time.isoformat()
        stats["processing_duration_seconds"] = round(processing_duration, 2)
        # Cost calculation remains the same
        cost_input_per_1k = 0.0005
        cost_output_per_1k = 0.0015
        estimated_cost = (stats["api_usage"]["openrouter_prompt_tokens"] / 1000) * cost_input_per_1k + \
                            (stats["api_usage"]["openrouter_completion_tokens"] / 1000) * cost_output_per_1k
        estimated_cost += (stats["api_usage"]["tavily_search_calls"] / 1000) * 4.0
        stats["api_usage"]["cost_estimate_usd"] = round(estimated_cost, 4)
        # --- End Finalize stats ---

        # --- Final Commit Block ---
        try:
            logger.info("Attempting final job completion update in database.")
            # --- UPDATED: Pass the processed detailed_results_for_db to the complete method ---
            job.complete(output_file_name, stats, detailed_results_for_db)
            # --- END UPDATED ---
            job.progress = 1.0 # Ensure progress is 1.0 on completion
            logger.info(f"[_process_vendor_file_async] Committing final job completion status.")
            db.commit()
            logger.info(f"Job completed successfully",
                        extra={
                            "processing_duration": processing_duration,
                            "output_file": output_file_name,
                            "target_level": target_level,
                            # --- UPDATED: Log if detailed results were stored ---
                            "detailed_results_stored": bool(detailed_results_for_db),
                            "detailed_results_count": len(detailed_results_for_db) if detailed_results_for_db else 0,
                            # --- END UPDATED ---
                            "openrouter_calls": stats["api_usage"]["openrouter_calls"],
                            "tokens_used": stats["api_usage"]["openrouter_total_tokens"],
                            "tavily_calls": stats["api_usage"]["tavily_search_calls"],
                            "estimated_cost": stats["api_usage"]["cost_estimate_usd"],
                            "invalid_category_errors": stats.get("invalid_category_errors", 0),
                            "successfully_classified_l5_total": stats.get("successfully_classified_l5", 0)
                        })
        except Exception as final_commit_err:
            logger.error("CRITICAL: Failed to commit final job completion status!", exc_info=True)
            db.rollback()
            try:
                # Re-fetch job in new session to attempt marking as failed
                db_fail_final = SessionLocal()
                job_fail_final = db_fail_final.query(Job).filter(Job.id == job_id).first()
                if job_fail_final:
                    err_msg = f"Failed during final commit: {type(final_commit_err).__name__}: {str(final_commit_err)}"
                    job_fail_final.fail(err_msg[:2000])
                    db_fail_final.commit()
                else:
                    logger.error("Job not found when trying to mark as failed after final commit error.")
                db_fail_final.close()
            except Exception as fail_err:
                logger.error("CRITICAL: Also failed to mark job as failed after final commit error.", exc_info=fail_err)
                # db.rollback() # Already rolled back original session
        # --- End Final Commit Block ---

    except (ValueError, FileNotFoundError, IOError) as file_err:
        logger.error(f"[_process_vendor_file_async] File reading or writing error", exc_info=True,
                    extra={"error": str(file_err)})
        if job:
            err_msg = f"File processing error: {type(file_err).__name__}: {str(file_err)}"
            job.fail(err_msg[:2000])
            db.commit()
        else:
            logger.error("Job object was None during file error handling.")
    except SQLAlchemyError as db_err:
        logger.error(f"[_process_vendor_file_async] Database error during processing", exc_info=True,
                    extra={"error": str(db_err)})
        db.rollback() # Rollback on DB error
        if job:
            # Re-fetch job in new session to attempt marking as failed
            db_fail_db = SessionLocal()
            job_fail_db = db_fail_db.query(Job).filter(Job.id == job_id).first()
            if job_fail_db and job_fail_db.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                    err_msg = f"Database error: {type(db_err).__name__}: {str(db_err)}"
                    job_fail_db.fail(err_msg[:2000])
                    db_fail_db.commit()
            elif job_fail_db:
                    logger.warning(f"Database error occurred but job status was already {job_fail_db.status}. Error: {db_err}")
            else:
                logger.error("Job not found when trying to mark as failed after database error.")
            db_fail_db.close()
        else:
            logger.error("Job object was None during database error handling.")
    except Exception as async_err:
        logger.error(f"[_process_vendor_file_async] Unexpected error during async processing", exc_info=True,
                    extra={"error": str(async_err)})
        db.rollback() # Rollback on unexpected error
        if job:
            # Re-fetch job in new session to attempt marking as failed
            db_fail_unexpected = SessionLocal()
            job_fail_unexpected = db_fail_unexpected.query(Job).filter(Job.id == job_id).first()
            if job_fail_unexpected and job_fail_unexpected.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                err_msg = f"Unexpected error: {type(async_err).__name__}: {str(async_err)}"
                job_fail_unexpected.fail(err_msg[:2000])
                db_fail_unexpected.commit()
            elif job_fail_unexpected:
                logger.warning(f"Unexpected error occurred but job status was already {job_fail_unexpected.status}. Error: {async_err}")
            else:
                logger.error("Job not found when trying to mark as failed after unexpected error.")
            db_fail_unexpected.close()
        else:
            logger.error("Job object was None during unexpected error handling.")
    finally:
        logger.info(f"[_process_vendor_file_async] Finished async processing for job {job_id}")


# --- ADDED: Reclassification Task ---
@shared_task(bind=True)
def reclassify_flagged_vendors_task(self, review_job_id: str):
    """
    Celery task entry point for re-classifying flagged vendors (REVIEW job type).
    Orchestrates the reclassification process.

    Args:
        review_job_id: The ID of the REVIEW job.
    """
    task_id = self.request.id if self.request and self.request.id else "UnknownTaskID"
    logger.info(f"***** reclassify_flagged_vendors_task TASK RECEIVED *****",
                extra={"celery_task_id": task_id, "review_job_id": review_job_id})

    set_correlation_id(review_job_id) # Use review job ID as correlation ID
    set_job_id(review_job_id)
    set_log_context({"job_type": JobType.REVIEW.value})
    logger.info(f"Starting reclassification task", extra={"review_job_id": review_job_id})

    # Initialize loop within the task context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    logger.debug(f"Created and set new asyncio event loop for review job {review_job_id}")

    db = SessionLocal()
    review_job = None

    try:
        review_job = db.query(Job).filter(Job.id == review_job_id).first()
        if not review_job:
            logger.error("Review job not found in database at start of task!", extra={"review_job_id": review_job_id})
            raise ValueError("Review job not found.")

        # Ensure job type is REVIEW
        if review_job.job_type != JobType.REVIEW.value:
            logger.error(f"reclassify_flagged_vendors_task called for a non-REVIEW job.", extra={"review_job_id": review_job_id, "job_type": review_job.job_type})
            raise ValueError(f"Job {review_job_id} is not a REVIEW job.")

        set_log_context({
            "company_name": review_job.company_name,
            "creator": review_job.created_by,
            "parent_job_id": review_job.parent_job_id
        })
        logger.info(f"Processing review for company", extra={"company": review_job.company_name})

        # --- Call the async reclassification logic ---
        logger.info(f"About to run async reclassification processing for review job {review_job_id}")
        with LogTimer(logger, "Complete reclassification processing", level=logging.INFO, include_in_stats=True):
            loop.run_until_complete(_process_reclassification_async(review_job_id, db))

        logger.info(f"Reclassification processing completed successfully (async part finished)")

    except Exception as e:
        logger.error(f"Error processing reclassification task", exc_info=True, extra={"review_job_id": review_job_id})
        try:
            # Re-query the job within this exception handler
            db_error_session = SessionLocal()
            try:
                job_in_error = db_error_session.query(Job).filter(Job.id == review_job_id).first()
                if job_in_error:
                    if job_in_error.status != JobStatus.COMPLETED.value:
                        err_msg = f"Reclassification task failed: {type(e).__name__}: {str(e)}"
                        job_in_error.fail(err_msg[:2000])
                        db_error_session.commit()
                        logger.info(f"Review job status updated to failed due to task error", extra={"error": str(e)})
                    else:
                        logger.warning(f"Task error occurred after review job was marked completed, status not changed.", extra={"error": str(e)})
                else:
                    logger.error("Review job not found when trying to mark as failed.", extra={"review_job_id": review_job_id})
            except Exception as db_error:
                logger.error(f"Error updating review job status during task failure handling", exc_info=True,
                            extra={"original_error": str(e), "db_error": str(db_error)})
                db_error_session.rollback()
            finally:
                db_error_session.close()
        except Exception as final_db_error:
            logger.critical(f"CRITICAL: Failed even to handle database update in reclassification task error handler.", exc_info=final_db_error)

    finally:
        if db:
            db.close()
            logger.debug(f"Main database session closed for reclassification task.")
        if loop and not loop.is_closed():
            loop.close()
            logger.debug(f"Event loop closed for reclassification task.")
        clear_all_context()
        logger.info(f"***** reclassify_flagged_vendors_task TASK FINISHED *****", extra={"review_job_id": review_job_id})


async def _process_reclassification_async(review_job_id: str, db: Session):
    """
    Asynchronous part of the reclassification task.
    Sets up services, calls the core reclassification logic, stores results.
    """
    logger.info(f"[_process_reclassification_async] Starting async processing for review job {review_job_id}")

    llm_service = LLMService()
    # search_service is not needed for reclassification

    review_job = db.query(Job).filter(Job.id == review_job_id).first()
    if not review_job:
        logger.error(f"[_process_reclassification_async] Review job not found in database", extra={"review_job_id": review_job_id})
        return

    # Import the core logic function here to avoid circular imports at module level
    from .reclassification_logic import process_reclassification

    review_results_list = None
    final_stats = {}

    try:
        review_job.status = JobStatus.PROCESSING.value
        review_job.current_stage = ProcessingStage.RECLASSIFICATION.value
        review_job.progress = 0.1 # Start progress
        logger.info(f"[_process_reclassification_async] Committing initial status update: {review_job.status}, {review_job.current_stage}, {review_job.progress}")
        db.commit()
        logger.info(f"Review job status updated",
                    extra={"status": review_job.status, "stage": review_job.current_stage, "progress": review_job.progress})

        # --- Call the reclassification logic ---
        # This function will handle fetching parent data, calling LLM, etc.
        review_results_list, final_stats = await process_reclassification(
            review_job=review_job,
            db=db,
            llm_service=llm_service
        )
        # --- End call ---

        logger.info(f"Reclassification logic completed. Processed {final_stats.get('total_items_processed', 0)} items.")
        review_job.progress = 0.95 # Mark logic as complete

        # --- Final Commit Block ---
        try:
            logger.info("Attempting final review job completion update in database.")
            # Pass None for output_file_name as review jobs don't generate one
            review_job.complete(output_file_name=None, stats=final_stats, detailed_results=review_results_list)
            review_job.progress = 1.0
            logger.info(f"[_process_reclassification_async] Committing final review job completion status.")
            db.commit()
            logger.info(f"Review job completed successfully",
                        extra={
                            "processing_duration": final_stats.get("processing_duration_seconds"),
                            "items_processed": final_stats.get("total_items_processed"),
                            "successful": final_stats.get("successful_reclassifications"),
                            "failed": final_stats.get("failed_reclassifications"),
                            "openrouter_calls": final_stats.get("api_usage", {}).get("openrouter_calls"),
                            "tokens_used": final_stats.get("api_usage", {}).get("openrouter_total_tokens"),
                            "estimated_cost": final_stats.get("api_usage", {}).get("cost_estimate_usd")
                        })
        except Exception as final_commit_err:
            logger.error("CRITICAL: Failed to commit final review job completion status!", exc_info=True)
            db.rollback()
            # Attempt to mark as failed (similar logic as in main task handler)
            try:
                db_fail_final = SessionLocal()
                job_fail_final = db_fail_final.query(Job).filter(Job.id == review_job_id).first()
                if job_fail_final:
                    err_msg = f"Failed during final commit: {type(final_commit_err).__name__}: {str(final_commit_err)}"
                    job_fail_final.fail(err_msg[:2000])
                    db_fail_final.commit()
                db_fail_final.close()
            except Exception as fail_err:
                logger.error("CRITICAL: Also failed to mark review job as failed after final commit error.", exc_info=fail_err)
        # --- End Final Commit Block ---

    # Handle specific errors from process_reclassification or other issues
    except (ValueError, FileNotFoundError) as logic_err:
        logger.error(f"[_process_reclassification_async] Data or File error during reclassification logic", exc_info=True,
                    extra={"error": str(logic_err)})
        if review_job:
            err_msg = f"Reclassification data error: {type(logic_err).__name__}: {str(logic_err)}"
            review_job.fail(err_msg[:2000])
            db.commit()
    except SQLAlchemyError as db_err:
         logger.error(f"[_process_reclassification_async] Database error during reclassification processing", exc_info=True,
                     extra={"error": str(db_err)})
         db.rollback()
         # Attempt to mark as failed (similar logic as in main task handler)
         try:
            db_fail_db = SessionLocal()
            job_fail_db = db_fail_db.query(Job).filter(Job.id == review_job_id).first()
            if job_fail_db and job_fail_db.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                 err_msg = f"Database error: {type(db_err).__name__}: {str(db_err)}"
                 job_fail_db.fail(err_msg[:2000])
                 db_fail_db.commit()
            db_fail_db.close()
         except Exception as fail_err:
            logger.error("CRITICAL: Also failed to mark review job as failed after database error.", exc_info=fail_err)

    except Exception as async_err:
        logger.error(f"[_process_reclassification_async] Unexpected error during async reclassification processing", exc_info=True,
                    extra={"error": str(async_err)})
        db.rollback()
        # Attempt to mark as failed (similar logic as in main task handler)
        try:
            db_fail_unexpected = SessionLocal()
            job_fail_unexpected = db_fail_unexpected.query(Job).filter(Job.id == review_job_id).first()
            if job_fail_unexpected and job_fail_unexpected.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                err_msg = f"Unexpected error: {type(async_err).__name__}: {str(async_err)}"
                job_fail_unexpected.fail(err_msg[:2000])
                db_fail_unexpected.commit()
            db_fail_unexpected.close()
        except Exception as fail_err:
            logger.error("CRITICAL: Also failed to mark review job as failed after unexpected error.", exc_info=fail_err)
    finally:
        logger.info(f"[_process_reclassification_async] Finished async processing for review job {review_job_id}")

# --- END ADDED ---
</file>

<file path='app/tasks/reclassification_logic.py'>
# app/tasks/reclassification_logic.py
import os # Keep the import
import asyncio
import logging
from datetime import datetime
from sqlalchemy.orm import Session
from typing import Dict, Any, List, Tuple, Optional

from core.database import SessionLocal # Assuming SessionLocal might be needed
from core.logging_config import get_logger
from core.log_context import set_log_context, clear_log_context # Assuming context might be used
from utils.log_utils import LogTimer, log_duration # Assuming logging utils might be used
from utils.taxonomy_loader import load_taxonomy, Taxonomy # Assuming taxonomy is needed
from models.job import Job, JobStatus, ProcessingStage, JobType # Assuming Job model is needed
from services.llm_service import LLMService # Assuming LLM service is needed
from schemas.review import ReclassifyRequestItem, ReviewResultItem # Assuming review schemas are needed
from schemas.job import JobResultItem # Assuming job result schema is needed for structure

# Import classification prompts if needed for reclassification
from .reclassification_prompts import generate_reclassification_prompt # Example: Assuming a specific prompt exists

logger = get_logger("vendor_classification.reclassification_logic")

async def process_reclassification(
    review_job: Job,
    db: Session,
    llm_service: LLMService
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Processes the reclassification request based on the review job details.
    Fetches original results, applies hints using LLM, and generates new results.
    """
    logger.info(f"Starting reclassification logic for review job {review_job.id}")
    set_log_context({"review_job_id": review_job.id, "parent_job_id": review_job.parent_job_id})

    start_time = datetime.now()
    # Initialize stats structure similar to classification, but focused on review
    final_stats: Dict[str, Any] = {
        "job_id": review_job.id,
        "parent_job_id": review_job.parent_job_id,
        "company_name": review_job.company_name,
        "target_level": review_job.target_level,
        "start_time": start_time.isoformat(),
        "end_time": None,
        "processing_duration_seconds": None,
        "total_items_processed": 0,
        "successful_reclassifications": 0,
        "failed_reclassifications": 0,
        "api_usage": {
            "openrouter_calls": 0,
            "openrouter_prompt_tokens": 0,
            "openrouter_completion_tokens": 0,
            "openrouter_total_tokens": 0,
            "tavily_search_calls": 0, # Should remain 0
            "cost_estimate_usd": 0.0
        }
    }
    review_results_list: List[Dict[str, Any]] = [] # Holds ReviewResultItem dicts

    try:
        # 1. Validate Input
        if not review_job or not review_job.parent_job_id or not review_job.stats or 'reclassify_input' not in review_job.stats:
            logger.error("Review job is missing parent job ID or input hints.", extra={"job_stats": review_job.stats if review_job else None})
            raise ValueError("Review job is missing parent job ID or input hints.")

        # --- FIX: Handle potential non-list or invalid items in reclassify_input ---
        input_items = review_job.stats['reclassify_input']
        items_to_reclassify: List[ReclassifyRequestItem] = []
        if isinstance(input_items, list):
            for item in input_items:
                try:
                    # Validate each item conforms to the Pydantic model
                    items_to_reclassify.append(ReclassifyRequestItem.model_validate(item))
                except Exception as item_validation_err:
                    logger.warning(f"Skipping invalid item in reclassify_input: {item}. Error: {item_validation_err}", exc_info=False)
        else:
            logger.error(f"reclassify_input in job stats is not a list: {type(input_items)}", extra={"job_id": review_job.id})
            raise ValueError("Invalid format for reclassify_input in job stats.")
        # --- END FIX ---

        final_stats["total_items_processed"] = len(items_to_reclassify)
        if not items_to_reclassify:
             logger.warning("No valid items found in reclassify_input stats.")
             # Complete the job successfully but with 0 items processed
             end_time = datetime.now()
             final_stats["end_time"] = end_time.isoformat()
             final_stats["processing_duration_seconds"] = (end_time - start_time).total_seconds()
             return [], final_stats # Return empty results and stats

        logger.info(f"Found {len(items_to_reclassify)} items to reclassify.")

        # 2. Fetch Parent Job's Detailed Results
        # Use a separate session or ensure the passed `db` session is robust
        parent_job = db.query(Job).filter(Job.id == review_job.parent_job_id).first()
        if not parent_job:
             logger.error(f"Parent job {review_job.parent_job_id} not found.")
             raise ValueError(f"Parent job {review_job.parent_job_id} not found.")
        if not parent_job.detailed_results:
             logger.error(f"Parent job {review_job.parent_job_id} has no detailed results.")
             # Decide how to handle this: fail or process items as failures?
             raise ValueError(f"Parent job {review_job.parent_job_id} has no detailed results.")

        original_results_map: Dict[str, JobResultItem] = {}
        try:
            for item_dict in parent_job.detailed_results:
                 # Validate each item conforms to JobResultItem before adding
                 validated_item = JobResultItem.model_validate(item_dict)
                 original_results_map[validated_item.vendor_name] = validated_item
        except Exception as validation_err:
             logger.error(f"Error validating original results from parent job {parent_job.id}", exc_info=True)
             raise ValueError("Failed to parse original results from parent job.")

        logger.info(f"Loaded {len(original_results_map)} original results from parent job.")

        # 3. Load Taxonomy
        taxonomy = load_taxonomy()

        # 4. Iterate and Reclassify each item
        update_interval = max(1, len(items_to_reclassify) // 10)
        processed_count = 0

        # Prepare batch for LLM if applicable (might call LLM per item or in batches)
        # For simplicity, let's assume one call per item for now
        for item_data in items_to_reclassify:
            vendor_name = item_data.vendor_name
            hint = item_data.hint
            logger.info(f"Reclassifying vendor: '{vendor_name}' with hint: '{hint}'")
            set_log_context({"current_vendor": vendor_name}) # Add vendor to context

            original_result_model = original_results_map.get(vendor_name)
            if not original_result_model:
                logger.warning(f"Vendor '{vendor_name}' from review request not found in parent job results. Skipping.")
                final_stats["failed_reclassifications"] += 1
                # Create a failure entry for this item
                failed_result_item = ReviewResultItem(
                    vendor_name=vendor_name,
                    hint=hint,
                    original_result={"error": "Original result not found in parent job"}, # Indicate error source
                    new_result=JobResultItem( # Use JobResultItem for structure consistency
                        vendor_name=vendor_name,
                        level1_id="ERROR", level1_name="ERROR",
                        level2_id=None, level2_name=None, level3_id=None, level3_name=None,
                        level4_id=None, level4_name=None, level5_id=None, level5_name=None,
                        final_confidence=0.0, final_status="Error", classification_source="Review",
                        classification_notes_or_reason="Original result not found in parent job",
                        achieved_level=0
                    ).model_dump()
                )
                review_results_list.append(failed_result_item.model_dump())
                clear_log_context(["current_vendor"])
                continue

            # --- Actual Reclassification LLM Call ---
            new_result_model = None
            try:
                # Fetch original vendor data (assuming it's stored appropriately or derivable)
                # For this example, let's assume original data might be part of the original_result_model
                # or needs fetching separately. We'll use a placeholder if not readily available.
                # A better approach would be to ensure the parent job stores original vendor input data.
                original_vendor_input_data = original_result_model.model_dump() # Use the stored result as a proxy for now
                original_vendor_input_data['vendor_name'] = vendor_name # Ensure name is correct

                # Generate the specific prompt for reclassification
                prompt = generate_reclassification_prompt(
                    original_vendor_data=original_vendor_input_data, # Pass original data
                    user_hint=hint,
                    original_classification=original_result_model.model_dump(), # Pass previous result dict
                    taxonomy=taxonomy, # Pass the whole taxonomy object
                    target_level=review_job.target_level, # Pass the target level for reclassification
                    attempt_id=f"{review_job.id}-{vendor_name}" # Create a unique ID for this attempt
                )

                logger.debug(f"Generated reclassification prompt for '{vendor_name}'") # Prompt content logged by LLM service now

                # --- UPDATED: Call the new LLM service method ---
                # Pass the nested api_usage dict directly for updates
                parsed_llm_output = await llm_service.call_llm_with_prompt(
                    prompt=prompt,
                    stats_dict=final_stats["api_usage"], # Pass the nested dict
                    job_id=review_job.id, # Pass review job ID for logging/cache key
                    max_tokens=1024 # Adjust if needed for reclassification output size
                )
                # --- END UPDATED ---

                # Remove the old parse call:
                # parsed_llm_output = llm_service.parse_json_response(llm_response_str, ...)

                logger.debug(f"Parsed LLM response dictionary for '{vendor_name}': {parsed_llm_output}")

                # Extract the classification result for this vendor
                # The prompt asks for a specific JSON output format, so parse that
                # Expecting format like: {"batch_id": "...", "level": N, "classifications": [...]}
                # Or potentially just the classification dict directly if the prompt asks for single output
                llm_output_data = None # Initialize
                if parsed_llm_output and isinstance(parsed_llm_output, dict):
                    # Check if the response has the expected 'classifications' list structure
                    if "classifications" in parsed_llm_output and isinstance(parsed_llm_output["classifications"], list) and len(parsed_llm_output["classifications"]) > 0:
                        llm_output_data = parsed_llm_output["classifications"][0]
                        logger.debug(f"Extracted classification data from 'classifications' list for '{vendor_name}': {llm_output_data}")
                    # Fallback: Check if the root object itself looks like the classification structure
                    elif "vendor_name" in parsed_llm_output and "level1" in parsed_llm_output:
                         llm_output_data = parsed_llm_output
                         logger.debug(f"Using root LLM response object as classification data for '{vendor_name}': {llm_output_data}")
                    else:
                        logger.warning(f"Parsed LLM output for '{vendor_name}' does not match expected structures ('classifications' list or direct result). Output: {parsed_llm_output}")
                else:
                     logger.warning(f"Failed to get valid dictionary from LLM response for '{vendor_name}'. Response: {parsed_llm_output}")


                # --- More robust check for L1 data from parsed output ---
                l1_data = llm_output_data.get("level1") if llm_output_data else None
                l1_category_id = l1_data.get("category_id") if l1_data else None
                l1_category_name = l1_data.get("category_name") if l1_data else None
                classification_not_possible = l1_data.get("classification_not_possible", True) if l1_data else True

                # Check if we got valid L1 classification data
                if l1_data and l1_category_id and l1_category_name and l1_category_id not in ["ERROR", "N/A"] and not classification_not_possible:
                     # --- Recursive Classification based on Hint ---
                     achieved_level = 0
                     final_confidence = 0.0
                     final_notes = None
                     final_status = "Not Possible" # Default

                     new_result_data = {
                         "vendor_name": vendor_name,
                         "level1_id": None, "level1_name": None,
                         "level2_id": None, "level2_name": None,
                         "level3_id": None, "level3_name": None,
                         "level4_id": None, "level4_name": None,
                         "level5_id": None, "level5_name": None,
                         "final_confidence": 0.0,
                         "final_status": "Not Possible",
                         "classification_source": "Review", # Mark as reviewed
                         "classification_notes_or_reason": None,
                         "achieved_level": 0
                     }

                     # Populate levels from LLM output
                     for level in range(1, review_job.target_level + 1):
                         level_key = f"level{level}"
                         level_data = llm_output_data.get(level_key)
                         if level_data and isinstance(level_data, dict):
                             cat_id = level_data.get("category_id")
                             cat_name = level_data.get("category_name")
                             level_not_possible = level_data.get("classification_not_possible", True)

                             if cat_id and cat_name and cat_id not in ["N/A", "ERROR"] and not level_not_possible:
                                 new_result_data[f"level{level}_id"] = cat_id
                                 new_result_data[f"level{level}_name"] = cat_name
                                 achieved_level = level
                                 final_confidence = level_data.get("confidence", 0.0)
                                 final_notes = level_data.get("notes") # Store notes from the deepest successful level
                                 final_status = "Classified"
                             else:
                                 # Stop populating further levels if this one failed or wasn't possible
                                 if achieved_level == 0 and level == 1: # Capture reason from L1 failure
                                     final_notes = level_data.get("classification_not_possible_reason") or level_data.get("notes")
                                 break # Stop processing levels for this vendor
                         else:
                             # Stop if expected level data is missing
                             if achieved_level == 0 and level == 1:
                                 final_notes = "LLM response missing Level 1 data."
                             break

                     # Finalize the result item
                     new_result_data["achieved_level"] = achieved_level
                     new_result_data["final_status"] = final_status
                     new_result_data["final_confidence"] = final_confidence
                     new_result_data["classification_notes_or_reason"] = final_notes or f"Reclassified based on hint: {hint}"


                     new_result_model = JobResultItem(**new_result_data)
                     final_stats["successful_reclassifications"] += 1
                     logger.info(f"Successfully reclassified '{vendor_name}' to Level {achieved_level}: '{new_result_model.level1_name}{' -> ' + new_result_model.level2_name if achieved_level > 1 else ''}...'.")

                else:
                     # Handle classification_not_possible or ERROR from LLM or missing L1 data
                     reason = "LLM response did not include valid Level 1 data." # Default reason
                     if l1_data:
                         if l1_category_id in ["ERROR", "N/A"]:
                             reason = l1_data.get("classification_not_possible_reason", f"LLM returned '{l1_category_id}' for Level 1")
                         elif classification_not_possible:
                             reason = l1_data.get("classification_not_possible_reason", "LLM indicated Level 1 classification not possible")
                         elif not l1_category_id or not l1_category_name:
                             reason = "LLM response missing Level 1 category_id or category_name."
                     elif llm_output_data is None:
                         # Reason is based on why llm_output_data became None earlier
                         if parsed_llm_output is None:
                             reason = "LLM call failed or response parsing failed."
                         else:
                             reason = "LLM response structure did not match expected format."
                     else: # llm_output_data exists but no l1_data
                         reason = "LLM response missing 'level1' field."


                     logger.warning(f"LLM could not reclassify '{vendor_name}' with hint. Reason: {reason}")
                     final_stats["failed_reclassifications"] += 1
                     new_result_model = JobResultItem(
                         vendor_name=vendor_name,
                         level1_id=None, level1_name=None, # Or ERROR if appropriate
                         level2_id=None, level2_name=None, level3_id=None, level3_name=None,
                         level4_id=None, level4_name=None, level5_id=None, level5_name=None,
                         final_confidence=0.0,
                         final_status="Not Possible", # Or "Error"
                         classification_source="Review",
                         classification_notes_or_reason=f"Reclassification failed: {reason}", # Store the specific reason
                         achieved_level=0
                     )

            except Exception as llm_err:
                logger.error(f"Error during LLM reclassification call or processing for '{vendor_name}'", exc_info=True)
                final_stats["failed_reclassifications"] += 1
                new_result_model = JobResultItem( # Create error result
                    vendor_name=vendor_name,
                    level1_id="ERROR", level1_name="ERROR",
                    level2_id=None, level2_name=None, level3_id=None, level3_name=None,
                    level4_id=None, level4_name=None, level5_id=None, level5_name=None,
                    final_confidence=0.0, final_status="Error", classification_source="Review",
                    classification_notes_or_reason=f"Error during reclassification processing: {llm_err}",
                    achieved_level=0
                )
            # --- End Actual Reclassification LLM Call ---

            # Store the result (original + new)
            review_item = ReviewResultItem(
                vendor_name=vendor_name,
                hint=hint,
                original_result=original_result_model.model_dump(), # Store as dict
                new_result=new_result_model.model_dump() # Store as dict
            )
            review_results_list.append(review_item.model_dump()) # Append the dict representation

            processed_count += 1
            if processed_count % update_interval == 0 or processed_count == len(items_to_reclassify):
                 progress = 0.1 + 0.85 * (processed_count / len(items_to_reclassify))
                 # Use try-except for update_progress as the session might become invalid
                 try:
                     review_job.update_progress(progress=min(0.95, progress), stage=ProcessingStage.RECLASSIFICATION, db_session=db)
                     logger.debug(f"Updated review job progress: {progress:.2f}")
                 except Exception as db_update_err:
                      logger.error("Failed to update job progress during reclassification loop", exc_info=db_update_err)
                      db.rollback() # Rollback potential partial commit within update_progress

            clear_log_context(["current_vendor"]) # Clear vendor from context


        # 5. Finalize Stats
        end_time = datetime.now()
        processing_duration = (end_time - start_time).total_seconds()
        final_stats["end_time"] = end_time.isoformat()
        final_stats["processing_duration_seconds"] = round(processing_duration, 2)
        # Calculate final cost based on accumulated API usage
        cost_input_per_1k = 0.0005
        cost_output_per_1k = 0.0015
        api_usage = final_stats["api_usage"]
        estimated_cost = (api_usage["openrouter_prompt_tokens"] / 1000) * cost_input_per_1k + \
                           (api_usage["openrouter_completion_tokens"] / 1000) * cost_output_per_1k
        # No Tavily cost expected here
        api_usage["cost_estimate_usd"] = round(estimated_cost, 4)

        logger.info(f"Reclassification logic finished for review job {review_job.id}. Results: {final_stats}")

    except Exception as e:
        logger.error(f"Error during reclassification logic for review job {review_job.id}", exc_info=True)
        # Ensure the job is marked as failed by the caller (_process_reclassification_async)
        final_stats["error_message"] = f"Reclassification logic failed: {type(e).__name__}: {str(e)}"
        # Attempt to add an overall error marker to results if possible
        if not review_results_list: # If no results were added yet
             review_results_list.append({"error": final_stats["error_message"]})
        # Return potentially partial results and error stats
        return review_results_list, final_stats
    finally:
        clear_log_context() # Clear job-specific context

    return review_results_list, final_stats
</file>

<file path='app/tasks/reclassification_prompts.py'>
# app/tasks/reclassification_prompts.py
import json
import logging
from typing import Dict, Any, Optional

from models.taxonomy import Taxonomy, TaxonomyCategory
from schemas.job import JobResultItem # To understand the structure of original_result

logger = logging.getLogger("vendor_classification.reclassification_prompts")

def generate_reclassification_prompt(
    original_vendor_data: Dict[str, Any],
    user_hint: str,
    original_classification: Optional[Dict[str, Any]], # Dict matching JobResultItem
    taxonomy: Taxonomy,
    target_level: int, # The target level for this reclassification attempt
    attempt_id: str = "unknown-attempt"
) -> str:
    """
    Create a prompt for re-classifying a single vendor based on original data,
    user hint, and previous classification attempt. Aims for the target_level.
    """
    vendor_name = original_vendor_data.get('vendor_name', 'UnknownVendor')
    logger.debug(f"Generating reclassification prompt for vendor: {vendor_name}",
                extra={"target_level": target_level, "attempt_id": attempt_id})

    # --- Build Original Vendor Data Section ---
    vendor_data_xml = "<original_vendor_data>\n"
    vendor_data_xml += f"  <name>{vendor_name}</name>\n"
    # Include all available fields from the original data
    optional_fields = [
        'example_goods_services', 'address', 'website',
        'internal_category', 'parent_company', 'spend_category'
    ]
    # Map internal keys to XML tags if needed (adjust based on original_vendor_data structure)
    field_map = {
        'example_goods_services': 'example_goods_services',
        'address': 'address',
        'website': 'website',
        'internal_category': 'internal_category',
        'parent_company': 'parent_company',
        'spend_category': 'spend_category',
        # Add mappings if keys in original_vendor_data are different
        'example': 'example_goods_services',
        'vendor_address': 'address',
        'vendor_website': 'website',
    }
    for field_key, xml_tag in field_map.items():
        value = original_vendor_data.get(field_key)
        if value:
            vendor_data_xml += f"  <{xml_tag}>{str(value)[:300]}</{xml_tag}>\n" # Limit length
    vendor_data_xml += "</original_vendor_data>"

    # --- Build User Hint Section ---
    user_hint_xml = f"<user_hint>{user_hint}</user_hint>"

    # --- Build Original Classification Section (Optional but helpful) ---
    original_classification_xml = "<original_classification_attempt>\n"
    if original_classification:
        original_status = original_classification.get('final_status', 'Unknown')
        original_level = original_classification.get('achieved_level', 0)
        original_reason = original_classification.get('classification_notes_or_reason', 'N/A')
        original_classification_xml += f"  <status>{original_status}</status>\n"
        original_classification_xml += f"  <achieved_level>{original_level}</achieved_level>\n"
        original_classification_xml += f"  <reason_or_notes>{original_reason}</reason_or_notes>\n"
        # Include original L1-L5 IDs/Names if available
        for i in range(1, 6):
             id_key = f'level{i}_id'
             name_key = f'level{i}_name'
             cat_id = original_classification.get(id_key)
             cat_name = original_classification.get(name_key)
             if cat_id and cat_name:
                 original_classification_xml += f"  <level_{i}_result id=\"{cat_id}\" name=\"{cat_name}\"/>\n"
    else:
        original_classification_xml += "  <message>No previous classification data available.</message>\n"
    original_classification_xml += "</original_classification_attempt>"

    # --- Define Output Format Section (Standard Classification Result) ---
    # We want the LLM to output the *new* classification in the standard format
    # It needs to perform the hierarchical classification again based on the hint.
    output_format_xml = f"""<output_format>
Respond *only* with a valid JSON object containing the *new* classification result for this vendor, based *primarily* on the <user_hint> and <original_vendor_data>.
The JSON object should represent the full classification attempt up to Level {target_level}, following the standard structure used previously.

json
{{
  "level": {target_level}, // The target level for this reclassification
  "attempt_id": "{attempt_id}", // ID for this specific attempt
  "vendor_name": "{vendor_name}", // Exact vendor name
  "classifications": [ // Array with ONE entry for this vendor
    {{
      "vendor_name": "{vendor_name}", // Vendor name again
      // --- L1 Result ---
      "level1": {{
        "category_id": "string", // L1 ID from taxonomy or "N/A"
        "category_name": "string", // L1 Name or "N/A"
        "confidence": "float", // 0.0-1.0
        "classification_not_possible": "boolean",
        "classification_not_possible_reason": "string | null",
        "notes": "string | null" // Justification based on hint/data
      }},
      // --- L2 Result (if L1 possible and target_level >= 2) ---
      "level2": {{ // Include ONLY if L1 was possible AND target_level >= 2
        "category_id": "string", // L2 ID or "N/A"
        "category_name": "string", // L2 Name or "N/A"
        "confidence": "float",
        "classification_not_possible": "boolean",
        "classification_not_possible_reason": "string | null",
        "notes": "string | null"
      }} // , ... include level3, level4, level5 similarly if possible and target_level allows
      // --- L3 Result (if L2 possible and target_level >= 3) ---
      // --- L4 Result (if L3 possible and target_level >= 4) ---
      // --- L5 Result (if L4 possible and target_level >= 5) ---
    }}
  ]
}}

</output_format>"""

    # --- Assemble Final Prompt ---
    prompt = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy. You are re-evaluating a previous classification based on new user input.</role>

<task>Re-classify the vendor described in `<original_vendor_data>` using the crucial information provided in `<user_hint>`. The previous attempt is in `<original_classification_attempt>` for context. Your goal is to determine the most accurate NAICS classification up to **Level {target_level}** based *primarily* on the user hint combined with the original data.</task>

<instructions>
1.  **Prioritize the `<user_hint>`**. Assume it provides the most accurate context about the vendor's primary business activity for the user's purposes.
2.  Use the `<original_vendor_data>` to supplement the hint if necessary.
3.  Refer to the `<original_classification_attempt>` only for context on why the previous classification might have been incorrect or insufficient. Do not simply repeat the old result unless the hint strongly confirms it.
4.  Perform a hierarchical classification starting from Level 1 up to the target Level {target_level}.
5.  For **each level**:
    a.  Determine the most appropriate category based on the hint and data. Use the provided taxonomy structure (implicitly known or explicitly provided if needed in future versions).
    b.  If a confident classification for the current level is possible, provide the `category_id`, `category_name`, `confidence` (> 0.0), set `classification_not_possible` to `false`, and optionally add `notes`. Proceed to the next level if the target level allows.
    c.  If classification for the current level is **not possible** (due to ambiguity even with the hint, or the hint pointing to an activity outside the available subcategories), set `classification_not_possible` to `true`, `confidence` to `0.0`, provide a `classification_not_possible_reason`, set `category_id`/`category_name` to "N/A", and **stop** the classification process for this vendor (do not include results for subsequent levels).
6.  Structure your response as a **single JSON object** matching the schema in `<output_format>`. Ensure it contains results for all levels attempted up to the point of success or failure.
7.  The output JSON should represent the *new* classification attempt based on the hint.
8.  Respond *only* with the valid JSON object.
</instructions>

{vendor_data_xml}

{user_hint_xml}

{original_classification_xml}

{output_format_xml}
"""
    # Note: This prompt implicitly relies on the LLM having access to the taxonomy structure
    # or being trained on it. For dynamic taxonomies, the relevant category options for each
    # level would need to be injected similar to the original batch prompt.
    # For now, we assume the LLM can infer the hierarchy and valid IDs based on the target level and task.
    # A future enhancement could involve passing the relevant taxonomy branches.

    return prompt
</file>

<file path='frontend/vue_frontend/src/components/JobHistory.vue'>
<template>
    <div class="mt-10 bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200">
        <h4 class="text-xl font-semibold mb-0">Job History</h4>
      </div>
      <div class="p-6 sm:p-8">
        <!-- Loading State -->
        <div v-if="historyLoading" class="text-center text-gray-500 py-8">
          <svg class="animate-spin inline-block h-6 w-6 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
          <span>Loading job history...</span>
        </div>
  
        <!-- Error State -->
        <div v-else-if="historyError" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
          <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
          <span>Error loading history: {{ historyError }}</span>
        </div>
  
        <!-- Empty State -->
        <div v-else-if="!jobHistory || jobHistory.length === 0" class="text-center text-gray-500 py-8">
          <p>No job history found.</p>
          <p class="text-sm">Upload a file to start your first job.</p>
        </div>
  
        <!-- History Table -->
        <div v-else class="overflow-x-auto">
          <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
              <tr>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Job ID
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Company
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Status
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Created
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Actions
                </th>
              </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
              <tr v-for="job in jobHistory" :key="job.id" class="hover:bg-gray-50 cursor-pointer" @click="selectJob(job.id)">
                <td class="px-4 py-3 whitespace-nowrap text-xs font-mono text-gray-700">
                  {{ job.id.substring(0, 8) }}...
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-800 font-medium">
                  {{ job.company_name }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap">
                  <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="getStatusBadgeClass(job.status)">
                    {{ job.status }}
                  </span>
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-500">
                  {{ formatDateTime(job.created_at) }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-right text-sm font-medium">
                  <button
                    v-if="job.status === 'completed'"
                    @click.stop="downloadResults(job.id, $event)"
                    :disabled="isDownloadLoading(job.id)"
                    class="text-primary hover:text-primary-hover disabled:opacity-50 disabled:cursor-not-allowed inline-flex items-center"
                    title="Download Results"
                  >
                     <svg v-if="isDownloadLoading(job.id)" class="animate-spin h-4 w-4 text-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                     </svg>
                     <ArrowDownTrayIcon v-else class="h-4 w-4" />
                    <!-- <span class="ml-1">Download</span> -->
                  </button>
                  <span v-else-if="job.status === 'failed'" class="text-red-500 text-xs italic" title="Job Failed">Failed</span>
                  <span v-else class="text-gray-400 text-xs italic" title="Processing or Pending">In Progress</span>
                  <!-- Add View Details button if needed -->
                  <!-- <button @click.stop="selectJob(job.id)" class="text-indigo-600 hover:text-indigo-900 ml-3">View</button> -->
                </td>
              </tr>
            </tbody>
          </table>
        </div>
  
        <!-- TODO: Add Pagination Controls if needed -->
  
      </div>
    </div>
  </template>
  
  <script setup lang="ts">
  import { computed, onMounted, ref } from 'vue';
  import { useJobStore } from '@/stores/job';
  import { ExclamationTriangleIcon, ArrowDownTrayIcon } from '@heroicons/vue/20/solid';
  import apiService from '@/services/api';
  
  const jobStore = useJobStore();
  
  const jobHistory = computed(() => jobStore.jobHistory);
  const historyLoading = computed(() => jobStore.historyLoading);
  const historyError = computed(() => jobStore.historyError);
  
  // State for managing individual download button loading
  const downloadingJobs = ref<Set<string>>(new Set());
  const downloadErrors = ref<Record<string, string | null>>({});
  
  const isDownloadLoading = (jobId: string) => downloadingJobs.value.has(jobId);
  
  const fetchHistory = async () => {
    await jobStore.fetchJobHistory({ limit: 100 }); // Fetch latest 100 jobs on mount
  };
  
  const selectJob = (jobId: string) => {
    console.log(`JobHistory: Selecting job ${jobId}`);
    jobStore.setCurrentJobId(jobId);
    // Optional: Scroll to the top or to the JobStatus component
    window.scrollTo({ top: 0, behavior: 'smooth' });
  };
  
  const formatDateTime = (isoString: string | null | undefined): string => {
    if (!isoString) return 'N/A';
    try {
      return new Date(isoString).toLocaleString(undefined, {
        year: 'numeric', month: 'short', day: 'numeric',
        hour: 'numeric', minute: '2-digit', hour12: true
      });
    } catch {
      return 'Invalid Date';
    }
  };
  
  const getStatusBadgeClass = (status: string | undefined) => {
    switch (status) {
      case 'completed': return 'bg-green-100 text-green-800';
      case 'failed': return 'bg-red-100 text-red-800';
      case 'processing': return 'bg-blue-100 text-blue-800';
      default: return 'bg-gray-100 text-gray-800';
    }
  };
  
  const downloadResults = async (jobId: string, event: Event) => {
     event.stopPropagation(); // Prevent row click when clicking button
     if (!jobId || downloadingJobs.value.has(jobId)) return;
  
     downloadingJobs.value.add(jobId);
     downloadErrors.value[jobId] = null;
  
    try {
      const { blob, filename } = await apiService.downloadResults(jobId);
      const url = window.URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.style.display = 'none';
      a.href = url;
      a.download = filename;
      document.body.appendChild(a);
      a.click();
      window.URL.revokeObjectURL(url);
      document.body.removeChild(a);
    } catch (error: any) {
      console.error(`Download failed for job ${jobId}:`, error);
      downloadErrors.value[jobId] = `Download failed: ${error.message || 'Error'}`;
      // Optionally clear the error message after a delay
      setTimeout(() => { downloadErrors.value[jobId] = null; }, 5000);
    } finally {
      downloadingJobs.value.delete(jobId);
    }
  };
  
  
  onMounted(() => {
    fetchHistory();
  });
  </script>
  
  <style scoped>
  /* Add specific styles if needed */
  tbody tr:hover {
    background-color: #f9fafb; /* Tailwind gray-50 */
  }
  </style>
</file>

<file path='frontend/vue_frontend/src/components/JobResultsTable.vue'>
<template>
  <div class="mt-8 p-4 sm:p-6 bg-gray-50 rounded-lg border border-gray-200 shadow-inner">
    <h5 class="text-lg font-semibold text-gray-800 mb-4">
      {{ isIntegratedView ? 'Integrated Classification Results' : 'Detailed Classification Results' }}
    </h5>
    <p v-if="isIntegratedView" class="text-sm text-gray-600 mb-4">
      Showing original classification results alongside the latest reviewed results (if available). Target level: **Level {{ targetLevel }}**.
    </p>
     <p v-else class="text-sm text-gray-600 mb-4">
      Target classification level for this job was **Level {{ targetLevel }}**.
    </p>

    <!-- Search Input -->
    <div class="mb-4">
      <label for="results-search" class="sr-only">Search Results</label>
      <div class="relative rounded-md shadow-sm">
         <div class="absolute inset-y-0 left-0 pl-3 flex items-center pointer-events-none">
            <MagnifyingGlassIcon class="h-5 w-5 text-gray-400" aria-hidden="true" />
          </div>
        <input
          type="text"
          id="results-search"
          v-model="searchTerm"
          placeholder="Search Vendor, Category, ID, Hint, Notes..."
          class="block w-full pl-10 pr-3 py-2 border border-gray-300 rounded-md placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm"
        />
      </div>
    </div>

    <!-- Action Buttons (Submit Flags) -->
    <div class="mb-4 text-right" v-if="jobStore.hasFlaggedItems">
        <button
          type="button"
          @click="submitFlags"
          :disabled="jobStore.reclassifyLoading"
          class="inline-flex items-center rounded-md bg-primary px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-primary-dark focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-primary disabled:opacity-50"
        >
          <ArrowPathIcon v-if="jobStore.reclassifyLoading" class="animate-spin -ml-0.5 mr-1.5 h-5 w-5" aria-hidden="true" />
          <PaperAirplaneIcon v-else class="-ml-0.5 mr-1.5 h-5 w-5" aria-hidden="true" />
          Submit {{ jobStore.flaggedForReview.size }} Flag{{ jobStore.flaggedForReview.size !== 1 ? 's' : '' }} for Re-classification
        </button>
         <p v-if="jobStore.reclassifyError" class="text-xs text-red-600 mt-1 text-right">{{ jobStore.reclassifyError }}</p>
    </div>


    <!-- Loading/Error States -->
    <div v-if="loading" class="text-center py-5 text-gray-500">
      <svg class="animate-spin h-6 w-6 text-primary mx-auto" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
      </svg>
      <p class="mt-2 text-sm">Loading detailed results...</p>
    </div>
    <div v-else-if="error" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm">
      Error loading results: {{ error }}
    </div>
    <div v-else-if="!originalResults || originalResults.length === 0" class="text-center py-5 text-gray-500">
      No detailed results found for this job.
    </div>

    <!-- Results Table -->
    <div v-else class="overflow-x-auto border border-gray-200 rounded-md">
      <table class="min-w-full divide-y divide-gray-200">
        <thead class="bg-gray-100">
          <tr>
            <!-- Flag Column (Sticky) -->
            <th scope="col" class="sticky left-0 z-10 bg-gray-100 px-2 py-3 text-center text-xs font-medium text-gray-600 uppercase tracking-wider w-12">Flag</th>
            <!-- Dynamically generate headers -->
            <th v-for="header in dynamicHeaders" :key="header.key"
                scope="col"
                @click="header.sortable ? sortBy(header.key) : null"
                :class="[
                  'px-3 py-3 text-left text-xs font-medium text-gray-600 uppercase tracking-wider whitespace-nowrap',
                   header.sortable ? 'cursor-pointer hover:bg-gray-200' : '',
                   header.sticky ? 'sticky left-[48px] z-10 bg-gray-100' : '', // Adjusted left offset for flag column
                   header.minWidth ? `min-w-[${header.minWidth}]` : '',
                   header.isOriginal && isIntegratedView ? 'bg-blue-50' : '', // Style original columns only in integrated view
                   header.isNew ? 'bg-green-50' : '', // Style new columns
                ]">
              {{ header.label }}
              <SortIcon v-if="header.sortable" :direction="sortKey === header.key ? sortDirection : null" />
            </th>
          </tr>
        </thead>
        <tbody class="bg-white divide-y divide-gray-200">
          <tr v-if="filteredAndSortedItems.length === 0">
            <td :colspan="dynamicHeaders.length + 1" class="px-4 py-4 whitespace-nowrap text-sm text-gray-500 text-center">No results match your search criteria.</td>
          </tr>
          <!-- Iterate through combined/processed items -->
          <tr v-for="(item, index) in filteredAndSortedItems" :key="item.vendor_name + '-' + index" class="hover:bg-gray-50 align-top" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">
             <!-- Flag Button Cell (Sticky) -->
             <td class="sticky left-0 z-10 bg-white px-2 py-2 text-center align-middle" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">
                 <button
                    @click="toggleFlag(item.vendor_name, item.review_hint)"
                    :title="jobStore.isFlagged(item.vendor_name) ? 'Edit hint or remove flag' : 'Flag for re-classification'"
                    class="p-1 rounded-full hover:bg-gray-200 focus:outline-none focus:ring-2 focus:ring-offset-1 focus:ring-primary"
                    :class="jobStore.isFlagged(item.vendor_name) ? 'text-primary' : 'text-gray-400 hover:text-primary-dark'"
                  >
                    <FlagIconSolid v-if="jobStore.isFlagged(item.vendor_name)" class="h-5 w-5" aria-hidden="true" />
                    <FlagIconOutline v-else class="h-5 w-5" aria-hidden="true" />
                    <span class="sr-only">Flag item</span>
                  </button>
             </td>
             <!-- Vendor Name Cell (Sticky) -->
             <td class="sticky left-[48px] z-10 bg-white px-3 py-2 whitespace-nowrap text-sm font-medium text-gray-900" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">{{ item.vendor_name }}</td>

             <!-- Hint Cell (Only in Integrated View) -->
             <td v-if="isIntegratedView" class="px-3 py-2 text-xs text-gray-600 max-w-xs break-words">
                 <span v-if="!jobStore.isFlagged(item.vendor_name)">{{ item.review_hint || '-' }}</span>
                 <!-- Inline Hint Editor when Flagged -->
                 <div v-else>
                    <label :for="'hint-' + index" class="sr-only">Hint for {{ item.vendor_name }}</label>
                    <textarea :id="'hint-' + index"
                              rows="2"
                              :value="jobStore.getHint(item.vendor_name)"
                              @input="updateHint(item.vendor_name, ($event.target as HTMLTextAreaElement).value)"
                              placeholder="Enter hint..."
                              class="block w-full text-xs rounded-md border-gray-300 shadow-sm focus:border-primary focus:ring-primary"
                    ></textarea>
                    <p v-if="!jobStore.getHint(item.vendor_name)" class="text-red-600 text-xs mt-1">Hint required for submission.</p>
                 </div>
             </td>

             <!-- Original Classification Columns -->
             <td class="px-3 py-2 whitespace-nowrap text-xs font-mono" :class="[getCellClass(item.original_result, 1), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level1_id || '-' }}</td>
             <td class="px-3 py-2 text-xs" :class="[getCellClass(item.original_result, 1), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level1_name || '-' }}</td>
             <td class="px-3 py-2 whitespace-nowrap text-xs font-mono" :class="[getCellClass(item.original_result, 2), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level2_id || '-' }}</td>
             <td class="px-3 py-2 text-xs" :class="[getCellClass(item.original_result, 2), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level2_name || '-' }}</td>
             <td class="px-3 py-2 whitespace-nowrap text-xs font-mono" :class="[getCellClass(item.original_result, 3), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level3_id || '-' }}</td>
             <td class="px-3 py-2 text-xs" :class="[getCellClass(item.original_result, 3), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level3_name || '-' }}</td>
             <td class="px-3 py-2 whitespace-nowrap text-xs font-mono" :class="[getCellClass(item.original_result, 4), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level4_id || '-' }}</td>
             <td class="px-3 py-2 text-xs" :class="[getCellClass(item.original_result, 4), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level4_name || '-' }}</td>
             <td class="px-3 py-2 whitespace-nowrap text-xs font-mono" :class="[getCellClass(item.original_result, 5), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level5_id || '-' }}</td>
             <td class="px-3 py-2 text-xs" :class="[getCellClass(item.original_result, 5), isIntegratedView ? 'bg-blue-50' : '']">{{ item.original_result?.level5_name || '-' }}</td>
             <td class="px-3 py-2 whitespace-nowrap text-sm text-center" :class="isIntegratedView ? 'bg-blue-50' : ''">
               <span v-if="item.original_result?.final_confidence !== null && item.original_result?.final_confidence !== undefined"
                     :class="getConfidenceClass(item.original_result.final_confidence)">
                 {{ (item.original_result.final_confidence * 100).toFixed(1) }}%
               </span>
               <span v-else class="text-gray-400 text-xs">N/A</span>
             </td>
             <td class="px-3 py-2 whitespace-nowrap text-xs text-center" :class="isIntegratedView ? 'bg-blue-50' : ''">
                <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                     :class="getStatusClass(item.original_result?.final_status)">
                 {{ item.original_result?.final_status }}
               </span>
             </td>
              <td class="px-3 py-2 whitespace-nowrap text-xs text-center" :class="isIntegratedView ? 'bg-blue-50' : ''">
               <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                     :class="getSourceClass(item.original_result?.classification_source)">
                 {{ item.original_result?.classification_source }}
               </span>
             </td>
             <td class="px-3 py-2 text-xs text-gray-500 max-w-xs break-words" :class="isIntegratedView ? 'bg-blue-50' : ''">
                  <!-- Show hint input if flagged, otherwise original notes -->
                  <div v-if="jobStore.isFlagged(item.vendor_name) && !isIntegratedView">
                     <label :for="'hint-' + index" class="sr-only">Hint for {{ item.vendor_name }}</label>
                     <textarea :id="'hint-' + index"
                               rows="2"
                               :value="jobStore.getHint(item.vendor_name)"
                               @input="updateHint(item.vendor_name, ($event.target as HTMLTextAreaElement).value)"
                               placeholder="Enter hint..."
                               class="block w-full text-xs rounded-md border-gray-300 shadow-sm focus:border-primary focus:ring-primary"
                     ></textarea>
                     <p v-if="!jobStore.getHint(item.vendor_name)" class="text-red-600 text-xs mt-1">Hint required for submission.</p>
                  </div>
                  <span v-else>{{ item.original_result?.classification_notes_or_reason || '-' }}</span>
             </td>

             <!-- New Classification Columns (Only in Integrated View) -->
             <template v-if="isIntegratedView">
                <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 1)">{{ item.new_result?.level1_id || '-' }}</td>
                <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 1)">{{ item.new_result?.level1_name || '-' }}</td>
                <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 2)">{{ item.new_result?.level2_id || '-' }}</td>
                <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 2)">{{ item.new_result?.level2_name || '-' }}</td>
                <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 3)">{{ item.new_result?.level3_id || '-' }}</td>
                <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 3)">{{ item.new_result?.level3_name || '-' }}</td>
                <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 4)">{{ item.new_result?.level4_id || '-' }}</td>
                <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 4)">{{ item.new_result?.level4_name || '-' }}</td>
                <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 5)">{{ item.new_result?.level5_id || '-' }}</td>
                <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 5)">{{ item.new_result?.level5_name || '-' }}</td>
                <td class="px-3 py-2 whitespace-nowrap text-sm text-center bg-green-50">
                  <span v-if="item.new_result?.final_confidence !== null && item.new_result?.final_confidence !== undefined"
                        :class="getConfidenceClass(item.new_result.final_confidence)">
                    {{ (item.new_result.final_confidence * 100).toFixed(1) }}%
                  </span>
                  <span v-else class="text-gray-400 text-xs">N/A</span>
                </td>
                <td class="px-3 py-2 whitespace-nowrap text-xs text-center bg-green-50">
                   <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                        :class="getStatusClass(item.new_result?.final_status)">
                    {{ item.new_result?.final_status }}
                  </span>
                </td>
                 <td class="px-3 py-2 whitespace-nowrap text-xs text-center bg-green-50">
                   <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                        :class="getSourceClass(item.new_result?.classification_source)">
                    {{ item.new_result?.classification_source }}
                  </span>
                 </td>
                <td class="px-3 py-2 text-xs text-gray-500 max-w-xs break-words bg-green-50">
                  {{ item.new_result?.classification_notes_or_reason || '-' }}
                </td>
             </template>
          </tr>
        </tbody>
      </table>
    </div>

     <!-- Row Count -->
    <div class="mt-3 text-xs text-gray-500">
      Showing {{ filteredAndSortedItems.length }} of {{ originalResults?.length || 0 }} results.
    </div>

  </div>
</template>

<script setup lang="ts">
import { ref, computed, type PropType, watch } from 'vue';
import { useJobStore, type JobResultItem, type ReviewResultItem } from '@/stores/job';
import { FlagIcon as FlagIconOutline, MagnifyingGlassIcon, PaperAirplaneIcon, ArrowPathIcon } from '@heroicons/vue/24/outline';
import { FlagIcon as FlagIconSolid, ChevronUpIcon, ChevronDownIcon, ChevronUpDownIcon } from '@heroicons/vue/20/solid';

// --- Define Header Interface ---
interface TableHeader {
  key: string; // Use string for complex/nested keys or combined fields
  label: string;
  sortable: boolean;
  sticky?: boolean;
  minWidth?: string;
  isOriginal?: boolean; // Flag for styling/grouping
  isNew?: boolean;      // Flag for styling/grouping
}

// --- Define Combined Item Interface for internal use ---
interface CombinedResultItem {
    vendor_name: string;
    original_result: JobResultItem;
    review_hint: string | null; // Hint from review job
    new_result: JobResultItem | null; // New result from review job (can be null if not reviewed)
}

// --- Props ---
const props = defineProps({
  // Use JobResultItem[] for original results (always expected for CLASSIFICATION job view)
  originalResults: {
    type: Array as PropType<JobResultItem[] | null>,
    required: true,
  },
  // Use ReviewResultItem[] for related review results (optional)
  reviewResults: {
    type: Array as PropType<ReviewResultItem[] | null>,
    default: null,
  },
  loading: {
    type: Boolean,
    default: false,
  },
  error: {
    type: String as PropType<string | null>,
    default: null,
  },
  targetLevel: {
    type: Number,
    required: true,
  }
});

const emit = defineEmits(['submit-flags']);

// --- Store ---
const jobStore = useJobStore();

// --- Internal State ---
const searchTerm = ref('');
const sortKey = ref<string | null>('vendor_name'); // Default sort key
const sortDirection = ref<'asc' | 'desc' | null>('asc');

// --- Computed Properties ---

const isIntegratedView = computed(() => !!props.reviewResults && props.reviewResults.length > 0);

// Combine original and review results into a single structure for easier iteration/filtering/sorting
const combinedItems = computed((): CombinedResultItem[] => {
    if (!props.originalResults) return [];

    const reviewMap = new Map<string, ReviewResultItem>();
    if (props.reviewResults) {
        props.reviewResults.forEach(reviewItem => {
            reviewMap.set(reviewItem.vendor_name, reviewItem);
        });
    }

    return props.originalResults.map(originalItem => {
        const reviewItem = reviewMap.get(originalItem.vendor_name);
        return {
            vendor_name: originalItem.vendor_name,
            original_result: originalItem,
            review_hint: reviewItem ? reviewItem.hint : null,
            new_result: reviewItem ? reviewItem.new_result : null,
        };
    });
});

// Generate dynamic headers based on whether it's an integrated view
const dynamicHeaders = computed((): TableHeader[] => {
    const baseHeaders: TableHeader[] = [
        { key: 'vendor_name', label: 'Vendor Name', sortable: true, sticky: true, minWidth: '150px' },
    ];

    const originalResultHeaders: TableHeader[] = [
        { key: 'original_result.level1_id', label: 'L1 ID', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.level1_name', label: 'L1 Name', sortable: true, minWidth: '120px', isOriginal: true },
        { key: 'original_result.level2_id', label: 'L2 ID', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.level2_name', label: 'L2 Name', sortable: true, minWidth: '120px', isOriginal: true },
        { key: 'original_result.level3_id', label: 'L3 ID', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.level3_name', label: 'L3 Name', sortable: true, minWidth: '120px', isOriginal: true },
        { key: 'original_result.level4_id', label: 'L4 ID', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.level4_name', label: 'L4 Name', sortable: true, minWidth: '120px', isOriginal: true },
        { key: 'original_result.level5_id', label: 'L5 ID', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.level5_name', label: 'L5 Name', sortable: true, minWidth: '120px', isOriginal: true },
        { key: 'original_result.final_confidence', label: 'Confidence', sortable: true, minWidth: '100px', isOriginal: true },
        { key: 'original_result.final_status', label: 'Status', sortable: true, minWidth: '100px', isOriginal: true },
        { key: 'original_result.classification_source', label: 'Source', sortable: true, minWidth: '80px', isOriginal: true },
        { key: 'original_result.classification_notes_or_reason', label: 'Notes/Reason', sortable: false, minWidth: '200px', isOriginal: true }, // Combined column for original notes
    ];

    const reviewHintHeader: TableHeader = { key: 'review_hint', label: 'User Hint', sortable: true, minWidth: '180px' };

    const newResultHeaders: TableHeader[] = [
        { key: 'new_result.level1_id', label: 'New L1 ID', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.level1_name', label: 'New L1 Name', sortable: true, minWidth: '120px', isNew: true },
        { key: 'new_result.level2_id', label: 'New L2 ID', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.level2_name', label: 'New L2 Name', sortable: true, minWidth: '120px', isNew: true },
        { key: 'new_result.level3_id', label: 'New L3 ID', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.level3_name', label: 'New L3 Name', sortable: true, minWidth: '120px', isNew: true },
        { key: 'new_result.level4_id', label: 'New L4 ID', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.level4_name', label: 'New L4 Name', sortable: true, minWidth: '120px', isNew: true },
        { key: 'new_result.level5_id', label: 'New L5 ID', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.level5_name', label: 'New L5 Name', sortable: true, minWidth: '120px', isNew: true },
        { key: 'new_result.final_confidence', label: 'New Confidence', sortable: true, minWidth: '100px', isNew: true },
        { key: 'new_result.final_status', label: 'New Status', sortable: true, minWidth: '100px', isNew: true },
        { key: 'new_result.classification_source', label: 'New Source', sortable: true, minWidth: '80px', isNew: true },
        { key: 'new_result.classification_notes_or_reason', label: 'New Notes/Reason', sortable: false, minWidth: '200px', isNew: true },
    ];

    if (isIntegratedView.value) {
        // Modify original notes header label
        const origNotesHeader = originalResultHeaders.find(h => h.key === 'original_result.classification_notes_or_reason');
        if (origNotesHeader) origNotesHeader.label = 'Orig Notes/Reason';

        return [
            ...baseHeaders,
            reviewHintHeader, // Add hint column
            ...originalResultHeaders,
            ...newResultHeaders
        ];
    } else {
        // Modify original notes header label and make it the hint column if flagged
        const origNotesHeader = originalResultHeaders.find(h => h.key === 'original_result.classification_notes_or_reason');
        if (origNotesHeader) origNotesHeader.label = 'Hint / Notes / Reason';
        return [
            ...baseHeaders,
            ...originalResultHeaders
        ];
    }
});

// Helper to get nested values for filtering/sorting
const getNestedValue = (obj: any, path: string): any => {
  if (!obj) return null;
  // Handle direct keys first
  if (path.indexOf('.') === -1) {
      return obj[path] ?? null;
  }
  // Handle nested keys
  return path.split('.').reduce((value, key) => (value && value[key] !== undefined && value[key] !== null ? value[key] : null), obj);
};

const filteredAndSortedItems = computed(() => {
  let filtered = combinedItems.value;

  // Filtering
  if (searchTerm.value) {
    const lowerSearchTerm = searchTerm.value.toLowerCase();
    filtered = filtered.filter(item => {
        // Search direct fields
        if (item.vendor_name?.toLowerCase().includes(lowerSearchTerm)) return true;
        if (isIntegratedView.value && item.review_hint?.toLowerCase().includes(lowerSearchTerm)) return true;

        // Search original result fields (using header keys for consistency)
        const originalHeaders = dynamicHeaders.value.filter(h => h.isOriginal);
        for (const header of originalHeaders) {
             const value = getNestedValue(item, header.key);
             if (value && String(value).toLowerCase().includes(lowerSearchTerm)) return true;
        }

        // Search new result fields if integrated view
        if (isIntegratedView.value) {
            const newHeaders = dynamicHeaders.value.filter(h => h.isNew);
            for (const header of newHeaders) {
                const value = getNestedValue(item, header.key);
                if (value && String(value).toLowerCase().includes(lowerSearchTerm)) return true;
            }
        }
        // Search hint if flagged (even in non-integrated view)
        if (jobStore.isFlagged(item.vendor_name) && jobStore.getHint(item.vendor_name)?.toLowerCase().includes(lowerSearchTerm)) return true;

        return false; // No match
    });
  }

  // Sorting
  if (sortKey.value && sortDirection.value) {
    const key = sortKey.value;
    const direction = sortDirection.value === 'asc' ? 1 : -1;

    // Special handling for the hint column in non-integrated view
    const effectiveSortKey = (!isIntegratedView.value && key === 'original_result.classification_notes_or_reason') ? 'hint_or_notes' : key;

    filtered = filtered.slice().sort((a, b) => {
      let valA: any;
      let valB: any;

      if (effectiveSortKey === 'hint_or_notes') {
          valA = jobStore.isFlagged(a.vendor_name) ? jobStore.getHint(a.vendor_name) : a.original_result?.classification_notes_or_reason;
          valB = jobStore.isFlagged(b.vendor_name) ? jobStore.getHint(b.vendor_name) : b.original_result?.classification_notes_or_reason;
      } else {
          valA = getNestedValue(a, key);
          valB = getNestedValue(b, key);
      }

      const aIsNull = valA === null || valA === undefined || valA === '';
      const bIsNull = valB === null || valB === undefined || valB === '';

      if (aIsNull && bIsNull) return 0;
      if (aIsNull) return 1 * direction;
      if (bIsNull) return -1 * direction;

      if (typeof valA === 'string' && typeof valB === 'string') {
        return valA.localeCompare(valB) * direction;
      }
      if (typeof valA === 'number' && typeof valB === 'number') {
        return (valA - valB) * direction;
      }

      const strA = String(valA).toLowerCase();
      const strB = String(valB).toLowerCase();
      if (strA < strB) return -1 * direction;
      if (strA > strB) return 1 * direction;
      return 0;
    });
  }

  return filtered;
});

// --- Methods ---

function sortBy(key: string) {
  if (sortKey.value === key) {
    if (sortDirection.value === 'asc') sortDirection.value = 'desc';
    else if (sortDirection.value === 'desc') {
        sortDirection.value = null; // Cycle to no sort
        sortKey.value = null;
    } else { // Was null
        sortDirection.value = 'asc';
        sortKey.value = key; // Re-apply key
    }
  } else {
    sortKey.value = key;
    sortDirection.value = 'asc';
  }
}

function getConfidenceClass(confidence: number | null | undefined): string {
  if (confidence === null || confidence === undefined) return 'text-gray-400';
  if (confidence >= 0.8) return 'text-green-700 font-medium';
  if (confidence >= 0.5) return 'text-yellow-700';
  return 'text-red-700';
}

function getStatusClass(status: string | null | undefined): string {
    switch(status?.toLowerCase()){
        case 'classified': return 'bg-green-100 text-green-800';
        case 'not possible': return 'bg-yellow-100 text-yellow-800';
        case 'error': return 'bg-red-100 text-red-800';
        default: return 'bg-gray-100 text-gray-800';
    }
}

function getSourceClass(source: string | null | undefined): string {
    switch(source?.toLowerCase()){
        case 'initial': return 'bg-green-100 text-green-800';
        case 'search': return 'bg-blue-100 text-blue-800';
        case 'review': return 'bg-purple-100 text-purple-800'; // Added style for review source
        default: return 'bg-gray-100 text-gray-800';
    }
}

// Highlight cells beyond the target classification depth or if ID is null/empty
function getCellClass(item: JobResultItem | null | undefined, level: number): string {
    const baseClass = 'text-gray-700';
    const beyondDepthClass = 'text-gray-400 italic';
    const nullClass = 'text-gray-400';

    if (!item) return nullClass;

    const levelIdKey = `level${level}_id` as keyof JobResultItem;
    const hasId = item[levelIdKey] !== null && item[levelIdKey] !== undefined && String(item[levelIdKey]).trim() !== '';

    if (!hasId) return nullClass;
    if (level > props.targetLevel) return beyondDepthClass;
    return baseClass;
}

// --- Flagging and Hint Handling ---
function toggleFlag(vendorName: string, reviewHint: string | null) {
    if (jobStore.isFlagged(vendorName)) {
        jobStore.unflagVendor(vendorName);
    } else {
        // Pre-populate hint with the review hint if available in integrated view, otherwise null
        const initialHint = isIntegratedView.value ? reviewHint : null;
        jobStore.flagVendor(vendorName, initialHint);
    }
}

function updateHint(vendorName: string, hint: string) {
    jobStore.setHint(vendorName, hint);
}

async function submitFlags() {
    emit('submit-flags'); // Notify parent
}


// --- Helper Component for Sort Icons ---
const SortIcon = {
  props: {
    direction: {
      type: String as PropType<'asc' | 'desc' | null>,
      default: null,
    },
  },
  components: { ChevronUpIcon, ChevronDownIcon, ChevronUpDownIcon },
  template: `
    <span class="inline-block ml-1 w-4 h-4 align-middle">
      <ChevronUpIcon v-if="direction === 'asc'" class="w-4 h-4 text-gray-700" />
      <ChevronDownIcon v-else-if="direction === 'desc'" class="w-4 h-4 text-gray-700" />
      <ChevronUpDownIcon v-else class="w-4 h-4 text-gray-400 opacity-50" />
    </span>
  `,
};

// Watch for prop changes to potentially clear sort/filter (optional)
watch(() => [props.originalResults, props.reviewResults], () => {
    console.log("Results props changed, resetting sort/filter");
    searchTerm.value = '';
    sortKey.value = 'vendor_name';
    sortDirection.value = 'asc';
});

</script>

<style scoped>
/* Ensure sticky header cells have appropriate background */
thead th.sticky {
  position: sticky;
  background-color: #f3f4f6; /* bg-gray-100 */
}

/* Ensure sticky body cells have appropriate background */
tbody td.sticky {
    position: sticky;
    background-color: inherit; /* Inherit from parent tr */
}
/* Ensure flagged rows inherit sticky background correctly */
tbody tr.bg-indigo-50 td.sticky {
    background-color: #e0e7ff; /* bg-indigo-50 */
}


/* Add slight borders for visual separation in integrated view */
th.isOriginal, td.isOriginal {
    border-left: 1px solid #e5e7eb; /* gray-200 */
}
th.isNew, td.isNew {
    border-left: 1px solid #e5e7eb; /* gray-200 */
}
th:first-child, td:first-child { /* Flag column */
    border-left: none;
}
th:nth-child(2), td:nth-child(2) { /* Vendor name column */
     border-left: none;
}
/* Adjust border for hint column if it's the first after vendor name */
th:nth-child(3), td:nth-child(3) {
    border-left: v-bind("isIntegratedView ? '1px solid #e5e7eb' : 'none'");
}

/* REMOVED empty ruleset */
/* Style for cells beyond requested depth */
/* .text-gray-400.italic { */
    /* background-color: #f9fafb; */
/* } */
</style>
</file>

<file path='frontend/vue_frontend/src/components/JobStatus.vue'>
<template>
    <div v-if="jobStore.currentJobId" class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200 flex justify-between items-center">
        <h4 class="text-xl font-semibold mb-0">Job Status</h4>
         <!-- Link to Parent Job (if this is a Review Job) -->
         <button v-if="jobDetails?.job_type === 'REVIEW' && jobDetails.parent_job_id"
                @click="viewParentJob"
                title="View Original Classification Job"
                class="text-xs inline-flex items-center px-2.5 py-1.5 border border-gray-300 shadow-sm font-medium rounded text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-1 focus:ring-primary">
            <ArrowUturnLeftIcon class="h-4 w-4 mr-1.5 text-gray-500"/>
            View Original Job
        </button>
      </div>
      <div class="p-6 sm:p-8 space-y-6"> <!-- Increased spacing -->

        <!-- Error Message (Polling/General) -->
        <div v-if="errorMessage" class="p-3 bg-yellow-100 border border-yellow-300 text-yellow-800 rounded-md text-sm flex items-center">
            <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-yellow-600 flex-shrink-0"/>
            <span>{{ errorMessage }}</span>
        </div>

        <!-- Reclassification Started Message -->
        <div v-if="showReclassifySuccessMessage && jobStore.lastReviewJobId" class="p-3 bg-green-100 border border-green-300 text-green-800 rounded-md text-sm flex items-center justify-between">
             <div class="flex items-center">
                 <CheckCircleIcon class="h-5 w-5 mr-2 text-green-600 flex-shrink-0"/>
                 <span>Re-classification job started successfully (ID: {{ jobStore.lastReviewJobId }}).</span>
             </div>
             <button @click="viewReviewJob(jobStore.lastReviewJobId!)" class="ml-4 text-xs font-semibold text-green-700 hover:text-green-900 underline">View Review Job</button>
        </div>


        <!-- Job ID & Status Row -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm border-b border-gray-100 pb-4">
          <div>
             <strong class="text-gray-600 block mb-1">Job ID:</strong>
             <span class="text-gray-900 font-mono text-xs bg-gray-100 px-2 py-1 rounded break-all">{{ jobStore.currentJobId }}</span>
             <span v-if="jobDetails?.job_type === 'REVIEW'" class="ml-2 inline-block px-1.5 py-0.5 rounded text-xs font-semibold bg-purple-100 text-purple-800 align-middle">REVIEW</span>
             <span v-else-if="jobDetails?.job_type === 'CLASSIFICATION'" class="ml-2 inline-block px-1.5 py-0.5 rounded text-xs font-semibold bg-blue-100 text-blue-800 align-middle">CLASSIFICATION</span>
          </div>
           <div class="flex items-center space-x-2">
             <strong class="text-gray-600">Status:</strong>
             <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="statusBadgeClass">
                 {{ jobDetails?.status || 'Loading...' }}
             </span>
           </div>
        </div>

        <!-- Stage & Error (if failed) -->
        <div class="text-sm">
            <strong class="text-gray-600 block mb-1">Current Stage:</strong>
            <span class="text-gray-800 font-medium">{{ formattedStage }}</span>
            <div v-if="jobDetails?.status === 'failed' && jobDetails?.error_message" class="mt-3 p-4 bg-red-50 border border-red-200 text-red-800 rounded-md text-xs shadow-sm">
              <strong class="block mb-1 font-semibold">Error Details:</strong>
              <p class="whitespace-pre-wrap">{{ jobDetails.error_message }}</p> <!-- Preserve whitespace -->
            </div>
        </div>

        <!-- Progress Bar -->
        <div>
          <label class="block text-sm font-medium text-gray-600 mb-1.5">Progress:</label>
          <div class="w-full bg-gray-200 rounded-full h-3 overflow-hidden"> <!-- Slimmer progress bar -->
            <div
              class="h-3 rounded-full transition-all duration-500 ease-out"
              :class="progressColorClass"
              :style="{ width: progressPercent + '%' }"
              ></div>
          </div>
          <div class="text-right text-xs text-gray-500 mt-1">{{ progressPercent }}% Complete</div>
        </div>

        <!-- Timestamps Row -->
        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-xs text-gray-500 border-t border-gray-100 pt-5">
            <div>
                 <strong class="block text-gray-600 mb-0.5">Created:</strong>
                 <span>{{ formattedCreatedAt }}</span>
            </div>
            <div>
                 <strong class="block text-gray-600 mb-0.5">Updated:</strong>
                 <span>{{ formattedUpdatedAt }}</span>
            </div>
             <div>
                <strong class="block text-gray-600 mb-0.5">Est. Completion:</strong>
                <span>{{ formattedEstimatedCompletion }}</span>
            </div>
        </div>

        <!-- Notification Section -->
        <div v-if="canRequestNotify" class="pt-5 border-t border-gray-100">
            <label for="notificationEmail" class="block text-sm font-medium text-gray-700 mb-1.5">Get Notified Upon Completion</label>
            <div class="flex flex-col sm:flex-row sm:space-x-2">
                <input type="email"
                       class="mb-2 sm:mb-0 flex-grow block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100"
                       id="notificationEmail"
                       placeholder="Enter your email"
                       v-model="notificationEmail"
                       :disabled="isNotifyLoading" />
                <button
                    type="button"
                    @click="requestNotification"
                    :disabled="isNotifyLoading || !notificationEmail"
                    class="w-full sm:w-auto inline-flex justify-center items-center px-4 py-2 border border-gray-300 rounded-md shadow-sm text-sm font-medium text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed"
                >
                    <svg v-if="isNotifyLoading" class="animate-spin -ml-1 mr-2 h-4 w-4 text-gray-700" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                       <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                       <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                     <EnvelopeIcon v-else class="h-4 w-4 mr-2 -ml-1 text-gray-500"/>
                    {{ isNotifyLoading ? 'Sending...' : 'Notify Me' }}
                </button>
            </div>
            <!-- Notification Feedback -->
            <p v-if="notifyMessage" :class="notifyMessageIsError ? 'text-red-600' : 'text-green-600'" class="mt-2 text-xs">{{ notifyMessage }}</p>
        </div>

        <!-- Download Section (Only for completed CLASSIFICATION jobs) -->
        <div v-if="jobDetails?.status === 'completed' && jobDetails?.job_type === 'CLASSIFICATION'" class="pt-5 border-t border-gray-100">
          <button @click="downloadResults"
                  class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-green-600 hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-green-500 disabled:opacity-50 disabled:cursor-not-allowed"
                  :disabled="isDownloadLoading">
             <svg v-if="isDownloadLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
             </svg>
             <ArrowDownTrayIcon v-else class="h-5 w-5 mr-2 -ml-1" />
            {{ isDownloadLoading ? ' Preparing Download...' : 'Download Excel Results' }}
          </button>
          <p v-if="downloadError" class="mt-2 text-xs text-red-600 text-center">{{ downloadError }}</p>
        </div>

        <!-- Stats Section (Rendered within JobStatus when complete) -->
         <JobStats v-if="jobDetails?.status === 'completed' && jobDetails?.id" :job-id="jobDetails.id" />

        <!-- ***** UPDATED: Conditional Detailed Results Table Section ***** -->
        <!-- Show CLASSIFICATION results table (potentially integrated view) -->
        <JobResultsTable
            v-if="jobDetails?.status === 'completed' && jobDetails?.id && jobDetails?.job_type === 'CLASSIFICATION'"
            :original-results="jobStore.jobResults as JobResultItem[] | null"
            :review-results="jobStore.relatedReviewResults"
            :loading="jobStore.resultsLoading"
            :error="jobStore.resultsError"
            :target-level="jobDetails.target_level || 5"
            @submit-flags="handleSubmitFlags" /> <!-- Listen for submit event -->

        <!-- Show REVIEW results table -->
        <ReviewResultsTable
            v-if="jobDetails?.status === 'completed' && jobDetails?.id && jobDetails?.job_type === 'REVIEW'"
            :results="jobStore.jobResults as ReviewResultItem[] | null"
            :loading="jobStore.resultsLoading"
            :error="jobStore.resultsError"
            :target-level="jobDetails.target_level || 5"
            @submit-flags="handleSubmitFlags" /> <!-- Listen for submit event -->
        <!-- ***** END UPDATED Section ***** -->

      </div>
    </div>
      <div v-else class="text-center py-10 text-gray-500">
        <!-- Message shown when no job is selected -->
        <!-- Select a job from the history or upload a new file. -->
      </div>
  </template>

  <script setup lang="ts">
  import { ref, computed, onMounted, onUnmounted, watch } from 'vue';
  import apiService from '@/services/api';
  // Import store and types (Removed JobDetails from here)
  import { useJobStore, type JobResultItem, type ReviewResultItem } from '@/stores/job';
  import JobStats from './JobStats.vue';
  // Import both results tables
  import JobResultsTable from './JobResultsTable.vue';
  import ReviewResultsTable from './ReviewResultsTable.vue';
  import { EnvelopeIcon, ArrowDownTrayIcon, ExclamationTriangleIcon, ArrowUturnLeftIcon, CheckCircleIcon } from '@heroicons/vue/20/solid';

  const POLLING_INTERVAL = 5000; // Poll every 5 seconds
  const jobStore = useJobStore();
  const jobDetails = computed(() => jobStore.jobDetails); // Use jobDetails directly from the store
  const isLoading = ref(false); // Tracks if a poll request is currently in flight
  const errorMessage = ref<string | null>(null); // Stores polling or general errors
  const pollingIntervalId = ref<number | null>(null); // Stores the ID from setInterval
  const showReclassifySuccessMessage = ref(false); // Control visibility of success message

  // --- Notification State ---
  const notificationEmail = ref('');
  const isNotifyLoading = ref(false);
  const notifyMessage = ref<string | null>(null);
  const notifyMessageIsError = ref(false);

  // --- Download State ---
  const isDownloadLoading = ref(false);
  const downloadError = ref<string | null>(null);

  // --- Computed Properties ---
  const formattedStage = computed(() => {
    const stage = jobDetails.value?.current_stage;
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'Completed';
    if (status === 'failed') return 'Failed';
    if (status === 'pending') return 'Pending Start';
    if (!stage) return 'Loading...';
    const stageNames: { [key: string]: string } = {
      'ingestion': 'Ingesting File', 'normalization': 'Normalizing Data',
      'classification_level_1': 'Classifying (L1)', 'classification_level_2': 'Classifying (L2)',
      'classification_level_3': 'Classifying (L3)', 'classification_level_4': 'Classifying (L4)',
      'classification_level_5': 'Classifying (L5)', 'search_unknown_vendors': 'Researching Vendors',
      'reclassification': 'Re-classifying', 'result_generation': 'Generating Results',
      'pending': 'Pending Start',
    };
    return stageNames[stage] || stage;
  });

  const progressPercent = computed(() => {
    const status = jobDetails.value?.status;
    const progress = jobDetails.value?.progress ?? 0;
    if (status === 'completed') return 100;
    if (status === 'pending') return 0;
    return Math.max(0, Math.min(100, Math.round(progress * 100)));
  });

  const statusBadgeClass = computed(() => {
      switch (jobDetails.value?.status) {
          case 'completed': return 'bg-green-100 text-green-800';
          case 'failed': return 'bg-red-100 text-red-800';
          case 'processing': return 'bg-blue-100 text-blue-800 animate-pulse';
          default: return 'bg-gray-100 text-gray-800';
      }
  });

  const progressColorClass = computed(() => {
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'bg-green-500';
    if (status === 'failed') return 'bg-red-500';
    return 'bg-primary animate-pulse';
  });

  const formatDateTime = (isoString: string | null | undefined): string => {
      if (!isoString) return 'N/A';
      try {
          return new Date(isoString).toLocaleString(undefined, {
              year: 'numeric', month: 'short', day: 'numeric',
              hour: 'numeric', minute: '2-digit', hour12: true
          });
      } catch { return 'Invalid Date'; }
  };

  const formattedCreatedAt = computed(() => formatDateTime(jobDetails.value?.created_at));
  const formattedUpdatedAt = computed(() => formatDateTime(jobDetails.value?.updated_at));

  const formattedEstimatedCompletion = computed(() => {
      const status = jobDetails.value?.status;
      if (status === 'completed' && jobDetails.value?.completed_at) {
          return formatDateTime(jobDetails.value.completed_at);
      }
      const estCompletion = jobDetails.value?.estimated_completion;
      if (status === 'processing' && estCompletion) {
          return `${formatDateTime(estCompletion)} (est.)`;
      }
      if (status === 'processing') return 'Calculating...';
      if (status === 'failed') return 'N/A';
      if (status === 'pending') return 'Pending Start';
      return 'N/A';
  });

  const canRequestNotify = computed(() => {
      const status = jobDetails.value?.status;
      return status === 'pending' || status === 'processing';
  });

  // --- Methods ---
  const pollJobStatus = async (jobId: string | null | undefined) => {
     if (!jobId || jobStore.currentJobId !== jobId) {
         console.log(`JobStatus: [pollJobStatus] Stopping polling. Reason: Job ID mismatch or null. (Poll ID: ${jobId}, Store ID: ${jobStore.currentJobId})`);
         stopPolling();
         return;
     }
     if (isLoading.value) {
         console.log(`JobStatus: [pollJobStatus] Skipping poll for ${jobId} - already in progress.`);
         return;
     }

     isLoading.value = true;
     console.log(`JobStatus: [pollJobStatus] Polling status for job ${jobId}...`);
    try {
        const data = await apiService.getJobStatus(jobId);
        if (jobStore.currentJobId === jobId) {
            console.log(`JobStatus: [pollJobStatus] Received status for ${jobId}: Status=${data.status}, Progress=${data.progress}, Type=${data.job_type}`);
            const previousStatus = jobStore.jobDetails?.status; // Store previous status before update
            jobStore.updateJobDetails(data);
            errorMessage.value = null;

            // Check if job just completed or failed
            const justCompleted = data.status === 'completed' && previousStatus !== 'completed';
            const justFailed = data.status === 'failed' && previousStatus !== 'failed';

            if (justCompleted || justFailed) {
                console.log(`JobStatus: [pollJobStatus] Job ${jobId} reached terminal state (${data.status}). Stopping polling.`);
                stopPolling();
                if (justCompleted) {
                    console.log(`JobStatus: [pollJobStatus] Job ${jobId} completed. Triggering fetchCurrentJobResults.`);
                    // Use the new action to fetch results (handles both types)
                    if (!jobStore.resultsLoading && !jobStore.jobResults && !jobStore.relatedReviewResults) { // Check both result states
                         jobStore.fetchCurrentJobResults();
                    } else {
                         console.log(`JobStatus: [pollJobStatus] Job ${jobId} completed, but results already loading or present. Skipping fetch.`);
                    }
                }
            }
        } else {
             console.log(`JobStatus: [pollJobStatus] Job ID changed during API call for ${jobId}. Ignoring stale data.`);
        }
    } catch (error: any) {
        console.error(`JobStatus: [pollJobStatus] Error polling status for ${jobId}:`, error);
        if (jobStore.currentJobId === jobId) {
            errorMessage.value = `Polling Error: ${error.message || 'Failed to fetch status.'}`;
        }
        stopPolling();
    } finally {
        if (jobStore.currentJobId === jobId) {
            isLoading.value = false;
        }
    }
  };

  const startPolling = (jobId: string | null | undefined) => {
    if (!jobId) {
        console.log("JobStatus: [startPolling] Cannot start polling, no jobId provided.");
        return;
    }
    stopPolling();
    console.log(`JobStatus: [startPolling] Starting polling for job ${jobId}.`);
    pollJobStatus(jobId); // Poll immediately

    pollingIntervalId.value = window.setInterval(() => {
        console.log(`JobStatus: [setInterval] Checking poll condition for ${jobId}. Current store ID: ${jobStore.currentJobId}, Status: ${jobStore.jobDetails?.status}`);
        if (jobStore.currentJobId === jobId && jobStore.jobDetails?.status !== 'completed' && jobStore.jobDetails?.status !== 'failed') {
            pollJobStatus(jobId);
        } else {
            console.log(`JobStatus: [setInterval] Condition not met for job ${jobId}, stopping polling.`);
            stopPolling();
        }
    }, POLLING_INTERVAL);
  };

  const stopPolling = () => {
    if (pollingIntervalId.value !== null) {
        console.log(`JobStatus: [stopPolling] Stopping polling interval ID ${pollingIntervalId.value}.`);
        clearInterval(pollingIntervalId.value);
        pollingIntervalId.value = null;
    }
  };

  const requestNotification = async () => {
     const currentId = jobDetails.value?.id;
     if (!currentId || !notificationEmail.value) return;
     isNotifyLoading.value = true;
     notifyMessage.value = null;
     notifyMessageIsError.value = false;
     console.log(`JobStatus: Requesting notification for ${currentId} to ${notificationEmail.value}`);
    try {
        const response = await apiService.requestNotification(currentId, notificationEmail.value);
        console.log(`JobStatus: Notification request successful:`, response);
        notifyMessage.value = response.message || 'Notification request sent!';
        notificationEmail.value = '';
    } catch (error: any) {
        console.error(`JobStatus: Notification request failed:`, error);
        notifyMessage.value = `Error: ${error.message || 'Failed to send request.'}`;
        notifyMessageIsError.value = true;
    } finally {
        isNotifyLoading.value = false;
        setTimeout(() => { notifyMessage.value = null; }, 5000);
    }
  };

  const downloadResults = async () => {
     const currentId = jobDetails.value?.id;
     if (!currentId) return;
     isDownloadLoading.value = true;
     downloadError.value = null;
     console.log(`JobStatus: Attempting download for ${currentId}`);
    try {
        const { blob, filename } = await apiService.downloadResults(currentId);
        console.log(`JobStatus: Download blob received, filename: ${filename}`);
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none'; a.href = url; a.download = filename;
        document.body.appendChild(a); a.click();
        window.URL.revokeObjectURL(url); document.body.removeChild(a);
        console.log(`JobStatus: Download triggered for ${filename}`);
    } catch (error: any) {
        console.error(`JobStatus: Download failed:`, error);
        downloadError.value = `Download failed: ${error.message || 'Could not download results.'}`;
    } finally {
        isDownloadLoading.value = false;
    }
  };

  // --- Reclassification Submission Handler ---
  const handleSubmitFlags = async () => {
    console.log("JobStatus: Handling submit flags event...");
    const reviewJobId = await jobStore.submitFlagsForReview();
    if (reviewJobId) {
        showReclassifySuccessMessage.value = true;
        setTimeout(() => { showReclassifySuccessMessage.value = false; }, 7000);
        jobStore.fetchJobHistory(); // Refresh history list
    } else {
        console.log("JobStatus: Flag submission failed (error handled in store/table).");
        // Optionally show a generic error message here if needed, though specific errors are better shown near the button
    }
  };

  // --- Navigation ---
  const viewParentJob = () => {
      if (jobDetails.value?.parent_job_id) {
          jobStore.setCurrentJobId(jobDetails.value.parent_job_id);
      }
  };

  const viewReviewJob = (reviewJobId: string) => {
       jobStore.setCurrentJobId(reviewJobId);
       showReclassifySuccessMessage.value = false; // Hide message on navigation
  };

  // --- Fetch Initial Data Function ---
  const fetchInitialData = (jobId: string) => {
      errorMessage.value = null; // Clear previous errors
      const currentDetails = jobStore.jobDetails;

      // Fetch details if they are missing OR if the ID matches but type/status might be stale
      if (!currentDetails || currentDetails.id !== jobId) {
           console.log(`JobStatus: Fetching initial details and starting polling for ${jobId}`);
           startPolling(jobId); // Poll will fetch details
      } else if (currentDetails.status === 'completed') {
           console.log(`JobStatus: Job ${jobId} already completed. Fetching results if needed.`);
           stopPolling();
           // Use the new action to fetch results
           if (!jobStore.resultsLoading && !jobStore.jobResults && !jobStore.relatedReviewResults) {
                jobStore.fetchCurrentJobResults();
           }
      } else if (currentDetails.status === 'failed') {
           console.log(`JobStatus: Job ${jobId} already failed. Not polling or fetching results.`);
           stopPolling();
      } else {
           // Status is pending or processing, start polling
           console.log(`JobStatus: Job ${jobId} is ${currentDetails.status}. Starting polling.`);
           startPolling(jobId);
      }
  };

  // --- Lifecycle Hooks ---
  onMounted(() => {
      console.log(`JobStatus: Mounted. Current job ID from store: ${jobStore.currentJobId}`);
      if (jobStore.currentJobId) {
          fetchInitialData(jobStore.currentJobId);
      }
  });

  onUnmounted(() => {
      console.log("JobStatus: Unmounted, stopping polling.");
      stopPolling();
  });

  // --- Watchers ---
  watch(() => jobStore.currentJobId, (newJobId, oldJobId) => {
      console.log(`JobStatus: Watched currentJobId changed from ${oldJobId} to: ${newJobId}`);
      showReclassifySuccessMessage.value = false; // Hide success message on job change
      if (newJobId) {
          // Reset component state related to the specific job
          errorMessage.value = null;
          downloadError.value = null;
          notifyMessage.value = null;
          notificationEmail.value = '';
          isDownloadLoading.value = false;
          isNotifyLoading.value = false;
          // Fetch data for the new job
          fetchInitialData(newJobId);
      } else {
          // Job ID was cleared
          console.log("JobStatus: Job ID cleared, stopping polling.");
          stopPolling();
      }
  }, { immediate: false }); // Don't run immediately on mount, let onMounted handle initial load

  // Watch for the job status changing to completed (handles updates from polling)
  // This watcher seems redundant now as the polling logic handles stopping and fetching results.
  // Keep it for now as a potential backup, but consider removing if polling logic proves robust.
  watch(() => jobStore.jobDetails?.status, (newStatus, oldStatus) => {
      const currentId = jobStore.currentJobId;
      if (!currentId || newStatus === oldStatus) return; // Only act on change for the current job

      console.log(`JobStatus: Watched job status changed from ${oldStatus} to: ${newStatus} for job ${currentId}`);

      if (newStatus === 'completed') {
          console.log(`JobStatus: Job ${currentId} completed (detected by status watcher). Ensuring results are fetched.`);
          stopPolling(); // Ensure polling is stopped
          // Use the new action to fetch results if not already loading/present
          if (!jobStore.resultsLoading && !jobStore.jobResults && !jobStore.relatedReviewResults) {
            jobStore.fetchCurrentJobResults();
          }
      } else if (newStatus === 'failed') {
          console.log(`JobStatus: Job ${currentId} failed (detected by status watcher). Ensuring polling is stopped.`);
          stopPolling();
      }
  });

  </script>
</file>

<file path='frontend/vue_frontend/src/components/ReviewResultsTable.vue'>
<template>
  <div class="mt-8 p-4 sm:p-6 bg-gray-50 rounded-lg border border-gray-200 shadow-inner">
    <h5 class="text-lg font-semibold text-gray-800 mb-4">Reviewed Classification Results</h5>
    <p class="text-sm text-gray-600 mb-4">
      Showing results after applying user hints. Target classification level for this review was **Level {{ targetLevel }}**. You can flag items again for further review if needed.
    </p>

    <!-- Search Input -->
    <div class="mb-4">
      <label for="review-results-search" class="sr-only">Search Reviewed Results</label>
      <div class="relative rounded-md shadow-sm">
        <div class="absolute inset-y-0 left-0 pl-3 flex items-center pointer-events-none">
          <MagnifyingGlassIcon class="h-5 w-5 text-gray-400" aria-hidden="true" />
        </div>
        <input
          type="text"
          id="review-results-search"
          v-model="searchTerm"
          placeholder="Search Vendor, Hint, Category, ID, Notes..."
          class="block w-full pl-10 pr-3 py-2 border border-gray-300 rounded-md placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm"
        />
      </div>
    </div>

     <!-- Action Buttons (Submit Flags) -->
    <div class="mb-4 text-right" v-if="jobStore.hasFlaggedItems">
        <button
          type="button"
          @click="submitFlags"
          :disabled="jobStore.reclassifyLoading"
          class="inline-flex items-center rounded-md bg-primary px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-primary-dark focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-primary disabled:opacity-50"
        >
          <ArrowPathIcon v-if="jobStore.reclassifyLoading" class="animate-spin -ml-0.5 mr-1.5 h-5 w-5" aria-hidden="true" />
          <PaperAirplaneIcon v-else class="-ml-0.5 mr-1.5 h-5 w-5" aria-hidden="true" />
          Submit {{ jobStore.flaggedForReview.size }} Flag{{ jobStore.flaggedForReview.size !== 1 ? 's' : '' }} for Re-classification
        </button>
        <p v-if="jobStore.reclassifyError" class="text-xs text-red-600 mt-1 text-right">{{ jobStore.reclassifyError }}</p>
    </div>

    <!-- Loading/Error States -->
    <div v-if="loading" class="text-center py-5 text-gray-500">
      <svg class="animate-spin h-6 w-6 text-primary mx-auto" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
      </svg>
      <p class="mt-2 text-sm">Loading reviewed results...</p>
    </div>
    <div v-else-if="error" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm">
      Error loading reviewed results: {{ error }}
    </div>
    <div v-else-if="!results || results.length === 0" class="text-center py-5 text-gray-500">
      No reviewed results found for this job.
    </div>

    <!-- Results Table -->
    <div v-else class="overflow-x-auto border border-gray-200 rounded-md">
      <table class="min-w-full divide-y divide-gray-200">
        <thead class="bg-gray-100">
          <tr>
            <!-- Flag Column -->
            <th scope="col" class="sticky left-0 z-10 bg-gray-100 px-2 py-3 text-center text-xs font-medium text-gray-600 uppercase tracking-wider w-12">Flag</th>
            <!-- Dynamically generate headers -->
            <th v-for="header in headers" :key="header.key"
                scope="col"
                @click="header.sortable ? sortBy(header.key) : null"
                :class="[
                  'px-3 py-3 text-left text-xs font-medium text-gray-600 uppercase tracking-wider whitespace-nowrap',
                   header.sortable ? 'cursor-pointer hover:bg-gray-200' : '',
                   header.sticky ? 'sticky left-[48px] z-10 bg-gray-100' : '', // Adjusted left offset for flag column
                   header.minWidth ? `min-w-[${header.minWidth}]` : '',
                   header.isOriginal ? 'bg-blue-50' : '', // Style original columns
                   header.isNew ? 'bg-green-50' : '', // Style new columns
                ]">
              {{ header.label }}
              <SortIcon v-if="header.sortable" :direction="sortKey === header.key ? sortDirection : null" />
            </th>
          </tr>
        </thead>
        <tbody class="bg-white divide-y divide-gray-200">
          <tr v-if="filteredAndSortedResults.length === 0">
            <td :colspan="headers.length + 1" class="px-4 py-4 whitespace-nowrap text-sm text-gray-500 text-center">No results match your search criteria.</td>
          </tr>
          <tr v-for="(item, index) in filteredAndSortedResults" :key="item.vendor_name + '-' + index" class="hover:bg-gray-50 align-top" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">
            <!-- Flag Button Cell (Sticky) -->
            <td class="sticky left-0 z-10 bg-white px-2 py-2 text-center align-middle" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">
                 <button
                    @click="toggleFlag(item.vendor_name)"
                    :title="jobStore.isFlagged(item.vendor_name) ? 'Remove flag and hint' : 'Flag for re-classification'"
                    class="p-1 rounded-full hover:bg-gray-200 focus:outline-none focus:ring-2 focus:ring-offset-1 focus:ring-primary"
                    :class="jobStore.isFlagged(item.vendor_name) ? 'text-primary' : 'text-gray-400 hover:text-primary-dark'"
                  >
                    <FlagIconSolid v-if="jobStore.isFlagged(item.vendor_name)" class="h-5 w-5" aria-hidden="true" />
                    <FlagIconOutline v-else class="h-5 w-5" aria-hidden="true" />
                    <span class="sr-only">Flag item</span>
                  </button>
            </td>
            <!-- Vendor Name Cell (Sticky) -->
            <td class="sticky left-[48px] z-10 bg-white px-3 py-2 whitespace-nowrap text-sm font-medium text-gray-900" :class="{'bg-indigo-50': jobStore.isFlagged(item.vendor_name)}">{{ item.vendor_name }}</td>
            <!-- Hint Cell -->
            <td class="px-3 py-2 text-xs text-gray-600 max-w-xs break-words">
                <span v-if="!jobStore.isFlagged(item.vendor_name)">{{ item.hint }}</span>
                 <!-- Inline Hint Editor when Flagged -->
                <textarea v-else
                          rows="2"
                          :value="jobStore.getHint(item.vendor_name)"
                          @input="updateHint(item.vendor_name, ($event.target as HTMLTextAreaElement).value)"
                          placeholder="Enter new hint..."
                          class="block w-full text-xs rounded-md border-gray-300 shadow-sm focus:border-primary focus:ring-primary"
                ></textarea>
            </td>
            <!-- Original Classification -->
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono text-gray-500 bg-blue-50">{{ item.original_result?.level1_id || '-' }}</td>
            <td class="px-3 py-2 text-xs text-gray-500 bg-blue-50">{{ item.original_result?.level1_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono text-gray-500 bg-blue-50">{{ item.original_result?.level2_id || '-' }}</td>
            <td class="px-3 py-2 text-xs text-gray-500 bg-blue-50">{{ item.original_result?.level2_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono text-gray-500 bg-blue-50">{{ item.original_result?.level3_id || '-' }}</td>
            <td class="px-3 py-2 text-xs text-gray-500 bg-blue-50">{{ item.original_result?.level3_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono text-gray-500 bg-blue-50">{{ item.original_result?.level4_id || '-' }}</td>
            <td class="px-3 py-2 text-xs text-gray-500 bg-blue-50">{{ item.original_result?.level4_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono text-gray-500 bg-blue-50">{{ item.original_result?.level5_id || '-' }}</td>
            <td class="px-3 py-2 text-xs text-gray-500 bg-blue-50">{{ item.original_result?.level5_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs text-center text-gray-500 bg-blue-50">
                <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                      :class="getStatusClass(item.original_result?.final_status)">
                    {{ item.original_result?.final_status }}
                </span>
            </td>

            <!-- New Classification -->
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 1)">{{ item.new_result?.level1_id || '-' }}</td>
            <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 1)">{{ item.new_result?.level1_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 2)">{{ item.new_result?.level2_id || '-' }}</td>
            <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 2)">{{ item.new_result?.level2_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 3)">{{ item.new_result?.level3_id || '-' }}</td>
            <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 3)">{{ item.new_result?.level3_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 4)">{{ item.new_result?.level4_id || '-' }}</td>
            <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 4)">{{ item.new_result?.level4_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-xs font-mono bg-green-50" :class="getCellClass(item.new_result, 5)">{{ item.new_result?.level5_id || '-' }}</td>
            <td class="px-3 py-2 text-xs bg-green-50" :class="getCellClass(item.new_result, 5)">{{ item.new_result?.level5_name || '-' }}</td>
            <td class="px-3 py-2 whitespace-nowrap text-sm text-center bg-green-50">
              <span v-if="item.new_result?.final_confidence !== null && item.new_result?.final_confidence !== undefined"
                    :class="getConfidenceClass(item.new_result.final_confidence)">
                {{ (item.new_result.final_confidence * 100).toFixed(1) }}%
              </span>
              <span v-else class="text-gray-400 text-xs">N/A</span>
            </td>
            <td class="px-3 py-2 whitespace-nowrap text-xs text-center bg-green-50">
               <span class="px-2 inline-flex text-xs leading-5 font-semibold rounded-full"
                    :class="getStatusClass(item.new_result?.final_status)">
                {{ item.new_result?.final_status }}
              </span>
            </td>
            <td class="px-3 py-2 text-xs text-gray-500 max-w-xs break-words bg-green-50">
              {{ item.new_result?.classification_notes_or_reason || '-' }}
            </td>
          </tr>
        </tbody>
      </table>
    </div>

     <!-- Row Count -->
    <div class="mt-3 text-xs text-gray-500">
      Showing {{ filteredAndSortedResults.length }} of {{ results?.length || 0 }} reviewed results.
    </div>

  </div>
</template>

<script setup lang="ts">
import { ref, computed, type PropType } from 'vue';
import { useJobStore, type ReviewResultItem, type JobResultItem } from '@/stores/job';
import { FlagIcon as FlagIconOutline, MagnifyingGlassIcon, PaperAirplaneIcon, ArrowPathIcon } from '@heroicons/vue/24/outline';
import { FlagIcon as FlagIconSolid, ChevronUpIcon, ChevronDownIcon, ChevronUpDownIcon } from '@heroicons/vue/20/solid';
// import HintInputModal from './HintInputModal.vue'; // Import if using modal

// --- Define Header Interface ---
interface ReviewTableHeader {
  key: string; // Use string for complex/nested keys
  label: string;
  sortable: boolean;
  sticky?: boolean; // For sticky columns
  minWidth?: string;
  isOriginal?: boolean; // Flag for styling/grouping
  isNew?: boolean;      // Flag for styling/grouping
}
// --- END Define Header Interface ---

// --- Props ---
const props = defineProps({
  results: {
    type: Array as PropType<ReviewResultItem[] | null>,
    required: true,
  },
  loading: {
    type: Boolean,
    default: false,
  },
  error: {
    type: String as PropType<string | null>,
    default: null,
  },
  targetLevel: { // Pass the job's target level
    type: Number,
    required: true,
  }
});

const emit = defineEmits(['submit-flags']); // Emit event when submit button is clicked

// --- Store ---
const jobStore = useJobStore();

// --- Internal State ---
const searchTerm = ref('');
const sortKey = ref<string | null>('vendor_name'); // Default sort by vendor name
const sortDirection = ref<'asc' | 'desc' | null>('asc'); // Default sort direction
// const showHintModal = ref(false); // State for modal
// const selectedVendorForHint = ref(''); // State for modal

// --- Table Headers Definition ---
// ADDED HEADERS FOR L2, L3, L4 for both Original and New sections
const headers = ref<ReviewTableHeader[]>([
  { key: 'vendor_name', label: 'Vendor Name', sortable: true, sticky: true, minWidth: '150px' }, // Make Vendor sticky
  { key: 'hint', label: 'User Hint', sortable: true, minWidth: '180px' },
  // Original Results
  { key: 'original_result.level1_id', label: 'Orig L1 ID', sortable: true, minWidth: '80px', isOriginal: true },
  { key: 'original_result.level1_name', label: 'Orig L1 Name', sortable: true, minWidth: '120px', isOriginal: true },
  { key: 'original_result.level2_id', label: 'Orig L2 ID', sortable: true, minWidth: '80px', isOriginal: true },
  { key: 'original_result.level2_name', label: 'Orig L2 Name', sortable: true, minWidth: '120px', isOriginal: true },
  { key: 'original_result.level3_id', label: 'Orig L3 ID', sortable: true, minWidth: '80px', isOriginal: true },
  { key: 'original_result.level3_name', label: 'Orig L3 Name', sortable: true, minWidth: '120px', isOriginal: true },
  { key: 'original_result.level4_id', label: 'Orig L4 ID', sortable: true, minWidth: '80px', isOriginal: true },
  { key: 'original_result.level4_name', label: 'Orig L4 Name', sortable: true, minWidth: '120px', isOriginal: true },
  { key: 'original_result.level5_id', label: 'Orig L5 ID', sortable: true, minWidth: '80px', isOriginal: true },
  { key: 'original_result.level5_name', label: 'Orig L5 Name', sortable: true, minWidth: '120px', isOriginal: true },
  { key: 'original_result.final_status', label: 'Orig Status', sortable: true, minWidth: '100px', isOriginal: true },
  // New Results
  { key: 'new_result.level1_id', label: 'New L1 ID', sortable: true, minWidth: '80px', isNew: true },
  { key: 'new_result.level1_name', label: 'New L1 Name', sortable: true, minWidth: '120px', isNew: true },
  { key: 'new_result.level2_id', label: 'New L2 ID', sortable: true, minWidth: '80px', isNew: true },
  { key: 'new_result.level2_name', label: 'New L2 Name', sortable: true, minWidth: '120px', isNew: true },
  { key: 'new_result.level3_id', label: 'New L3 ID', sortable: true, minWidth: '80px', isNew: true },
  { key: 'new_result.level3_name', label: 'New L3 Name', sortable: true, minWidth: '120px', isNew: true },
  { key: 'new_result.level4_id', label: 'New L4 ID', sortable: true, minWidth: '80px', isNew: true },
  { key: 'new_result.level4_name', label: 'New L4 Name', sortable: true, minWidth: '120px', isNew: true },
  { key: 'new_result.level5_id', label: 'New L5 ID', sortable: true, minWidth: '80px', isNew: true },
  { key: 'new_result.level5_name', label: 'New L5 Name', sortable: true, minWidth: '120px', isNew: true },
  { key: 'new_result.final_confidence', label: 'New Confidence', sortable: true, minWidth: '100px', isNew: true },
  { key: 'new_result.final_status', label: 'New Status', sortable: true, minWidth: '100px', isNew: true },
  { key: 'new_result.classification_notes_or_reason', label: 'New Notes / Reason', sortable: false, minWidth: '200px', isNew: true },
]);

// --- Computed Properties ---

// Helper to get nested values for sorting/filtering
const getNestedValue = (obj: any, path: string): any => {
  // Handle cases where obj might be null or undefined early
  if (!obj) return null;
  return path.split('.').reduce((value, key) => (value && value[key] !== undefined ? value[key] : null), obj);
};


const filteredAndSortedResults = computed(() => {
  if (!props.results) return [];

  let filtered = props.results;

  // Filtering
  if (searchTerm.value) {
    const lowerSearchTerm = searchTerm.value.toLowerCase();
    filtered = filtered.filter(item =>
      item.vendor_name?.toLowerCase().includes(lowerSearchTerm) ||
      item.hint?.toLowerCase().includes(lowerSearchTerm) ||
      // Search within original results (L1-L5)
      getNestedValue(item, 'original_result.level1_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level1_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level2_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level2_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level3_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level3_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level4_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level4_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level5_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.level5_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'original_result.final_status')?.toLowerCase().includes(lowerSearchTerm) ||
      // Search within new results (L1-L5)
      getNestedValue(item, 'new_result.level1_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level1_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level2_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level2_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level3_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level3_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level4_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level4_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level5_id')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.level5_name')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.final_status')?.toLowerCase().includes(lowerSearchTerm) ||
      getNestedValue(item, 'new_result.classification_notes_or_reason')?.toLowerCase().includes(lowerSearchTerm)
    );
  }

  // Sorting
  if (sortKey.value && sortDirection.value) {
    const key = sortKey.value;
    const direction = sortDirection.value === 'asc' ? 1 : -1;

    filtered = filtered.slice().sort((a, b) => {
      const valA = getNestedValue(a, key);
      const valB = getNestedValue(b, key);

      const aIsNull = valA === null || valA === undefined || valA === '';
      const bIsNull = valB === null || valB === undefined || valB === '';

      if (aIsNull && bIsNull) return 0;
      if (aIsNull) return 1 * direction; // Nulls/empty last when ascending
      if (bIsNull) return -1 * direction; // Nulls/empty last when ascending

      if (typeof valA === 'string' && typeof valB === 'string') {
        return valA.localeCompare(valB) * direction;
      }
      if (typeof valA === 'number' && typeof valB === 'number') {
        return (valA - valB) * direction;
      }

      // Fallback: compare as strings
      const strA = String(valA).toLowerCase();
      const strB = String(valB).toLowerCase();
      if (strA < strB) return -1 * direction;
      if (strA > strB) return 1 * direction;
      return 0;
    });
  }

  return filtered;
});

// --- Methods ---

function sortBy(key: string) { // Key is now string due to nesting
  if (sortKey.value === key) {
    if (sortDirection.value === 'asc') {
        sortDirection.value = 'desc';
    } else if (sortDirection.value === 'desc') {
        // Cycle back to no sort instead of asc
        sortDirection.value = null;
        sortKey.value = null;
    } else { // Was null
        sortDirection.value = 'asc'; // Start with asc
        sortKey.value = key;
    }
  } else {
    sortKey.value = key;
    sortDirection.value = 'asc'; // Default to asc when changing column
  }
}

function getConfidenceClass(confidence: number | null | undefined): string {
  if (confidence === null || confidence === undefined) return 'text-gray-400';
  if (confidence >= 0.8) return 'text-green-700 font-medium';
  if (confidence >= 0.5) return 'text-yellow-700';
  return 'text-red-700';
}

function getStatusClass(status: string | null | undefined): string {
    switch(status?.toLowerCase()){
        case 'classified': return 'bg-green-100 text-green-800';
        case 'not possible': return 'bg-yellow-100 text-yellow-800';
        case 'error': return 'bg-red-100 text-red-800';
        default: return 'bg-gray-100 text-gray-800';
    }
}

// Highlight cells beyond the target classification depth in the *new* result
// Or if the ID itself is null/empty
function getCellClass(item: JobResultItem | null | undefined, level: number): string {
    const baseClass = 'text-gray-700';
    const beyondDepthClass = 'text-gray-400 italic'; // Style for levels beyond target
    const nullClass = 'text-gray-400'; // Style for null/empty values

    if (!item) return nullClass; // Handle case where new_result might be null

    const levelIdKey = `level${level}_id` as keyof JobResultItem;
    const hasId = item[levelIdKey] !== null && item[levelIdKey] !== undefined && String(item[levelIdKey]).trim() !== '';

    if (!hasId) {
        return nullClass; // Use null style if ID is missing/empty
    }

    // Check if the *achieved* level for the *new* result is less than the current cell's level
    const achievedLevel = item.achieved_level ?? 0;
    if (level > achievedLevel) {
         // If the classification stopped before this level, but the ID somehow exists (unlikely but possible), style it as less important
         // Or, more likely, the ID *is* null, handled above. This check is secondary.
         // Let's prioritize the null check. If it has an ID, show it normally unless it's beyond the *target* level.
    }

    // Style differently if the cell's level is beyond the *job's* target level
    if (level > props.targetLevel) {
        return beyondDepthClass;
    }

    return baseClass; // Default style if it has an ID and is within target level
}

// --- Flagging and Hint Handling ---
function toggleFlag(vendorName: string) {
    if (jobStore.isFlagged(vendorName)) {
        jobStore.unflagVendor(vendorName);
    } else {
        // Pre-populate hint if available from the current item's hint field
        const currentItem = props.results?.find(r => r.vendor_name === vendorName);
        const initialHint = currentItem?.hint || ''; // Use existing hint or empty string
        jobStore.flagVendor(vendorName);
        jobStore.setHint(vendorName, initialHint); // Set the initial hint when flagging
        // Optionally open modal here if using one
        // selectedVendorForHint.value = vendorName;
        // showHintModal.value = true;
    }
}

function updateHint(vendorName: string, hint: string) {
    jobStore.setHint(vendorName, hint);
}

// function saveHint(hint: string) {
//     if (selectedVendorForHint.value) {
//         jobStore.setHint(selectedVendorForHint.value, hint);
//     }
//     selectedVendorForHint.value = ''; // Clear selection
// }

async function submitFlags() {
    emit('submit-flags'); // Notify parent (JobStatus) to handle submission logic
}

// --- Helper Component for Sort Icons ---
const SortIcon = {
  props: {
    direction: {
      type: String as PropType<'asc' | 'desc' | null>,
      default: null,
    },
  },
  components: { ChevronUpIcon, ChevronDownIcon, ChevronUpDownIcon },
  template: `
    <span class="inline-block ml-1 w-4 h-4 align-middle">
      <ChevronUpIcon v-if="direction === 'asc'" class="w-4 h-4 text-gray-700" />
      <ChevronDownIcon v-else-if="direction === 'desc'" class="w-4 h-4 text-gray-700" />
      <ChevronUpDownIcon v-else class="w-4 h-4 text-gray-400 opacity-50" />
    </span>
  `,
};

</script>

<style scoped>
/* Ensure sticky header cells have appropriate background */
thead th.sticky {
  position: sticky;
  /* Apply background color matching the thead */
  background-color: #f3f4f6; /* bg-gray-100 */
}

/* Ensure sticky body cells have appropriate background */
tbody td.sticky {
    position: sticky;
    /* Apply background color matching the row's background (consider hover/flagged states) */
    background-color: inherit; /* Inherit from parent tr */
}

/* Add slight borders for visual separation */
th.isOriginal, td.isOriginal {
    border-left: 1px solid #e5e7eb; /* gray-200 */
}
th.isNew, td.isNew {
    border-left: 1px solid #e5e7eb; /* gray-200 */
}
th:first-child, td:first-child {
    border-left: none;
}

</style>
</file>

<file path='frontend/vue_frontend/src/services/api.ts'>
import axios, {
    type AxiosInstance,
    type InternalAxiosRequestConfig,
    type AxiosError // Import AxiosError type
} from 'axios';
import { useAuthStore } from '@/stores/auth'; // Adjust path as needed
// --- UPDATED: Import JobResultItem and ReviewResultItem ---
import type { JobDetails, JobResultItem, ReviewResultItem } from '@/stores/job'; // Adjust path as needed
// --- END UPDATED ---

// --- Define API Response Interfaces ---

// Matches backend schemas/user.py -> UserResponse
export interface UserResponse {
    email: string;
    full_name: string | null;
    is_active: boolean | null;
    is_superuser: boolean | null;
    username: string;
    id: string; // UUID as string
    created_at: string; // ISO Date string
    updated_at: string; // ISO Date string
}

// Matches backend schemas/user.py -> UserCreate (for request body)
export interface UserCreateData {
    email: string;
    full_name?: string | null;
    is_active?: boolean | null;
    is_superuser?: boolean | null;
    username: string;
    password?: string; // Password required on create
}

// Matches backend schemas/user.py -> UserUpdate (for request body)
export interface UserUpdateData {
    email?: string | null;
    full_name?: string | null;
    password?: string | null; // Optional password update
    is_active?: boolean | null;
    is_superuser?: boolean | null;
}


// Matches backend response for /token (modified to include user object)
interface AuthResponse {
    access_token: string;
    token_type: string;
    user: UserResponse; // Include the user details
}

// --- ADDED: File Validation Response Interface ---
// Matches backend api/main.py -> FileValidationResponse
export interface FileValidationResponse {
    is_valid: boolean;
    message: string;
    detected_columns: string[];
    missing_mandatory_columns: string[];
}
// --- END ADDED ---

// Matches backend response for /api/v1/jobs/{job_id}/notify
interface NotifyResponse {
    success: boolean;
    message: string;
}

// Matches backend response for /api/v1/jobs/ (list endpoint)
// Should align with app/schemas/job.py -> JobResponse
export interface JobResponse {
    id: string;
    company_name: string;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string;
    created_at: string; // ISO Date string
    updated_at?: string | null;
    completed_at?: string | null;
    output_file_name?: string | null;
    input_file_name: string;
    created_by: string;
    error_message?: string | null;
    target_level: number; // Ensure target_level is included here
    // --- ADDED: Job Type and Parent Link ---
    job_type: 'CLASSIFICATION' | 'REVIEW';
    parent_job_id: string | null;
    // --- END ADDED ---
}

// --- ADDED: Job Results Response Interface ---
// Matches backend schemas/job.py -> JobResultsResponse
export interface JobResultsResponse {
    job_id: string;
    job_type: 'CLASSIFICATION' | 'REVIEW';
    results: JobResultItem[] | ReviewResultItem[]; // Union type
}
// --- END ADDED ---


// Matches backend models/classification.py -> ProcessingStats and console log
export interface JobStatsData {
    job_id: string;
    company_name: string;
    start_time: string | null; // Assuming ISO string
    end_time: string | null; // Assuming ISO string
    processing_duration_seconds: number | null; // Renamed from processing_time
    total_vendors: number | null; // Added
    unique_vendors: number | null; // Added (was present in console)
    target_level: number | null; // Added target level to stats
    successfully_classified_l4: number | null; // Keep for reference
    successfully_classified_l5: number | null; // Keep L5 count
    classification_not_possible_initial: number | null; // Added
    invalid_category_errors: number | null; // Added (was present in console)
    search_attempts: number | null; // Added
    search_successful_classifications_l1: number | null; // Added
    search_successful_classifications_l5: number | null; // Renamed from search_assisted_l5
    api_usage: { // Nested structure
        openrouter_calls: number | null;
        openrouter_prompt_tokens: number | null;
        openrouter_completion_tokens: number | null;
        openrouter_total_tokens: number | null;
        tavily_search_calls: number | null;
        cost_estimate_usd: number | null;
    } | null; // Allow api_usage itself to be null if not populated
    // --- ADDED: Stats specific to REVIEW jobs ---
    reclassify_input?: Array<{ vendor_name: string; hint: string }>; // Input hints
    total_items_processed?: number;
    successful_reclassifications?: number;
    failed_reclassifications?: number;
    parent_job_id?: string; // Include parent ID in stats for review jobs
    // --- END ADDED ---
}


// Structure for download result helper
interface DownloadResult {
    blob: Blob;
    filename: string;
}

// Parameters for the job history list endpoint
interface GetJobsParams {
    status?: string;
    start_date?: string; // ISO string format
    end_date?: string; // ISO string format
    job_type?: 'CLASSIFICATION' | 'REVIEW'; // Filter by type
    skip?: number;
    limit?: number;
}

// --- ADDED: Password Reset Interfaces ---
// Matches backend schemas/password_reset.py -> MessageResponse
interface MessageResponse {
    message: string;
}
// --- END ADDED ---

// --- ADDED: Reclassification Interfaces ---
// Matches backend schemas/review.py -> ReclassifyRequestItem
interface ReclassifyRequestItemData {
    vendor_name: string;
    hint: string;
}
// Matches backend schemas/review.py -> ReclassifyResponse
interface ReclassifyResponseData {
    review_job_id: string;
    message: string;
}
// --- END ADDED ---


// --- Axios Instance Setup ---

const axiosInstance: AxiosInstance = axios.create({
    baseURL: '/api/v1', // Assumes Vite dev server proxies /api/v1 to your backend
    timeout: 60000, // 60 seconds timeout
    headers: {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
    },
});

// --- Request Interceptor (Add Auth Token) ---
axiosInstance.interceptors.request.use(
    (config: InternalAxiosRequestConfig) => {
        const authStore = useAuthStore();
        const token = authStore.getToken();
        // Define URLs that should NOT receive the auth token
        // --- UPDATED: Added /users/register ---
        const noAuthUrls = ['/auth/password-recovery', '/auth/reset-password', '/users/register'];
        // --- END UPDATED ---

        // Check if the request URL matches any of the no-auth URLs
        const requiresAuth = token && config.url && !noAuthUrls.some(url => config.url?.startsWith(url));

        if (requiresAuth) {
            // LOGGING: Log token presence and target URL
            // console.log(`[api.ts Request Interceptor] Adding token for URL: ${config.url}`);
            config.headers.Authorization = `Bearer ${token}`;
        } else {
            // console.log(`[api.ts Request Interceptor] No token added for URL: ${config.url} (Token: ${token ? 'present' : 'missing'}, No-Auth Match: ${!requiresAuth && !!token})`);
        }
        return config;
    },
    (error: AxiosError) => {
        console.error('[api.ts Request Interceptor] Error:', error);
        return Promise.reject(error);
    }
);

// --- Response Interceptor (Handle Errors) ---
axiosInstance.interceptors.response.use(
    (response) => {
        // LOGGING: Log successful response status and URL
        // console.log(`[api.ts Response Interceptor] Success for URL: ${response.config.url} | Status: ${response.status}`);
        return response;
    },
    (error: AxiosError) => {
        console.error('[api.ts Response Interceptor] Error:', error.config?.url, error.response?.status, error.message);
        const authStore = useAuthStore();

        if (error.response) {
            const { status, data } = error.response;

            // Handle 401 Unauthorized (except for login attempts and password reset)
            const isLoginAttempt = error.config?.url === '/token'; // Base URL for login
            // --- UPDATED: Check register url ---
            const isPublicAuthOperation = error.config?.url?.startsWith('/auth/') || error.config?.url?.startsWith('/users/register');
            // --- END UPDATED ---

            // --- UPDATED: Check isPublicAuthOperation ---
            if (status === 401 && !isLoginAttempt && !isPublicAuthOperation) {
            // --- END UPDATED ---
                console.warn('[api.ts Response Interceptor] Received 401 Unauthorized on protected route. Logging out.');
                authStore.logout(); // Trigger logout action
                // No reload here, let the component handle redirection or UI change
                return Promise.reject(new Error('Session expired. Please log in again.'));
            }

            // Extract detailed error message from response data
            let detailMessage = 'An error occurred.';
            const responseData = data as any;

            // Handle FastAPI validation errors (detail is an array)
            if (responseData?.detail && Array.isArray(responseData.detail)) {
                 detailMessage = `Validation Error: ${responseData.detail.map((err: any) => `${err.loc?.join('.') ?? 'field'}: ${err.msg}`).join('; ')}`;
            }
            // Handle other FastAPI errors (detail is a string) or custom errors
            else if (responseData?.detail && typeof responseData.detail === 'string') {
                detailMessage = responseData.detail;
            }
            // Handle cases where the error might be directly in the data object (less common)
            else if (typeof data === 'string' && data.length > 0 && data.length < 300) {
                detailMessage = data;
            }
            // Fallback to Axios error message
            else if (error.message) {
                detailMessage = error.message;
            }

            // Prepend status code for clarity, unless it's a 422 validation error where the message is usually sufficient
            const errorMessage = status === 422 ? detailMessage : `Error ${status}: ${detailMessage}`;
            console.error(`[api.ts Response Interceptor] Rejecting with error: ${errorMessage}`); // LOGGING
            return Promise.reject(new Error(errorMessage));

        } else if (error.request) {
            console.error('[api.ts Response Interceptor] Network error or no response received:', error.request);
            return Promise.reject(new Error('Network error or server did not respond. Please check connection.'));
        } else {
            console.error('[api.ts Response Interceptor] Axios setup error:', error.message);
            return Promise.reject(new Error(`Request setup error: ${error.message}`));
        }
    }
);


// --- API Service Object ---

const apiService = {
    /**
        * Logs in a user. Uses base axios for specific headers.
        */
    async login(usernameInput: string, passwordInput: string): Promise<AuthResponse> {
        const params = new URLSearchParams();
        params.append('username', usernameInput);
        params.append('password', passwordInput);
        console.log(`[api.ts login] Attempting login for user: ${usernameInput}`); // LOGGING
        // Use base axios to avoid default JSON headers and ensure correct Content-Type
        // Also avoids the interceptor adding an Authorization header if a previous token exists
        const response = await axios.post<AuthResponse>('/token', params, {
            baseURL: '/', // Use root base URL since '/token' is not under /api/v1
            headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
        });
        console.log(`[api.ts login] Login successful for user: ${usernameInput}`); // LOGGING
        return response.data;
    },

    /**
        * Validates the header of an uploaded file.
        */
    async validateUpload(formData: FormData): Promise<FileValidationResponse> {
        console.log('[api.ts validateUpload] Attempting file header validation...'); // LOGGING
        // Uses axiosInstance, includes auth token if available and URL requires it
        const response = await axiosInstance.post<FileValidationResponse>('/validate-upload', formData, {
             headers: { 'Content-Type': undefined } // Let browser set Content-Type for FormData
        });
        console.log(`[api.ts validateUpload] Validation response received: isValid=${response.data.is_valid}`); // LOGGING
        return response.data;
    },


    /**
        * Uploads the vendor file (after validation).
        * Returns the full JobResponse object.
        */
    async uploadFile(formData: FormData): Promise<JobResponse> { // Return JobResponse
        console.log('[api.ts uploadFile] Attempting file upload...'); // LOGGING
        // This uses axiosInstance, so /api/v1 prefix is added automatically
        const response = await axiosInstance.post<JobResponse>('/upload', formData, { // Expect JobResponse
                headers: { 'Content-Type': undefined } // Let browser set Content-Type for FormData
        });
        console.log(`[api.ts uploadFile] Upload successful, job ID: ${response.data.id}, Target Level: ${response.data.target_level}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches the status and details of a specific job.
        */
    async getJobStatus(jobId: string): Promise<JobDetails> {
        console.log(`[api.ts getJobStatus] Fetching status for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobDetails>(`/jobs/${jobId}`);
        console.log(`[api.ts getJobStatus] Received status for job ${jobId}:`, response.data.status, `Target Level: ${response.data.target_level}`, `Job Type: ${response.data.job_type}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches statistics for a specific job.
        */
    async getJobStats(jobId: string): Promise<JobStatsData> { // Use the updated interface here
        console.log(`[api.ts getJobStats] Fetching stats for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobStatsData>(`/jobs/${jobId}/stats`);
        // LOGGING: Log the received stats structure
        console.log(`[api.ts getJobStats] Received stats for job ${jobId}:`, JSON.parse(JSON.stringify(response.data)));
        return response.data;
    },

    /**
     * Fetches the detailed classification results for a specific job.
     * Returns the JobResultsResponse structure containing job type and results list.
     */
    async getJobResults(jobId: string): Promise<JobResultsResponse> {
        console.log(`[api.ts getJobResults] Fetching detailed results for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobResultsResponse>(`/jobs/${jobId}/results`);
        console.log(`[api.ts getJobResults] Received ${response.data.results.length} detailed result items for job ${jobId} (Type: ${response.data.job_type}).`); // LOGGING
        return response.data;
    },

    /**
        * Requests email notification for a job completion.
        */
    async requestNotification(jobId: string, email: string): Promise<NotifyResponse> {
        console.log(`[api.ts requestNotification] Requesting notification for job ${jobId} to email ${email}`); // LOGGING
        const response = await axiosInstance.post<NotifyResponse>(`/jobs/${jobId}/notify`, { email });
        console.log(`[api.ts requestNotification] Notification request response:`, response.data.success); // LOGGING
        return response.data;
    },

    /**
        * Downloads the results file for a completed job.
        */
    async downloadResults(jobId: string): Promise<DownloadResult> {
        console.log(`[api.ts downloadResults] Requesting download for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get(`/jobs/${jobId}/download`, {
            responseType: 'blob',
        });
        const disposition = response.headers['content-disposition'];
        let filename = `results_${jobId}.xlsx`;
        if (disposition?.includes('attachment')) {
            const filenameMatch = disposition.match(/filename\*?=(?:(?:"((?:[^"\\]|\\.)*)")|(?:([^;\n]*)))/i);
            if (filenameMatch?.[1]) { filename = filenameMatch[1].replace(/\\"/g, '"'); }
            else if (filenameMatch?.[2]) {
                    const utf8Match = filenameMatch[2].match(/^UTF-8''(.*)/i);
                    if (utf8Match?.[1]) { try { filename = decodeURIComponent(utf8Match[1]); } catch (e) { filename = utf8Match[1]; } }
                    else { filename = filenameMatch[2]; }
            }
        }
        console.log(`[api.ts downloadResults] Determined download filename: ${filename}`); // LOGGING
        return { blob: response.data as Blob, filename };
    },

    /**
        * Fetches a list of jobs for the current user, with optional filtering/pagination.
        */
    async getJobs(params: GetJobsParams = {}): Promise<JobResponse[]> {
        const cleanedParams = Object.fromEntries(
            Object.entries(params).filter(([, value]) => value !== undefined && value !== null && value !== '')
        );
        console.log('[api.ts getJobs] Fetching job list with params:', cleanedParams); // LOGGING
        const response = await axiosInstance.get<JobResponse[]>('/jobs/', { params: cleanedParams });
        console.log(`[api.ts getJobs] Received ${response.data.length} jobs.`); // LOGGING
        return response.data;
    },

    // --- User Management API Methods ---

    /**
        * Fetches the current logged-in user's details.
        */
    async getCurrentUser(): Promise<UserResponse> {
        console.log('[api.ts getCurrentUser] Fetching current user details...'); // LOGGING
        const response = await axiosInstance.get<UserResponse>('/users/me');
        console.log(`[api.ts getCurrentUser] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches a list of all users (admin only).
        */
    async getUsers(skip: number = 0, limit: number = 100): Promise<UserResponse[]> {
        console.log(`[api.ts getUsers] Fetching user list (skip: ${skip}, limit: ${limit})...`); // LOGGING
        const response = await axiosInstance.get<UserResponse[]>('/users/', { params: { skip, limit } });
         console.log(`[api.ts getUsers] Received ${response.data.length} users.`); // LOGGING
        return response.data;
    },

        /**
        * Fetches a specific user by ID (admin or self).
        */
        async getUserById(userId: string): Promise<UserResponse> {
        console.log(`[api.ts getUserById] Fetching user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.get<UserResponse>(`/users/${userId}`);
        console.log(`[api.ts getUserById] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Creates a new user (admin only).
        */
    async createUser(userData: UserCreateData): Promise<UserResponse> {
        console.log(`[api.ts createUser] Attempting to create user (admin): ${userData.username}`); // LOGGING
        const response = await axiosInstance.post<UserResponse>('/users/', userData);
        console.log(`[api.ts createUser] User created successfully (admin): ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Updates a user (admin or self).
        */
    async updateUser(userId: string, userData: UserUpdateData): Promise<UserResponse> {
        console.log(`[api.ts updateUser] Attempting to update user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.put<UserResponse>(`/users/${userId}`, userData);
        console.log(`[api.ts updateUser] User updated successfully: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Deletes a user (admin only).
        */
    async deleteUser(userId: string): Promise<{ message: string }> {
        console.log(`[api.ts deleteUser] Attempting to delete user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.delete<{ message: string }>(`/users/${userId}`);
        console.log(`[api.ts deleteUser] User delete response: ${response.data.message}`); // LOGGING
        return response.data;
    },
    // --- END User Management API Methods ---

    // --- ADDED: Public Registration API Method ---
    /**
     * Registers a new user publicly.
     */
    async registerUser(userData: UserCreateData): Promise<UserResponse> {
        console.log(`[api.ts registerUser] Attempting public registration for user: ${userData.username}`);
        // Uses axiosInstance, interceptor skips auth token for this URL
        const response = await axiosInstance.post<UserResponse>('/users/register', userData);
        console.log(`[api.ts registerUser] Public registration successful: ${response.data.username}`);
        return response.data;
    },
    // --- END Public Registration API Method ---


    // --- ADDED: Password Reset API Methods ---
    /**
     * Requests a password reset email to be sent.
     */
    async requestPasswordRecovery(email: string): Promise<MessageResponse> {
        console.log(`[api.ts requestPasswordRecovery] Requesting password reset for email: ${email}`);
        // This uses axiosInstance, but the interceptor should skip adding auth token for this URL
        const response = await axiosInstance.post<MessageResponse>('/auth/password-recovery', { email });
        console.log(`[api.ts requestPasswordRecovery] Request response: ${response.data.message}`);
        return response.data;
    },

    /**
     * Resets the password using the provided token and new password.
     */
    async resetPassword(token: string, newPassword: string): Promise<MessageResponse> {
        console.log(`[api.ts resetPassword] Attempting password reset with token: ${token.substring(0, 10)}...`);
        // This uses axiosInstance, but the interceptor should skip adding auth token for this URL
        const response = await axiosInstance.post<MessageResponse>('/auth/reset-password', {
            token: token,
            new_password: newPassword
        });
        console.log(`[api.ts resetPassword] Reset response: ${response.data.message}`);
        return response.data;
    },
    // --- END Password Reset API Methods ---

    // --- ADDED: Reclassification API Method ---
    /**
     * Submits flagged items for reclassification.
     */
    async reclassifyJob(originalJobId: string, items: ReclassifyRequestItemData[]): Promise<ReclassifyResponseData> {
        console.log(`[api.ts reclassifyJob] Submitting ${items.length} items for reclassification for job ${originalJobId}`);
        const payload = { items: items };
        const response = await axiosInstance.post<ReclassifyResponseData>(`/jobs/${originalJobId}/reclassify`, payload);
        console.log(`[api.ts reclassifyJob] Reclassification job started: ${response.data.review_job_id}`);
        return response.data;
    }
    // --- END ADDED ---
};

export default apiService;
</file>

<file path='frontend/vue_frontend/src/stores/job.ts'>
// <file path='frontend/vue_frontend/src/stores/job.ts'>
import { defineStore } from 'pinia';
import { ref, reactive, computed } from 'vue';
// Removed JobResultsResponse from here
import apiService, { type JobResponse } from '@/services/api';

// Define the structure of the job details object based on your API response
// Should align with app/schemas/job.py -> JobResponse
export interface JobDetails {
    id: string;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string;
    created_at: string | null;
    updated_at: string | null;
    completed_at?: string | null;
    estimated_completion?: string | null;
    error_message: string | null;
    target_level: number;
    company_name?: string;
    input_file_name?: string;
    output_file_name?: string | null;
    created_by?: string;
    job_type: 'CLASSIFICATION' | 'REVIEW';
    parent_job_id: string | null;
}

// Interface for a single detailed result item (for CLASSIFICATION jobs)
// Should align with app/schemas/job.py -> JobResultItem
export interface JobResultItem {
    vendor_name: string;
    level1_id: string | null;
    level1_name: string | null;
    level2_id: string | null;
    level2_name: string | null;
    level3_id: string | null;
    level3_name: string | null;
    level4_id: string | null;
    level4_name: string | null;
    level5_id: string | null;
    level5_name: string | null;
    final_confidence: number | null;
    final_status: string; // 'Classified', 'Not Possible', 'Error'
    classification_source: string | null; // 'Initial', 'Search', 'Review'
    classification_notes_or_reason: string | null;
    achieved_level: number | null; // 0-5
}

// Interface for a single detailed result item (for REVIEW jobs)
// Should align with app/schemas/review.py -> ReviewResultItem
export interface ReviewResultItem {
    vendor_name: string;
    hint: string;
    original_result: JobResultItem; // Use JobResultItem type for structure
    new_result: JobResultItem; // Use JobResultItem type for structure
}


export const useJobStore = defineStore('job', () => {
    // --- State ---
    const currentJobId = ref<string | null>(null);
    const jobDetails = ref<JobDetails | null>(null);
    const isLoading = ref(false); // For tracking polling/loading state for CURRENT job details
    const error = ref<string | null>(null); // For storing errors related to fetching CURRENT job status

    const jobHistory = ref<JobResponse[]>([]);
    const historyLoading = ref(false);
    const historyError = ref<string | null>(null);

    // --- UPDATED: Results State ---
    // Holds the primary results for the currentJobId (JobResultItem[] or ReviewResultItem[])
    const jobResults = ref<JobResultItem[] | ReviewResultItem[] | null>(null);
    // Holds results from the latest review job *if* currentJobId is a CLASSIFICATION job
    const relatedReviewResults = ref<ReviewResultItem[] | null>(null); // NEW
    const resultsLoading = ref(false);
    const resultsError = ref<string | null>(null);
    // --- END UPDATED ---

    const flaggedForReview = reactive<Map<string, { hint: string | null }>>(new Map());
    const reclassifyLoading = ref(false);
    const reclassifyError = ref<string | null>(null);
    const lastReviewJobId = ref<string | null>(null);

    // --- Computed ---
    const hasFlaggedItems = computed(() => flaggedForReview.size > 0);

    // --- Actions ---
    function setCurrentJobId(jobId: string | null): void {
        console.log(`JobStore: Setting currentJobId from '${currentJobId.value}' to '${jobId}'`);
        if (currentJobId.value !== jobId) {
            currentJobId.value = jobId;
            // Clear details and results when ID changes
            jobDetails.value = null;
            jobResults.value = null;
            relatedReviewResults.value = null; // Clear related results too
            console.log(`JobStore: Cleared jobDetails and results due to ID change.`);
            error.value = null;
            isLoading.value = false;
            resultsLoading.value = false; // Reset results loading
            resultsError.value = null; // Reset results error
            // Clear flagging state
            flaggedForReview.clear();
            reclassifyLoading.value = false;
            reclassifyError.value = null;
            lastReviewJobId.value = null;
            console.log(`JobStore: Cleared flagging state due to ID change.`);

            // Update URL
            try {
                 const url = new URL(window.location.href);
                 if (jobId) {
                     url.searchParams.set('job_id', jobId);
                     console.log(`JobStore: Updated URL searchParam 'job_id' to ${jobId}`);
                 } else {
                     url.searchParams.delete('job_id');
                     console.log(`JobStore: Removed 'job_id' from URL searchParams.`);
                 }
                 window.history.replaceState({}, '', url.toString());
            } catch (e) {
                 console.error("JobStore: Failed to update URL:", e);
            }
        }
         // If the same job ID is set again, force a refresh
         else if (jobId !== null) {
             console.log(`JobStore: Re-setting same job ID ${jobId}, clearing details and results to force refresh.`);
             jobDetails.value = null;
             jobResults.value = null;
             relatedReviewResults.value = null; // Clear related results too
             error.value = null;
             isLoading.value = false;
             resultsLoading.value = false;
             resultsError.value = null;
             // Clear flagging state
             flaggedForReview.clear();
             reclassifyLoading.value = false;
             reclassifyError.value = null;
             lastReviewJobId.value = null;
         }
    }

    function updateJobDetails(details: JobDetails): void {
        if (details && details.id === currentJobId.value) {
            console.log(`JobStore: Updating jobDetails for ${currentJobId.value} with status ${details.status}, progress ${details.progress}, target_level ${details.target_level}, job_type ${details.job_type}`);
            // Check if job type changed (e.g., initial load vs poll update)
            const typeChanged = jobDetails.value?.job_type !== details.job_type;
            jobDetails.value = { ...details };
            error.value = null;
            // If type changed, results might need refetching/reinterpreting
            if (typeChanged) {
                console.log("JobStore: Job type changed, clearing existing results.");
                jobResults.value = null;
                relatedReviewResults.value = null;
                resultsLoading.value = false;
                resultsError.value = null;
                // Trigger results fetch again? Or let the calling component handle it.
                // Let's assume the component watching jobDetails will trigger fetchCurrentJobResults.
            }
        } else if (details) {
            console.warn(`JobStore: Received details for job ${details.id}, but currently tracking ${currentJobId.value}. Ignoring update.`);
        } else {
            console.warn(`JobStore: updateJobDetails called with invalid details object.`);
        }
    }

    function setLoading(loading: boolean): void {
        isLoading.value = loading;
    }

    function setError(errorMessage: string | null): void {
        error.value = errorMessage;
    }

    function clearJob(): void {
        console.log('JobStore: Clearing job state.');
        setCurrentJobId(null);
    }

    async function fetchJobHistory(params = {}): Promise<void> {
        console.log('JobStore: Fetching job history with params:', params);
        historyLoading.value = true;
        historyError.value = null;
        try {
            const jobs = await apiService.getJobs(params);
            jobHistory.value = jobs;
            console.log(`JobStore: Fetched ${jobs.length} jobs.`);
        } catch (err: any) {
            console.error('JobStore: Failed to fetch job history:', err);
            historyError.value = err.message || 'Failed to load job history.';
            jobHistory.value = [];
        } finally {
            historyLoading.value = false;
        }
    }

    // --- NEW HELPER: Find latest completed review job for a parent ---
    function findLatestReviewJobFor(parentId: string): JobResponse | null {
        const completedReviewJobs = jobHistory.value
            .filter(job =>
                job.parent_job_id === parentId &&
                job.job_type === 'REVIEW' &&
                job.status === 'completed' &&
                job.completed_at // Ensure completed_at is not null
            )
            .sort((a, b) => {
                // Sort descending by completed_at date
                const dateA = a.completed_at ? new Date(a.completed_at) : new Date(0);
                const dateB = b.completed_at ? new Date(b.completed_at) : new Date(0);
                return dateB.getTime() - dateA.getTime();
            });

        if (completedReviewJobs.length > 0) {
            console.log(`JobStore: Found latest review job ${completedReviewJobs[0].id} for parent ${parentId}`);
            return completedReviewJobs[0];
        }
        return null;
    }
    // --- END NEW HELPER ---

    // --- UPDATED: Action to fetch results based on current job type ---
    async function fetchCurrentJobResults(): Promise<void> {
        if (!currentJobId.value || !jobDetails.value) {
            console.warn("JobStore: fetchCurrentJobResults called without currentJobId or jobDetails.");
            resultsLoading.value = false; // Ensure loading is false if we exit early
            return;
        }
        // Avoid redundant fetches
        if (resultsLoading.value) {
            console.log("JobStore: fetchCurrentJobResults called while already loading. Skipping.");
            return;
        }

        const jobId = currentJobId.value;
        const jobType = jobDetails.value.job_type;

        console.log(`JobStore: Starting fetchCurrentJobResults for ${jobId} (Type: ${jobType})`);

        // Clear previous results
        jobResults.value = null;
        relatedReviewResults.value = null;
        resultsLoading.value = true;
        resultsError.value = null;

        try {
            // Fetch primary results for the current job ID
            console.log(`JobStore: Fetching primary results for ${jobId}...`);
            // The actual response type from apiService.getJobResults is inferred here
            const primaryResponse = await apiService.getJobResults(jobId);

            // Check if job ID changed *during* the API call
            if (jobId !== currentJobId.value) {
                 console.log(`JobStore: Job ID changed while fetching primary results for ${jobId}. Discarding.`);
                 resultsLoading.value = false; // Ensure loading is reset
                 return; // Exit early
            }

            // Store primary results (casting based on expected type)
            if (jobType === 'CLASSIFICATION') {
                jobResults.value = primaryResponse.results as JobResultItem[];
            } else if (jobType === 'REVIEW') {
                jobResults.value = primaryResponse.results as ReviewResultItem[];
            } else {
                console.error(`JobStore: Unknown job type '${jobType}' encountered.`);
                jobResults.value = primaryResponse.results; // Store as is, might cause issues downstream
            }
            console.log(`JobStore: Successfully fetched ${primaryResponse.results.length} primary results for ${jobId}.`);

            // If it's a CLASSIFICATION job, try to find and fetch the latest *completed* review results
            if (jobType === 'CLASSIFICATION') {
                // Ensure job history is available. Fetch if needed?
                // For simplicity, assume history is reasonably up-to-date when this is called.
                // If not, add: if (jobHistory.value.length === 0) await fetchJobHistory();
                const latestReviewJob = findLatestReviewJobFor(jobId);
                if (latestReviewJob) {
                    console.log(`JobStore: Found related completed review job: ${latestReviewJob.id}. Fetching its results...`);
                    try {
                        // The actual response type from apiService.getJobResults is inferred here
                        const reviewResponse = await apiService.getJobResults(latestReviewJob.id);
                         // Check if job ID changed *during* this second API call
                        if (jobId !== currentJobId.value) {
                             console.log(`JobStore: Job ID changed while fetching related review results for ${jobId}. Discarding.`);
                             // Don't reset loading here, let the finally block handle it
                             return; // Exit early
                        }
                        // Ensure the response is ReviewResultItem[]
                        if (latestReviewJob.job_type === 'REVIEW') { // Sanity check
                             relatedReviewResults.value = reviewResponse.results as ReviewResultItem[];
                             console.log(`JobStore: Successfully fetched ${reviewResponse.results.length} related review results for ${jobId} from review job ${latestReviewJob.id}`);
                        } else {
                             console.warn(`JobStore: Found related job ${latestReviewJob.id}, but it's not a REVIEW job type.`);
                        }
                    } catch (reviewErr: any) {
                        console.error(`JobStore: Failed to fetch results for related review job ${latestReviewJob.id}:`, reviewErr);
                        // Don't set the main resultsError, just log it.
                        // Clear related results if fetch failed
                        relatedReviewResults.value = null;
                    }
                } else {
                     console.log(`JobStore: No related completed review job found for classification job ${jobId}.`);
                     relatedReviewResults.value = null; // Ensure it's null if none found
                }
            }
        } catch (err: any) {
            console.error(`JobStore: Failed to fetch primary results for ${jobId}:`, err);
            // Only set error if it's for the currently selected job
            if (jobId === currentJobId.value) {
                resultsError.value = err.message || 'Failed to load results.';
                jobResults.value = null; // Clear results on primary fetch error
                relatedReviewResults.value = null; // Clear related results too
            }
        } finally {
             // Only stop loading if it's for the currently selected job
            if (jobId === currentJobId.value) {
                resultsLoading.value = false;
                console.log(`JobStore: Finished fetchCurrentJobResults for ${jobId}. Loading: ${resultsLoading.value}`);
            } else {
                 console.log(`JobStore: Finished fetchCurrentJobResults for ${jobId}, but current job is now ${currentJobId.value}. Loading state not updated for current job.`);
            }
        }
    }
    // --- END UPDATED ---

    // --- Reclassification Actions (Minor Adjustments) ---
    function isFlagged(vendorName: string): boolean {
        return flaggedForReview.has(vendorName);
    }

    function getHint(vendorName: string): string | null {
        return flaggedForReview.get(vendorName)?.hint ?? null;
    }

    function flagVendor(vendorName: string, initialHint: string | null = null): void { // Added optional initial hint
        if (!flaggedForReview.has(vendorName)) {
            flaggedForReview.set(vendorName, { hint: initialHint }); // Use initial hint if provided
            console.log(`JobStore: Flagged vendor '${vendorName}' for review.`);
        }
    }

    function unflagVendor(vendorName: string): void {
        if (flaggedForReview.has(vendorName)) {
            flaggedForReview.delete(vendorName);
            console.log(`JobStore: Unflagged vendor '${vendorName}'.`);
        }
    }

    function setHint(vendorName: string, hint: string | null): void {
        if (flaggedForReview.has(vendorName)) {
            flaggedForReview.set(vendorName, { hint });
            console.log(`JobStore: Set hint for '${vendorName}': ${hint ? `'${hint}'` : 'cleared'}`);
        } else {
            console.warn(`JobStore: Tried to set hint for unflagged vendor '${vendorName}'.`);
        }
    }

    async function submitFlagsForReview(): Promise<string | null> {
        // --- Ensure submission always uses the original classification job ID ---
        let submissionJobId = currentJobId.value;
        // If the current job is a REVIEW job, find its parent (original CLASSIFICATION job)
        if (jobDetails.value?.job_type === 'REVIEW' && jobDetails.value?.parent_job_id) {
            submissionJobId = jobDetails.value.parent_job_id;
            console.log(`JobStore: Current job is REVIEW type, submitting flags against parent job ${submissionJobId}`);
        } else if (jobDetails.value?.job_type === 'CLASSIFICATION') {
             console.log(`JobStore: Current job is CLASSIFICATION type, submitting flags against job ${submissionJobId}`);
        } else {
             console.error("JobStore: Cannot determine original job ID for submitting flags.");
             reclassifyError.value = "Cannot determine the original job to reclassify from.";
             return null;
        }

        if (!submissionJobId || flaggedForReview.size === 0) {
            console.warn("JobStore: submitFlagsForReview called with no submission job ID or no flagged items.");
            reclassifyError.value = "No items flagged for review.";
            return null;
        }
        // --- End Ensure submission job ID ---

        const itemsToReclassify = Array.from(flaggedForReview.entries())
            .filter(([_, data]) => data.hint && data.hint.trim() !== '')
            .map(([vendorName, data]) => ({
                vendor_name: vendorName,
                hint: data.hint!,
            }));

        if (itemsToReclassify.length === 0) {
            console.warn("JobStore: submitFlagsForReview called, but no flagged items have valid hints.");
            reclassifyError.value = "Please provide hints for the flagged items before submitting.";
            return null;
        }

        console.log(`JobStore: Submitting ${itemsToReclassify.length} flags for reclassification against job ${submissionJobId}...`);
        reclassifyLoading.value = true;
        reclassifyError.value = null;
        lastReviewJobId.value = null;

        try {
            // Use the determined submissionJobId
            const response = await apiService.reclassifyJob(submissionJobId, itemsToReclassify);
            console.log(`JobStore: Reclassification job started successfully. Review Job ID: ${response.review_job_id}`);
            lastReviewJobId.value = response.review_job_id;
            flaggedForReview.clear(); // Clear flags after successful submission
            // Optionally: Fetch job history again AFTER a short delay to show the new PENDING review job
            setTimeout(() => fetchJobHistory(), 1000); // Refresh history after 1s
            return response.review_job_id;
        } catch (err: any) {
            console.error('JobStore: Failed to submit flags for reclassification:', err);
            reclassifyError.value = err.message || 'Failed to start reclassification job.';
            return null;
        } finally {
            reclassifyLoading.value = false;
        }
    }
    // --- END Reclassification Actions ---


    return {
        // State
        currentJobId,
        jobDetails,
        isLoading,
        error,
        jobHistory,
        historyLoading,
        historyError,
        jobResults, // Holds JobResultItem[] or ReviewResultItem[]
        relatedReviewResults, // Holds ReviewResultItem[] or null
        resultsLoading,
        resultsError,
        flaggedForReview,
        reclassifyLoading,
        reclassifyError,
        lastReviewJobId,
        hasFlaggedItems,

        // Actions
        setCurrentJobId,
        updateJobDetails,
        setLoading,
        setError,
        clearJob,
        fetchJobHistory,
        fetchCurrentJobResults, // Use this action to fetch results
        // fetchJobResults, // Keep original name if preferred, but fetchCurrentJobResults is clearer
        isFlagged,
        getHint,
        flagVendor,
        unflagVendor,
        setHint,
        submitFlagsForReview,
    };
});
</file>

</Concatenated Source Code>