<goal or issue to address>
we need the job stats to reflect that each job can have a different depth level
</goal or issue to address>


<output instruction>
1) Reflect on 5-7 different possible sources of the problem based on the code provided and the goal/issue description.
2) Distill those down to the most likely root cause.
3) Provide the COMPLETE UPDATED VERSION of *only* the files that need changes to fix the likely root cause.

</output instruction>


<Tree of Included Files>
- app/api/jobs.py
- app/api/main.py
- app/models/classification.py
- app/models/job.py
- app/models/taxonomy.py
- app/schemas/job.py
- app/tasks/classification_logic.py
- app/tasks/classification_prompts.py
- app/tasks/classification_tasks.py
- app/utils/taxonomy_loader.py
- frontend/vue_frontend/src/components/JobHistory.vue
- frontend/vue_frontend/src/components/JobStats.vue
- frontend/vue_frontend/src/components/JobStatus.vue
- frontend/vue_frontend/src/components/UploadForm.vue
- frontend/vue_frontend/src/services/api.ts
- frontend/vue_frontend/src/stores/job.ts
</Tree of Included Files>


<Concatenated Source Code>

<file path='app/api/jobs.py'>
# <file path='app/api/jobs.py'>
# app/api/jobs.py
from fastapi import APIRouter, Depends, HTTPException, Query, status, Path # Added Path
from sqlalchemy.orm import Session
from typing import List, Optional, Dict
from datetime import datetime
import logging # Import logging

from core.database import get_db
from api.auth import get_current_user
from models.user import User
from models.job import Job, JobStatus
from schemas.job import JobResponse # Import the new schema
from core.logging_config import get_logger
from core.log_context import set_log_context
from core.config import settings # Need settings for file path construction


logger = get_logger("vendor_classification.api.jobs")
logger.debug("Successfully imported Dict from typing for jobs API.")

router = APIRouter()

@router.get("/", response_model=List[JobResponse])
async def list_jobs(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    status_filter: Optional[JobStatus] = Query(None, alias="status", description="Filter jobs by status"),
    start_date: Optional[datetime] = Query(None, description="Filter jobs created on or after this date (ISO format)"),
    end_date: Optional[datetime] = Query(None, description="Filter jobs created on or before this date (ISO format)"),
    skip: int = Query(0, ge=0, description="Number of jobs to skip for pagination"),
    limit: int = Query(100, ge=1, le=500, description="Maximum number of jobs to return"),
):
    """
    List jobs for the current user. Admins can see all jobs (optional enhancement).
    Supports filtering by status and date range, and pagination.
    """
    set_log_context({"username": current_user.username})
    logger.info("Fetching job history", extra={
        "status_filter": status_filter,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "skip": skip,
        "limit": limit,
    })

    query = db.query(Job)

    # Filter by user (Admins could potentially see all - add logic here if needed)
    # For now, all users only see their own jobs
    # if not current_user.is_superuser: # Example admin check
    query = query.filter(Job.created_by == current_user.username)

    # Apply filters
    if status_filter:
        query = query.filter(Job.status == status_filter.value)
    if start_date:
        query = query.filter(Job.created_at >= start_date)
    if end_date:
        # Add a day to end_date to make it inclusive of the whole day if time is not specified
        # Or adjust based on desired behavior (e.g., end_date < end_date + timedelta(days=1))
        query = query.filter(Job.created_at <= end_date)

    # Order by creation date (newest first)
    query = query.order_by(Job.created_at.desc())

    # Apply pagination
    jobs = query.offset(skip).limit(limit).all()

    logger.info(f"Retrieved {len(jobs)} jobs from history.")

    # Convert Job models to JobResponse schemas
    # Pydantic v2 handles this automatically with from_attributes=True
    return jobs

@router.get("/{job_id}", response_model=JobResponse)
async def read_job(
    # Use Path to ensure job_id is correctly extracted from the URL path
    job_id: str = Path(..., title="The ID of the job to get"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve details for a specific job by its ID.
    Ensures the current user owns the job (or is an admin - future enhancement).
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching details for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # --- Authorization Check ---
    # Ensure the user requesting the job is the one who created it
    # (Or add admin override logic here if needed)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access this job")
    # --- End Authorization Check ---

    # LOGGING: Log the job details being returned, especially target_level
    logger.info(f"Returning details for job ID: {job_id}", extra={"job_status": job.status, "target_level": job.target_level})
    return job # Pydantic will validate against JobResponse

# Use Dict for flexibility, or create a specific StatsResponse schema later if needed
@router.get("/{job_id}/stats", response_model=Dict)
async def read_job_stats(
    job_id: str = Path(..., title="The ID of the job to get stats for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve processing statistics for a specific job.
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching statistics for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for stats", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check (same as read_job)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access stats for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access stats for this job")

    # LOGGING: Log the raw stats being returned from the database
    logger.info(f"Returning statistics for job ID: {job_id}")
    logger.debug(f"Raw stats from DB for job {job_id}: {job.stats}") # Log the actual stats dict

    # The stats are stored as JSON in the Job model
    return job.stats if job.stats else {}

@router.get("/{job_id}/download")
async def download_job_results(
    job_id: str = Path(..., title="The ID of the job to download results for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Downloads the output Excel file for a completed job.
    """
    from fastapi.responses import FileResponse # Import here
    import os # Import os

    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Request to download results for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for download", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted download for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to download results for this job")

    if job.status != JobStatus.COMPLETED.value or not job.output_file_name:
        logger.warning(f"Download requested but job not completed or output file missing",
                       extra={"job_id": job_id, "status": job.status, "output_file": job.output_file_name})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Job not completed or output file not available.")

    # Construct the full path to the output file
    output_dir = os.path.join(settings.OUTPUT_DATA_DIR, job_id)
    file_path = os.path.join(output_dir, job.output_file_name)

    if not os.path.exists(file_path):
         logger.error(f"Output file record exists in DB but file not found on disk",
                      extra={"job_id": job_id, "expected_path": file_path})
         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Output file not found.")

    logger.info(f"Streaming output file for download",
                extra={"job_id": job_id, "file_path": file_path})
    return FileResponse(
        path=file_path,
        filename=job.output_file_name, # Suggest filename to browser
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )
</file>

<file path='app/api/main.py'>

# app/api/main.py
import socket
import sqlalchemy
import httpx
from fastapi import (
    FastAPI, Depends, HTTPException, UploadFile, File, Form,
    BackgroundTasks, status, Request
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from typing import Dict, Any, Optional, List
import uuid
import os
from datetime import datetime, timedelta
import logging
import time
from sqlalchemy.orm import Session

# --- Model Imports ---
from models.job import Job, JobStatus, ProcessingStage
from models.user import User

# --- Core Imports ---
from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import setup_logging, get_logger
from core.log_context import set_correlation_id, set_user, set_job_id, get_correlation_id
# Import middleware (which now uses updated context functions)
from middleware.logging_middleware import RequestLoggingMiddleware
from core.database import get_db, SessionLocal, engine
from core.initialize_db import initialize_database

# --- Service Imports ---
from services.file_service import save_upload_file

# --- Task Imports ---
from tasks.celery_app import celery_app
from tasks.classification_tasks import process_vendor_file

# --- Utility Imports ---
from utils.taxonomy_loader import load_taxonomy

# --- Auth Imports ---
from fastapi.security import OAuth2PasswordRequestForm
from api.auth import (
    get_current_user,
    authenticate_user,
    create_access_token,
    get_current_active_user
)

# --- Router Imports ---
from api import jobs as jobs_router
from api import users as users_router

# --- Schema Imports ---
from schemas.job import JobResponse
from schemas.user import UserResponse as UserResponseSchema

# --- Logging Setup ---
# Initialize logging BEFORE creating the FastAPI app instance
# This ensures loggers are ready when middleware/routers are attached
setup_logging(log_level=logging.DEBUG, log_to_file=True, log_dir=settings.TAXONOMY_DATA_DIR.replace('taxonomy', 'logs')) # Use settings for log dir
logger = get_logger("vendor_classification.api")

# --- FastAPI App Initialization ---
app = FastAPI(
    title="NAICS Vendor Classification API",
    description="API for classifying vendors according to NAICS taxonomy",
    version="1.0.0",
)

# --- Middleware ---
app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Allow all origins for now, restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Include Routers ---
logger.info("Including API routers...")
app.include_router(
    jobs_router.router,
    prefix="/api/v1/jobs",
    tags=["Jobs"],
    dependencies=[Depends(get_current_user)]
)
logger.info("Included jobs router with prefix /api/v1/jobs")

app.include_router(
    users_router.router,
    prefix="/api/v1/users",
    tags=["Users"],
)
logger.info("Included users router with prefix /api/v1/users")
# --- End Include Routers ---

# --- Vue.js Frontend Serving Setup ---
VUE_BUILD_DIR = "/app/frontend/dist"
VUE_INDEX_FILE = os.path.join(VUE_BUILD_DIR, "index.html")
logger.info(f"Attempting to serve Vue frontend from: {VUE_BUILD_DIR}")
if not os.path.exists(VUE_BUILD_DIR):
    logger.error(f"Vue build directory NOT FOUND at {VUE_BUILD_DIR}. Frontend will not be served.")
elif not os.path.exists(VUE_INDEX_FILE):
    logger.error(f"Vue index.html NOT FOUND at {VUE_INDEX_FILE}. Frontend serving might fail.")
else:
    logger.info(f"Vue build directory and index.html found. Static files will be mounted.")

# --- API ROUTES ---

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    hostname = socket.gethostname()
    local_ip = ""
    try:
        local_ip = socket.gethostbyname(hostname)
    except socket.gaierror:
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
            s.close()
        except Exception:
                local_ip = "Could not resolve IP"

    logger.info(f"Health check called", extra={"hostname": hostname, "ip": local_ip})
    db_status = "unknown"
    db = None
    try:
        db = SessionLocal()
        db.execute(sqlalchemy.text("SELECT 1"))
        db_status = "connected"
    except Exception as e:
        logger.error(f"Health Check: Database connection error", exc_info=True, extra={"error_details": str(e)})
        db_status = f"error: {str(e)[:100]}"
    finally:
        if db:
            db.close()

    vue_frontend_status = "found" if os.path.exists(VUE_INDEX_FILE) else "missing"

    celery_broker_status = "unknown"
    celery_connection = None
    try:
        celery_connection = celery_app.connection(heartbeat=2.0)
        celery_connection.ensure_connection(max_retries=1, timeout=2)
        celery_broker_status = "connected"
    except Exception as celery_e:
        logger.error(f"Celery broker connection error during health check: {str(celery_e)}", exc_info=False)
        celery_broker_status = f"error: {str(celery_e)[:100]}"
    finally:
            if celery_connection:
                try: celery_connection.close()
                except Exception as close_err: logger.warning(f"Error closing celery connection in health check: {close_err}")

    openrouter_status = "unknown"
    tavily_status = "unknown"
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
                or_url = f"{settings.OPENROUTER_API_BASE}/models"
                or_headers = {"Authorization": f"Bearer {settings.OPENROUTER_API_KEY}"}
                or_resp = await client.get(or_url, headers=or_headers)
                openrouter_status = "connected" if or_resp.status_code == 200 else f"error: {or_resp.status_code}"

                tv_url = "https://api.tavily.com/search"
                tv_payload = {"api_key": settings.TAVILY_API_KEY, "query": "test", "max_results": 1}
                tv_resp = await client.post(tv_url, json=tv_payload)
                tavily_status = "connected" if tv_resp.status_code == 200 else f"error: {tv_resp.status_code}"

    except httpx.RequestError as http_err:
            logger.warning(f"HTTPX RequestError during external API health check: {http_err}")
            openrouter_status = openrouter_status if openrouter_status != "unknown" else "connection_error"
            tavily_status = tavily_status if tavily_status != "unknown" else "connection_error"
    except Exception as api_err:
            logger.error(f"Error checking external APIs during health check: {api_err}")
            openrouter_status = openrouter_status if openrouter_status != "unknown" else "check_error"
            tavily_status = tavily_status if tavily_status != "unknown" else "check_error"

    return {
        "status": "healthy",
        "hostname": hostname,
        "ip": local_ip,
        "database": db_status,
        "celery_broker": celery_broker_status,
        "vue_frontend_index": vue_frontend_status,
        "external_api_openrouter": openrouter_status,
        "external_api_tavily": tavily_status,
        "timestamp": datetime.now().isoformat()
    }


# --- Exception Handlers ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    correlation_id = get_correlation_id() or str(uuid.uuid4())
    try: body_preview = str(await request.body())[:500]
    except Exception: body_preview = "[Could not read request body]"
    logger.error("Request validation failed (422)", extra={
        "error_details": exc.errors(), "request_body_preview": body_preview,
        "request_headers": dict(request.headers), "correlation_id": correlation_id,
        "path": request.url.path
    })
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": exc.errors()},
        headers={"X-Correlation-ID": correlation_id}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    correlation_id = get_correlation_id() or str(uuid.uuid4())
    logger.error(f"Unhandled exception during request to {request.url.path}", exc_info=True, extra={
        "correlation_id": correlation_id, "request_headers": dict(request.headers),
        "path": request.url.path, "method": request.method,
    })
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": "An internal server error occurred.", "correlation_id": correlation_id},
        headers={"X-Correlation-ID": correlation_id}
    )


# --- Authentication Endpoint ---
@app.post("/token", response_model=Dict[str, Any])
async def login_for_access_token(
    request: Request,
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: Session = Depends(get_db)
):
    """Handles user login and returns JWT token and user details."""
    correlation_id = str(uuid.uuid4())
    set_correlation_id(correlation_id)
    client_host = request.client.host if request.client else "Unknown"
    logger.info(f"Login attempt", extra={"username": form_data.username, "ip": client_host})

    try:
        user = authenticate_user(db, form_data.username, form_data.password)
        if not user:
            logger.warning(f"Login failed: invalid credentials", extra={"username": form_data.username, "ip": client_host})
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password",
                headers={"WWW-Authenticate": "Bearer"},
            )

        if not user.is_active:
                logger.warning(f"Login failed: user '{user.username}' is inactive.", extra={"ip": client_host})
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Inactive user.",
                )

        set_user(user) # Set context for logging
        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
        access_token = create_access_token(
            data={"sub": user.username}, expires_delta=access_token_expires
        )

        logger.info(f"Login successful, token generated", extra={ "username": user.username, "ip": client_host, "token_expires_in_minutes": settings.ACCESS_TOKEN_EXPIRE_MINUTES})

        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": UserResponseSchema.model_validate(user)
        }

    except HTTPException as http_exc:
        if http_exc.status_code not in [status.HTTP_401_UNAUTHORIZED, status.HTTP_400_BAD_REQUEST]:
                logger.error(f"HTTP exception during login", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"Unexpected login error", exc_info=True, extra={"error": str(e), "username": form_data.username})
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An error occurred during the login process."
        )

# --- UPLOAD ROUTE (Updated) ---
@app.post("/api/v1/upload", response_model=JobResponse, status_code=status.HTTP_202_ACCEPTED)
async def upload_vendor_file(
    background_tasks: BackgroundTasks,
    company_name: str = Form(...),
    # --- ADDED: target_level parameter ---
    target_level: int = Form(..., ge=1, le=5, description="Target classification level (1-5)"),
    # --- END ADDED ---
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Accepts vendor file upload, creates a job, and queues it for processing.
    Allows specifying the target classification level.
    """
    job_id = str(uuid.uuid4())
    set_job_id(job_id)
    set_user(current_user)

    logger.info(f"Upload request received", extra={
        "job_id": job_id,
        "company_name": company_name,
        "target_level": target_level, # Log the target level
        "uploaded_filename": file.filename,
        "content_type": file.content_type,
        "username": current_user.username
    })

    if not file.filename:
        logger.warning("Upload attempt with no filename.", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No filename provided.")
    if not file.filename.lower().endswith(('.xlsx', '.xls')):
        logger.warning(f"Invalid file type uploaded: {file.filename}", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid file type. Please upload an Excel file (.xlsx or .xls).")

    saved_file_path = None
    try:
        logger.debug(f"Attempting to save uploaded file for job {job_id}")
        saved_file_path = save_upload_file(file=file, job_id=job_id)
        logger.info(f"File saved successfully for job {job_id}", extra={"saved_path": saved_file_path})
    except IOError as e:
        logger.error(f"Failed to save uploaded file for job {job_id}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not save file: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during file upload/saving for job {job_id}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error processing upload: {e}")

    job = None
    try:
        logger.debug(f"Creating database job record for job {job_id}")
        job = Job(
            id=job_id,
            company_name=company_name,
            input_file_name=os.path.basename(saved_file_path),
            status=JobStatus.PENDING.value,
            current_stage=ProcessingStage.INGESTION.value,
            created_by=current_user.username,
            target_level=target_level # Save the target level
        )
        db.add(job)
        db.commit()
        db.refresh(job)
        logger.info(f"Database job record created successfully for job {job_id}", extra={"target_level": job.target_level})
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create database job record for job {job_id}", exc_info=True)
        if saved_file_path and os.path.exists(saved_file_path):
            try: os.remove(saved_file_path)
            except OSError: logger.warning(f"Could not remove file {saved_file_path} after DB error.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create job record.")

    try:
        logger.info(f"Adding Celery task 'process_vendor_file' to background tasks for job {job_id}")
        # --- UPDATED: Pass target_level to Celery task ---
        background_tasks.add_task(process_vendor_file.delay, job_id=job_id, file_path=saved_file_path, target_level=target_level)
        # --- END UPDATED ---
        logger.info(f"Celery task queued successfully for job {job_id}")
    except Exception as e:
        logger.error(f"Failed to queue Celery task for job {job_id}", exc_info=True)
        job.fail(f"Failed to queue processing task: {str(e)}")
        db.commit()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to queue job for processing.")

    logger.info(f"Upload request for job {job_id} processed successfully, returning 202 Accepted.")
    # Use model_validate for Pydantic v2
    return JobResponse.model_validate(job)
# --- END UPLOAD ROUTE ---


# --- Mount Static Files (Vue App) ---
if os.path.exists(VUE_BUILD_DIR) and os.path.exists(VUE_INDEX_FILE):
    logger.info(f"Mounting Vue app from directory: {VUE_BUILD_DIR}")
    app.mount("/", StaticFiles(directory=VUE_BUILD_DIR, html=True), name="app")
else:
    logger.error(f"Cannot mount Vue app: Directory {VUE_BUILD_DIR} or index file {VUE_INDEX_FILE} not found.")
    @app.get("/")
    async def missing_frontend():
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": f"Frontend not found. Expected build files in {VUE_BUILD_DIR}"}
        )
# --- END VUE.JS FRONTEND SERVING SETUP ---
</file>

<file path='app/models/classification.py'>
# <file path='app/models/classification.py'>
# --- file path='app/models/classification.py' ---
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any

class VendorClassification(BaseModel):
    """Vendor classification model."""
    vendor_name: str
    category_id: str
    category_name: str
    confidence: float = Field(ge=0.0, le=1.0)
    notes: Optional[str] = None
    classification_not_possible: bool = False
    classification_not_possible_reason: Optional[str] = None
    sources: Optional[List[Dict[str, str]]] = None
    classification_source: Optional[str] = None # e.g., 'Initial', 'Search'

class ClassificationBatchResponse(BaseModel):
    """Response model for classification batch (expected from LLM)."""
    # --- MODIFIED: Allow level up to 5 ---
    level: int = Field(ge=1, le=5)
    # --- END MODIFIED ---
    batch_id: str
    parent_category_id: Optional[str] = None
    classifications: List[VendorClassification] # LLM should return this structure

class ApiUsage(BaseModel):
    """API usage statistics."""
    # Field names match the keys used in the stats dictionary
    openrouter_calls: int = 0
    openrouter_prompt_tokens: int = 0
    openrouter_completion_tokens: int = 0
    openrouter_total_tokens: int = 0
    tavily_search_calls: int = 0
    cost_estimate_usd: float = 0.0

class ProcessingStats(BaseModel):
    """Processing statistics for a job (stored in Job.stats JSON)."""
    job_id: str
    company_name: str
    start_time: Any # Can be datetime or ISO string
    end_time: Optional[Any] = None
    processing_duration_seconds: Optional[float] = None
    total_vendors: int = 0
    unique_vendors: int = 0
    # --- UPDATED/ADDED Fields ---
    successfully_classified_l4: int = 0 # Keep L4 count for reference/comparison
    successfully_classified_l5: int = 0 # NEW: Total vendors reaching L5 (initial or post-search)
    classification_not_possible_initial: int = 0 # Vendors needing search initially
    invalid_category_errors: int = 0 # Count of times LLM returned invalid category ID
    search_attempts: int = 0 # How many vendors triggered the search path
    search_successful_classifications_l1: int = 0 # Vendors getting L1 via search
    search_successful_classifications_l5: int = 0 # NEW: Vendors getting L5 via search path
    # --- END UPDATED/ADDED ---
    api_usage: ApiUsage = Field(default_factory=ApiUsage)

</file>

<file path='app/models/job.py'>

# <file path='app/models/job.py'>
# --- file path='app/models/job.py' ---
from sqlalchemy import Column, String, Float, DateTime, Enum as SQLEnum, JSON, Text, Integer # <<< ADDED Integer
from sqlalchemy.sql import func
from sqlalchemy.orm import Session # <<< ADDED IMPORT FOR TYPE HINTING
from enum import Enum as PyEnum
from datetime import datetime
from typing import Optional, Dict, Any

from core.database import Base

class JobStatus(str, PyEnum):
    """Job status enum."""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingStage(str, PyEnum):
    """Processing stage enum."""
    INGESTION = "ingestion"
    NORMALIZATION = "normalization"
    CLASSIFICATION_L1 = "classification_level_1"
    CLASSIFICATION_L2 = "classification_level_2"
    CLASSIFICATION_L3 = "classification_level_3"
    CLASSIFICATION_L4 = "classification_level_4"
    CLASSIFICATION_L5 = "classification_level_5"
    SEARCH = "search_unknown_vendors" # This stage now covers search AND recursive post-search classification
    RESULT_GENERATION = "result_generation"

class Job(Base):
    """Job model for tracking classification jobs."""

    __tablename__ = "jobs"

    id = Column(String, primary_key=True, index=True)
    company_name = Column(String, nullable=False)
    input_file_name = Column(String, nullable=False)
    output_file_name = Column(String, nullable=True)
    status = Column(String, default=JobStatus.PENDING.value)
    current_stage = Column(String, default=ProcessingStage.INGESTION.value)
    progress = Column(Float, default=0.0)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now(), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    notification_email = Column(String, nullable=True)
    error_message = Column(Text, nullable=True)
    stats = Column(JSON, default={}) # Structure defined by ProcessingStats model
    created_by = Column(String, nullable=False)
    # --- ADDED: Target Level ---
    target_level = Column(Integer, nullable=False, default=5) # Store the desired classification depth (1-5)
    # --- END ADDED ---

    def update_progress(self, progress: float, stage: ProcessingStage, db_session: Optional[Session] = None): # Type hint now valid
        """Update job progress and stage, optionally committing."""
        self.progress = progress
        self.current_stage = stage.value
        self.updated_at = datetime.now()
        # Optionally commit immediately if session provided
        if db_session:
            try:
                db_session.commit()
            except Exception as e:
                from core.logging_config import get_logger # Local import for safety
                logger = get_logger("vendor_classification.job_model")
                logger.error(f"Failed to commit progress update for job {self.id}", exc_info=True)
                db_session.rollback()


    def complete(self, output_file_name: str, stats: Dict[str, Any]):
        """Mark job as completed."""
        self.status = JobStatus.COMPLETED.value
        self.progress = 1.0
        self.current_stage = ProcessingStage.RESULT_GENERATION.value # Ensure stage reflects completion
        self.output_file_name = output_file_name
        self.completed_at = datetime.now()
        self.stats = stats
        self.updated_at = self.completed_at # Align updated_at with completed_at

    def fail(self, error_message: str):
        """Mark job as failed."""
        self.status = JobStatus.FAILED.value
        # Optionally set progress to 1.0 or leave as is upon failure
        # self.progress = 1.0
        self.error_message = error_message
        self.updated_at = datetime.now()
        # Ensure completed_at is Null if it failed
        self.completed_at = None
</file>

<file path='app/models/taxonomy.py'>
# <file path='app/models/taxonomy.py'>
# --- file path='app/models/taxonomy.py' ---
from pydantic import BaseModel, Field
from typing import Dict, List, Optional
import re # Added import

# --- ADDED: Import logger ---
from core.logging_config import get_logger
logger = get_logger("vendor_classification.taxonomy_model")
# --- END ADDED ---


class TaxonomyCategory(BaseModel):
    """Base taxonomy category model."""
    id: str
    name: str
    description: Optional[str] = None

# --- ADDED: Level 5 Model ---
class TaxonomyLevel5(TaxonomyCategory):
    """Level 5 taxonomy category (most specific - typically 6 digits)."""
    pass
# --- END ADDED ---

class TaxonomyLevel4(TaxonomyCategory):
    """Level 4 taxonomy category (typically 5 digits)."""
    # --- MODIFIED: Add children for Level 5 ---
    children: Dict[str, TaxonomyLevel5] = Field(default_factory=dict)
    # --- END MODIFIED ---

class TaxonomyLevel3(TaxonomyCategory):
    """Level 3 taxonomy category."""
    children: Dict[str, TaxonomyLevel4] = Field(default_factory=dict)

class TaxonomyLevel2(TaxonomyCategory):
    """Level 2 taxonomy category."""
    children: Dict[str, TaxonomyLevel3] = Field(default_factory=dict)

class TaxonomyLevel1(TaxonomyCategory):
    """Level 1 taxonomy category (most general)."""
    children: Dict[str, TaxonomyLevel2] = Field(default_factory=dict)

class Taxonomy(BaseModel):
    """Complete taxonomy model."""
    name: str
    version: str
    description: Optional[str] = None
    categories: Dict[str, TaxonomyLevel1] = Field(default_factory=dict)

    def get_level1_categories(self) -> List[TaxonomyCategory]:
        """Get all level 1 categories."""
        logger.debug(f"get_level1_categories: Retrieving {len(self.categories)} L1 categories.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in self.categories.items()
        ]

    def get_level2_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 2 categories for a given parent."""
        logger.debug(f"get_level2_categories: Attempting to get children for L1 parent '{parent_id}'.")
        if parent_id not in self.categories:
            logger.warning(f"get_level2_categories: Parent ID '{parent_id}' not found in L1 categories.")
            return []

        level1_cat = self.categories[parent_id]
        if not hasattr(level1_cat, 'children') or not level1_cat.children:
             logger.warning(f"get_level2_categories: Parent L1 category '{parent_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level1_cat.children)
        logger.debug(f"get_level2_categories: Found {children_count} L2 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level1_cat.children.items()
        ]

    def get_level3_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 3 categories for a given parent ID (expected format: L1.L2 or just L2 ID)."""
        logger.debug(f"get_level3_categories: Attempting to get children for L2 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 2:
                level1_id, level2_id = parts[0], parts[1]
            else:
                 logger.error(f"get_level3_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2' or 'L2'.")
                 return []
        else:
            # Assume it's just the L2 ID - need to find its L1 parent
            level2_id = parent_id
            for l1_key, l1_node in self.categories.items():
                if level2_id in getattr(l1_node, 'children', {}):
                    level1_id = l1_key
                    break
            if not level1_id:
                logger.warning(f"get_level3_categories: Could not find L1 parent for L2 ID '{level2_id}'.")
                return []

        logger.debug(f"get_level3_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}'.")

        if level1_id not in self.categories:
            logger.warning(f"get_level3_categories: L1 parent ID '{level1_id}' not found.")
            return []

        level1_cat = self.categories[level1_id]
        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
             logger.warning(f"get_level3_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
             return []

        level2_cat = level1_cat.children[level2_id]
        if not hasattr(level2_cat, 'children') or not level2_cat.children:
             logger.warning(f"get_level3_categories: Parent L2 category '{level2_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level2_cat.children)
        logger.debug(f"get_level3_categories: Found {children_count} L3 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level2_cat.children.items()
        ]

    def get_level4_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 4 categories for a given parent ID (expected format: L1.L2.L3 or just L3 ID)."""
        logger.debug(f"get_level4_categories: Attempting to get children for L3 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        level3_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 3:
                level1_id, level2_id, level3_id = parts[0], parts[1], parts[2]
            else:
                 logger.error(f"get_level4_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2.L3' or 'L3'.")
                 return []
        else:
            # Assume it's just the L3 ID - need to find its L1/L2 parents
            level3_id = parent_id
            found = False
            for l1_key, l1_node in self.categories.items():
                for l2_key, l2_node in getattr(l1_node, 'children', {}).items():
                    if level3_id in getattr(l2_node, 'children', {}):
                        level1_id = l1_key
                        level2_id = l2_key
                        found = True
                        break
                if found:
                    break
            if not found:
                 logger.warning(f"get_level4_categories: Could not find L1/L2 parents for L3 ID '{level3_id}'.")
                 return []

        logger.debug(f"get_level4_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}', L3='{level3_id}'.")

        if level1_id not in self.categories:
            logger.warning(f"get_level4_categories: L1 parent ID '{level1_id}' not found.")
            return []

        level1_cat = self.categories[level1_id]
        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
            logger.warning(f"get_level4_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
            return []

        level2_cat = level1_cat.children[level2_id]
        if not hasattr(level2_cat, 'children') or level3_id not in level2_cat.children:
            logger.warning(f"get_level4_categories: L3 parent ID '{level3_id}' not found under L2 '{level2_id}'.")
            return []

        level3_cat = level2_cat.children[level3_id]
        if not hasattr(level3_cat, 'children') or not level3_cat.children:
             logger.warning(f"get_level4_categories: Parent L3 category '{level3_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level3_cat.children)
        logger.debug(f"get_level4_categories: Found {children_count} L4 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level3_cat.children.items()
        ]

    # --- ADDED: get_level5_categories ---
    def get_level5_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 5 categories for a given parent ID (expected format: L1.L2.L3.L4 or just L4 ID)."""
        logger.debug(f"get_level5_categories: Attempting to get children for L4 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        level3_id = None
        level4_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 4:
                level1_id, level2_id, level3_id, level4_id = parts[0], parts[1], parts[2], parts[3]
            else:
                 logger.error(f"get_level5_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2.L3.L4' or 'L4'.")
                 return []
        else:
            # Assume it's just the L4 ID - need to find its parents
            level4_id = parent_id
            found = False
            for l1_key, l1_node in self.categories.items():
                for l2_key, l2_node in getattr(l1_node, 'children', {}).items():
                    for l3_key, l3_node in getattr(l2_node, 'children', {}).items():
                        if level4_id in getattr(l3_node, 'children', {}):
                            level1_id = l1_key
                            level2_id = l2_key
                            level3_id = l3_key
                            found = True
                            break
                    if found: break
                if found: break
            if not found:
                 logger.warning(f"get_level5_categories: Could not find L1/L2/L3 parents for L4 ID '{level4_id}'.")
                 return []

        logger.debug(f"get_level5_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}', L3='{level3_id}', L4='{level4_id}'.")

        # Traverse the hierarchy
        if level1_id not in self.categories:
            logger.warning(f"get_level5_categories: L1 parent ID '{level1_id}' not found.")
            return []
        level1_cat = self.categories[level1_id]

        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
            logger.warning(f"get_level5_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
            return []
        level2_cat = level1_cat.children[level2_id]

        if not hasattr(level2_cat, 'children') or level3_id not in level2_cat.children:
            logger.warning(f"get_level5_categories: L3 parent ID '{level3_id}' not found under L2 '{level2_id}'.")
            return []
        level3_cat = level2_cat.children[level3_id]

        if not hasattr(level3_cat, 'children') or level4_id not in level3_cat.children:
            logger.warning(f"get_level5_categories: L4 parent ID '{level4_id}' not found under L3 '{level3_id}'.")
            return []
        level4_cat = level3_cat.children[level4_id]

        # Get L5 children
        if not hasattr(level4_cat, 'children') or not level4_cat.children:
             logger.warning(f"get_level5_categories: Parent L4 category '{level4_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level4_cat.children)
        logger.debug(f"get_level5_categories: Found {children_count} L5 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level4_cat.children.items()
        ]
    # --- END ADDED ---
</file>

<file path='app/schemas/job.py'>

# app/schemas/job.py
from pydantic import BaseModel, Field # <<< ADDED Field
from datetime import datetime
from typing import Optional, Dict, Any

from models.job import JobStatus, ProcessingStage # Import enums from model

class JobResponse(BaseModel):
    """Schema for returning job information."""
    id: str
    company_name: str
    status: JobStatus # Use the enum
    progress: float
    current_stage: ProcessingStage # Use the enum
    created_at: datetime
    updated_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    output_file_name: Optional[str] = None
    input_file_name: str
    created_by: str
    error_message: Optional[str] = None
    # --- ADDED: Target Level ---
    target_level: int = Field(..., ge=1, le=5) # Include target level in response
    # --- END ADDED ---
    # stats: Optional[Dict[str, Any]] = None # Optionally include stats summary

    class Config:
        from_attributes = True # Pydantic v2 way to enable ORM mode
        # orm_mode = True # Pydantic v1 way
</file>

<file path='app/tasks/classification_logic.py'>
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError

from core.config import settings
from core.logging_config import get_logger
# Import context functions if needed directly (though often used via logger)
from core.log_context import set_log_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call, log_duration

from models.job import Job, JobStatus, ProcessingStage
from models.taxonomy import Taxonomy
from services.llm_service import LLMService
from services.search_service import SearchService

logger = get_logger("vendor_classification.classification_logic")

# --- Constants ---
MAX_CONCURRENT_SEARCHES = 10 # Limit concurrent search/LLM processing for unknown vendors

# --- Helper Functions (Moved from classification_tasks.py) ---

def create_batches(items: List[Any], batch_size: int) -> List[List[Any]]:
    """Create batches from a list of items."""
    if not items: return []
    if not isinstance(items, list):
        logger.warning(f"create_batches expected a list, got {type(items)}. Returning empty list.")
        return []
    if batch_size <= 0:
        logger.warning(f"Invalid batch_size {batch_size}, using default from settings.")
        batch_size = settings.BATCH_SIZE
    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]

def group_by_parent_category(
    results: Dict[str, Dict],
    parent_level: int,
    vendors_to_group_names: List[str]
) -> Dict[Optional[str], List[str]]:
    """
    Group a specific list of vendor names based on their classification result at the parent_level.
    Only includes vendors that were successfully classified with a valid ID at the parent level.
    Returns a dictionary mapping parent category ID to a list of vendor *names*.
    """
    grouped: Dict[Optional[str], List[str]] = {}
    parent_key = f"level{parent_level}"
    logger.debug(f"group_by_parent_category: Grouping {len(vendors_to_group_names)} vendors based on results from '{parent_key}'.")

    grouped_count = 0
    excluded_count = 0

    for vendor_name in vendors_to_group_names:
        vendor_results = results.get(vendor_name)
        level_result = None
        if vendor_results is not None:
            level_result = vendor_results.get(parent_key)
        else:
            logger.warning(f"group_by_parent_category: Vendor '{vendor_name}' not found in results dictionary.")
            excluded_count += 1
            continue

        if level_result and isinstance(level_result, dict) and not level_result.get("classification_not_possible", True):
            category_id = level_result.get("category_id")
            if category_id and category_id not in ["N/A", "ERROR"]:
                if category_id not in grouped:
                    grouped[category_id] = []
                grouped[category_id].append(vendor_name)
                grouped_count += 1
                # Reduced verbosity: logger.debug(f"  Grouping vendor '{vendor_name}' under parent '{category_id}'.")
            else:
                logger.debug(f"  Excluding vendor '{vendor_name}': classified at '{parent_key}' but has invalid category_id '{category_id}'.")
                excluded_count += 1
        else:
            reason = "Not processed"
            if level_result and isinstance(level_result, dict):
                reason = level_result.get('classification_not_possible_reason', 'Marked not possible')
            elif not level_result:
                    reason = f"No result found for {parent_key}"
            logger.info(f"  Excluding vendor '{vendor_name}' from Level {parent_level + 1}: not successfully classified at '{parent_key}'. Reason: {reason}.")
            excluded_count += 1

    logger.info(f"group_by_parent_category: Finished grouping for Level {parent_level + 1}. Created {len(grouped)} groups, included {grouped_count} vendors, excluded {excluded_count} vendors.")
    return grouped

# --- Core Processing Logic (Moved from classification_tasks.py) ---

@log_function_call(logger, include_args=False) # Keep args=False
async def process_batch(
    batch_data: List[Dict[str, Any]], # Pass list of dicts including optional fields
    level: int,
    parent_category_id: Optional[str],
    taxonomy: Taxonomy,
    llm_service: LLMService,
    stats: Dict[str, Any],
    search_context: Optional[Dict[str, Any]] = None # ADDED: Optional search context
) -> Dict[str, Dict]:
    """
    Process a batch of vendors for a specific classification level (1-5), including taxonomy validation.
    Optionally uses search context for post-search classification attempts.
    Updates stats dictionary in place. Passes full vendor data and context to LLM.
    Returns results for the batch.
    """
    results = {}
    if not batch_data:
        logger.warning(f"process_batch called with empty batch_data for Level {level}, Parent '{parent_category_id}'.")
        return results

    batch_names = [vd.get('vendor_name', f'Unknown_{i}') for i, vd in enumerate(batch_data)] # For logging
    context_type = "Search Context" if search_context else "Initial Data"

    logger.info(f"process_batch: Starting Level {level} batch using {context_type}.",
                extra={"batch_size": len(batch_data), "parent_category_id": parent_category_id, "first_vendor": batch_names[0] if batch_names else 'N/A'})

    # --- Get valid category IDs for this level/parent (Updated for L5) ---
    valid_category_ids: Set[str] = set()
    category_id_lookup_error = False
    try:
        logger.debug(f"process_batch: Retrieving valid category IDs for Level {level}, Parent '{parent_category_id}'.")
        categories = []
        if level == 1:
            categories = taxonomy.get_level1_categories()
        elif parent_category_id:
            if level == 2:
                categories = taxonomy.get_level2_categories(parent_category_id)
            elif level == 3:
                categories = taxonomy.get_level3_categories(parent_category_id)
            elif level == 4:
                categories = taxonomy.get_level4_categories(parent_category_id)
            elif level == 5:
                categories = taxonomy.get_level5_categories(parent_category_id)
            else:
                    logger.error(f"process_batch: Invalid level {level} requested.")
                    categories = [] # Should not happen
        else: # level > 1 and no parent_category_id
                logger.error(f"process_batch: Parent category ID is required for Level {level} but was not provided.")
                categories = []

        valid_category_ids = {cat.id for cat in categories}

        if not valid_category_ids:
                if level > 1 and parent_category_id:
                    logger.warning(f"process_batch: No valid child categories found or retrieved for Level {level}, Parent '{parent_category_id}'. LLM cannot classify.")
                elif level == 1:
                    logger.error("process_batch: No Level 1 categories found in taxonomy!")
                    category_id_lookup_error = True
        # else: # Reduced verbosity
                # logger.debug(f"process_batch: Found {len(valid_category_ids)} valid IDs for Level {level}, Parent '{parent_category_id}'. Example: {list(valid_category_ids)[:5]}")

    except Exception as tax_err:
        logger.error(f"process_batch: Error getting valid categories from taxonomy", exc_info=True,
                        extra={"level": level, "parent_category_id": parent_category_id})
        valid_category_ids = set()
        category_id_lookup_error = True

    # --- Call LLM ---
    llm_response_data = None
    try:
        logger.info(f"process_batch: Calling LLM for Level {level}, Parent '{parent_category_id or 'None'}', Context: {context_type}")
        with LogTimer(logger, f"LLM classification - Level {level}, Parent '{parent_category_id or 'None'}' ({context_type})", include_in_stats=True):
            llm_response_data = await llm_service.classify_batch(
                batch_data=batch_data,
                level=level,
                taxonomy=taxonomy,
                parent_category_id=parent_category_id,
                search_context=search_context
            )
        logger.info(f"process_batch: LLM call completed for Level {level}, Parent '{parent_category_id or 'None'}'.")

        if llm_response_data and isinstance(llm_response_data.get("usage"), dict):
            usage = llm_response_data["usage"]
            stats["api_usage"]["openrouter_calls"] += 1
            stats["api_usage"]["openrouter_prompt_tokens"] += usage.get("prompt_tokens", 0)
            stats["api_usage"]["openrouter_completion_tokens"] += usage.get("completion_tokens", 0)
            stats["api_usage"]["openrouter_total_tokens"] += usage.get("total_tokens", 0)
            # logger.debug(f"process_batch: LLM API usage updated", extra=usage) # Reduced verbosity
        else:
            logger.warning("process_batch: LLM response missing or has invalid usage data.")

        if llm_response_data is None:
                logger.error("process_batch: Received None response from llm_service.classify_batch. Cannot process results.")
                raise ValueError("LLM service returned None, indicating a failure in the call.")

        llm_result = llm_response_data.get("result", {})
        classifications = llm_result.get("classifications", [])
        if not isinstance(classifications, list):
                logger.warning("LLM response 'classifications' is not a list.", extra={"response_preview": str(llm_result)[:500]})
                classifications = []

        logger.debug(f"process_batch: Received {len(classifications)} classifications from LLM for batch size {len(batch_data)} at Level {level}.")
        if llm_result.get("batch_id_mismatch"):
                logger.warning(f"process_batch: Processing batch despite batch_id mismatch warning from LLM service.")

        # --- Validate and process results ---
        processed_vendors_in_response = set()
        for classification in classifications:
            if not isinstance(classification, dict):
                logger.warning("Invalid classification item format received from LLM (not a dict)", extra={"item": classification})
                continue

            vendor_name = classification.get("vendor_name")
            if not vendor_name:
                logger.warning("Classification received without vendor_name", extra={"classification": classification})
                continue

            target_vendor_name = vendor_name
            processed_vendors_in_response.add(target_vendor_name)

            category_id = classification.get("category_id", "N/A")
            category_name = classification.get("category_name", "N/A")
            confidence = classification.get("confidence", 0.0)
            classification_not_possible = classification.get("classification_not_possible", False)
            reason = classification.get("classification_not_possible_reason")
            notes = classification.get("notes")
            is_valid_category = True

            # --- TAXONOMY VALIDATION ---
            if not classification_not_possible and not category_id_lookup_error and valid_category_ids:
                if category_id not in valid_category_ids:
                    is_valid_category = False
                    logger.warning(f"Invalid category ID '{category_id}' returned by LLM for vendor '{target_vendor_name}' at level {level}, parent '{parent_category_id}'.",
                                    extra={"valid_ids_count": len(valid_category_ids)})
                    classification_not_possible = True
                    reason = f"Invalid category ID '{category_id}' returned by LLM (Valid examples: {list(valid_category_ids)[:3]})"
                    confidence = 0.0
                    category_id = "N/A"
                    category_name = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1
                # else: # Reduced verbosity
                        # logger.debug(f"Category ID '{category_id}' for '{target_vendor_name}' is valid for Level {level}, Parent '{parent_category_id}'.")
            elif not classification_not_possible and category_id_lookup_error:
                    logger.warning(f"Cannot validate category ID '{category_id}' for '{target_vendor_name}' due to earlier taxonomy lookup error.")
            elif not classification_not_possible and not valid_category_ids and level > 1:
                    logger.warning(f"Cannot validate category ID '{category_id}' for '{target_vendor_name}' because no valid child categories were found for parent '{parent_category_id}'.")
                    is_valid_category = False
                    classification_not_possible = True
                    reason = f"LLM returned category '{category_id}' but no valid children found for parent '{parent_category_id}'."
                    confidence = 0.0
                    category_id = "N/A"; category_name = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1
            # --- End TAXONOMY VALIDATION ---

            # --- Consistency Checks ---
            if classification_not_possible and confidence > 0.0:
                logger.warning("Correcting confidence to 0.0 for classification_not_possible=true", extra={"vendor": target_vendor_name})
                confidence = 0.0
            if not classification_not_possible and is_valid_category and (category_id == "N/A" or not category_id):
                    logger.warning("Classification marked possible by LLM but category ID is 'N/A' or empty", extra={"vendor": target_vendor_name, "classification": classification})
                    classification_not_possible = True
                    reason = reason or "Missing category ID despite LLM success claim"
                    confidence = 0.0
                    category_id = "N/A"
                    category_name = "N/A"
            # --- End Consistency Checks ---

            results[target_vendor_name] = {
                "category_id": category_id,
                "category_name": category_name,
                "confidence": confidence,
                "classification_not_possible": classification_not_possible,
                "classification_not_possible_reason": reason,
                "notes": notes,
                "vendor_name": target_vendor_name,
                "classification_source": "Search" if search_context else "Initial" # Add source info
            }
            # logger.debug(f"process_batch: Processed result for '{target_vendor_name}' at Level {level}. Possible: {not classification_not_possible}, ID: {category_id}") # Reduced verbosity

        # Handle missing vendors from batch
        missing_vendors = set(batch_names) - processed_vendors_in_response
        if missing_vendors:
            logger.warning(f"LLM response did not include results for all vendors in the batch.", extra={"missing_vendors": list(missing_vendors), "level": level})
            for vendor_name in missing_vendors:
                results[vendor_name] = {
                    "category_id": "N/A", "category_name": "N/A", "confidence": 0.0,
                    "classification_not_possible": True,
                    "classification_not_possible_reason": "Vendor missing from LLM response batch",
                    "notes": None,
                    "vendor_name": vendor_name,
                    "classification_source": "Search" if search_context else "Initial" # Add source info
                }

    except Exception as e:
        logger.error(f"Failed to process batch at Level {level} ({context_type})", exc_info=True,
                        extra={"batch_names": batch_names, "error": str(e)})
        for vendor_name in batch_names:
            results[vendor_name] = {
                "category_id": "ERROR", "category_name": "ERROR", "confidence": 0.0,
                "classification_not_possible": True,
                "classification_not_possible_reason": f"Batch processing error: {str(e)[:100]}",
                "notes": None,
                "vendor_name": vendor_name,
                "classification_source": "Search" if search_context else "Initial" # Add source info
            }
    logger.info(f"process_batch: Finished Level {level} batch for parent '{parent_category_id or 'None'}'. Returning {len(results)} results.")
    return results


@log_function_call(logger, include_args=False)
async def search_and_classify_recursively(
    vendor_data: Dict[str, Any],
    taxonomy: Taxonomy,
    llm_service: LLMService,
    search_service: SearchService,
    stats: Dict[str, Any],
    semaphore: asyncio.Semaphore,
    target_level: int # <<< ADDED target_level
) -> Dict[str, Any]:
    """
    Performs Tavily search, attempts L1 classification, and then recursively
    attempts L2 up to target_level classification using the search context.
    Controlled by semaphore.
    Returns the search_result_data dictionary, potentially augmented with
    classification results (keyed as classification_l1, classification_l2, etc.).
    """
    vendor_name = vendor_data.get('vendor_name', 'UnknownVendor')
    logger.debug(f"search_and_classify_recursively: Waiting to acquire semaphore for vendor '{vendor_name}'.")
    async with semaphore: # Limit concurrency
        logger.info(f"search_and_classify_recursively: Acquired semaphore. Starting for vendor '{vendor_name}' up to Level {target_level}.")
        search_result_data = {
            "vendor": vendor_name,
            "search_query": f"{vendor_name} company business type industry",
            "sources": [],
            "summary": None,
            "error": None,
            # Keys for storing classification results obtained via search
            "classification_l1": None,
            "classification_l2": None,
            "classification_l3": None,
            "classification_l4": None,
            "classification_l5": None
            }

        # --- 1. Perform Tavily Search ---
        try:
            logger.debug(f"search_and_classify_recursively: Calling search_service.search_vendor for '{vendor_name}'.")
            with LogTimer(logger, f"Tavily search for '{vendor_name}'", include_in_stats=True):
                tavily_response = await search_service.search_vendor(vendor_name)
            logger.debug(f"search_and_classify_recursively: search_service.search_vendor returned for '{vendor_name}'.")

            stats["api_usage"]["tavily_search_calls"] += 1
            search_result_data.update(tavily_response) # Update with actual search results or error

            source_count = len(search_result_data.get("sources", []))
            if search_result_data.get("error"):
                logger.warning(f"search_and_classify_recursively: Search failed", extra={"vendor": vendor_name, "error": search_result_data["error"]})
                search_result_data["classification_l1"] = {
                        "classification_not_possible": True,
                        "classification_not_possible_reason": f"Search error: {str(search_result_data['error'])[:100]}",
                        "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed", "classification_source": "Search"
                }
                logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to search error for '{vendor_name}'.")
                return search_result_data # Stop if search failed
            else:
                logger.info(f"search_and_classify_recursively: Search completed", extra={"vendor": vendor_name, "source_count": source_count, "summary_present": bool(search_result_data.get('summary'))})

        except Exception as search_exc:
            logger.error(f"search_and_classify_recursively: Unexpected error during Tavily search for {vendor_name}", exc_info=True)
            search_result_data["error"] = f"Unexpected search error: {str(search_exc)}"
            search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": f"Search task error: {str(search_exc)[:100]}",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed", "classification_source": "Search"
                }
            logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to search exception for '{vendor_name}'.")
            return search_result_data # Stop if search failed

        # --- 2. Attempt L1 Classification using Search Results ---
        search_content_available = search_result_data.get("sources") or search_result_data.get("summary")
        if not search_content_available:
            logger.warning(f"search_and_classify_recursively: No usable search results found for vendor, cannot classify", extra={"vendor": vendor_name})
            search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": "No search results content found",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "No Search Content", "classification_source": "Search"
            }
            logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to no search content for '{vendor_name}'.")
            return search_result_data # Stop if no content

        valid_l1_category_ids: Set[str] = set(taxonomy.categories.keys())
        llm_response_l1 = None
        try:
            logger.debug(f"search_and_classify_recursively: Calling llm_service.process_search_results (L1) for '{vendor_name}'.")
            with LogTimer(logger, f"LLM L1 classification from search for '{vendor_name}'", include_in_stats=True):
                # This specific function is designed only for L1 from search results
                llm_response_l1 = await llm_service.process_search_results(vendor_data, search_result_data, taxonomy)
            logger.debug(f"search_and_classify_recursively: llm_service.process_search_results (L1) returned for '{vendor_name}'.")

            if llm_response_l1 is None:
                    logger.error("search_and_classify_recursively: Received None response from llm_service.process_search_results. Cannot process L1.")
                    raise ValueError("LLM service (process_search_results) returned None.")

            if isinstance(llm_response_l1.get("usage"), dict):
                usage = llm_response_l1["usage"]
                stats["api_usage"]["openrouter_calls"] += 1
                stats["api_usage"]["openrouter_prompt_tokens"] += usage.get("prompt_tokens", 0)
                stats["api_usage"]["openrouter_completion_tokens"] += usage.get("completion_tokens", 0)
                stats["api_usage"]["openrouter_total_tokens"] += usage.get("total_tokens", 0)

            l1_classification = llm_response_l1.get("result", {})
            if "vendor_name" not in l1_classification: l1_classification["vendor_name"] = vendor_name
            l1_classification["classification_source"] = "Search" # Ensure source is marked

            # Validate L1 result
            classification_not_possible_l1 = l1_classification.get("classification_not_possible", True)
            category_id_l1 = l1_classification.get("category_id", "N/A")
            is_valid_l1 = True

            if not classification_not_possible_l1 and valid_l1_category_ids:
                if category_id_l1 not in valid_l1_category_ids:
                    is_valid_l1 = False
                    logger.warning(f"Invalid L1 category ID '{category_id_l1}' from search LLM for '{vendor_name}'.", extra={"valid_ids_count": len(valid_l1_category_ids)})
                    l1_classification["classification_not_possible"] = True
                    l1_classification["classification_not_possible_reason"] = f"Invalid L1 category ID '{category_id_l1}' from search."
                    l1_classification["confidence"] = 0.0
                    l1_classification["category_id"] = "N/A"
                    l1_classification["category_name"] = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1

            if l1_classification.get("classification_not_possible") and l1_classification.get("confidence", 0.0) > 0.0: l1_classification["confidence"] = 0.0
            if not l1_classification.get("classification_not_possible") and not l1_classification.get("category_id", "N/A"):
                    l1_classification["classification_not_possible"] = True
                    l1_classification["classification_not_possible_reason"] = "Missing L1 category ID despite LLM success claim"
                    l1_classification["confidence"] = 0.0
                    l1_classification["category_id"] = "N/A"; l1_classification["category_name"] = "N/A"

            search_result_data["classification_l1"] = l1_classification # Store validated L1 result

        except Exception as llm_err:
                logger.error(f"search_and_classify_recursively: Error during LLM L1 processing for {vendor_name}", exc_info=True)
                search_result_data["error"] = search_result_data.get("error") or f"LLM L1 processing error: {str(llm_err)}"
                search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": f"LLM L1 processing error: {str(llm_err)[:100]}",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "LLM L1 Error", "classification_source": "Search"
                }
                logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to L1 LLM exception for '{vendor_name}'.")
                return search_result_data # Stop if L1 classification failed

        # --- 3. Recursive Classification L2 up to target_level using Search Context ---
        current_parent_id = search_result_data["classification_l1"].get("category_id")
        classification_possible = not search_result_data["classification_l1"].get("classification_not_possible", True)

        # --- UPDATED: Loop up to target_level ---
        if classification_possible and current_parent_id and current_parent_id != "N/A" and target_level > 1:
            logger.info(f"search_and_classify_recursively: L1 successful ({current_parent_id}), proceeding to L2-{target_level} for {vendor_name} using search context.")
            for level in range(2, target_level + 1):
        # --- END UPDATED ---
                logger.debug(f"Attempting post-search Level {level} for {vendor_name}, parent {current_parent_id}")
                try:
                    logger.debug(f"search_and_classify_recursively: Calling process_batch (Level {level}) for '{vendor_name}' with search context.")
                    # Use process_batch for consistency in validation and structure
                    batch_result_dict = await process_batch(
                        batch_data=[vendor_data], # Batch of one
                        level=level,
                        parent_category_id=current_parent_id,
                        taxonomy=taxonomy,
                        llm_service=llm_service,
                        stats=stats,
                        search_context=search_result_data # Pass the full search results as context
                    )
                    logger.debug(f"search_and_classify_recursively: process_batch (Level {level}) returned for '{vendor_name}'.")

                    level_result = batch_result_dict.get(vendor_name)
                    if level_result:
                        # Ensure source is marked correctly if process_batch didn't already
                        level_result["classification_source"] = "Search"
                        search_result_data[f"classification_l{level}"] = level_result # Store result
                        if level_result.get("classification_not_possible", True):
                            logger.info(f"Post-search classification stopped at Level {level} for {vendor_name}. Reason: {level_result.get('classification_not_possible_reason')}")
                            break # Stop recursion if classification fails
                        else:
                            current_parent_id = level_result.get("category_id") # Update parent for next level
                            if not current_parent_id or current_parent_id == "N/A":
                                logger.warning(f"Post-search Level {level} successful but returned invalid parent_id '{current_parent_id}' for {vendor_name}. Stopping recursion.")
                                break
                    else:
                        logger.error(f"Post-search Level {level} batch processing did not return result for {vendor_name}. Stopping recursion.")
                        search_result_data[f"classification_l{level}"] = {
                                "classification_not_possible": True,
                                "classification_not_possible_reason": f"Missing result from L{level} post-search batch",
                                "confidence": 0.0, "vendor_name": vendor_name, "notes": f"L{level} Error", "classification_source": "Search"
                        }
                        break

                except Exception as recursive_err:
                    logger.error(f"search_and_classify_recursively: Error during post-search Level {level} for {vendor_name}", exc_info=True)
                    search_result_data[f"classification_l{level}"] = {
                            "classification_not_possible": True,
                            "classification_not_possible_reason": f"L{level} processing error: {str(recursive_err)[:100]}",
                            "confidence": 0.0, "vendor_name": vendor_name, "notes": f"L{level} Error", "classification_source": "Search"
                    }
                    break # Stop recursion on error
        else:
            logger.info(f"search_and_classify_recursively: L1 classification failed or not possible for {vendor_name}, or target level is 1. Skipping L2-{target_level}.")

        logger.info(f"search_and_classify_recursively: Finished for vendor", extra={"vendor": vendor_name})
        logger.debug(f"search_and_classify_recursively: Releasing semaphore for vendor '{vendor_name}'.")
        return search_result_data


@log_function_call(logger, include_args=False) # Keep args=False
async def process_vendors(
    unique_vendors_map: Dict[str, Dict[str, Any]], # Pass map containing full vendor data
    taxonomy: Taxonomy,
    results: Dict[str, Dict],
    stats: Dict[str, Any],
    job: Job,
    db: Session,
    llm_service: LLMService,
    search_service: SearchService,
    target_level: int # <<< ADDED target_level
):
    """
    Main orchestration function for processing vendors through the classification workflow (up to target_level),
    including recursive search for unknowns (up to target_level). Updates results and stats dictionaries in place.
    """
    unique_vendor_names = list(unique_vendors_map.keys()) # Get names from map
    total_unique_vendors = len(unique_vendor_names)
    processed_count = 0 # Count unique vendors processed in batches

    logger.info(f"Starting classification loop for {total_unique_vendors} unique vendors up to target Level {target_level}.")

    # --- Initial Hierarchical Classification (Levels 1 to target_level) ---
    vendors_to_process_next_level_names = set(unique_vendor_names) # Start with all unique vendor names for Level 1
    initial_l4_success_count = 0 # Track L4 for stats
    initial_l5_success_count = 0 # Track L5 for stats

    # --- UPDATED: Loop up to target_level ---
    for level in range(1, target_level + 1):
    # --- END UPDATED ---
        if not vendors_to_process_next_level_names:
            logger.info(f"No vendors remaining to process for Level {level}. Skipping.")
            continue # Skip level if no vendors need processing

        current_vendors_for_this_level = list(vendors_to_process_next_level_names) # Copy names for processing this level
        vendors_successfully_classified_in_level_names = set() # Track vendors that pass this level

        stage_enum_name = f"CLASSIFICATION_L{level}"
        if hasattr(ProcessingStage, stage_enum_name):
                job.current_stage = getattr(ProcessingStage, stage_enum_name).value
        else:
                logger.error(f"ProcessingStage enum does not have member '{stage_enum_name}'. Using default.")
                job.current_stage = ProcessingStage.PROCESSING.value # Fallback

        # Adjust progress calculation based on target_level (distribute 0.7 across target_level steps)
        progress_per_level = 0.7 / target_level if target_level > 0 else 0.7
        job.progress = min(0.8, 0.1 + ((level - 1) * progress_per_level))
        logger.info(f"[process_vendors] Committing status update before Level {level}: {job.status}, {job.current_stage}, {job.progress:.3f}")
        try:
            db.commit()
        except Exception as db_err:
            logger.error("Failed to commit status update before level processing", exc_info=True)
            db.rollback() # Rollback if commit fails

        logger.info(f"===== Starting Initial Level {level} Classification =====",
                    extra={ "vendors_to_process": len(current_vendors_for_this_level), "progress": job.progress })

        if level == 1:
            grouped_vendors_names = { None: current_vendors_for_this_level }
            logger.info(f"Level 1: Processing all {len(current_vendors_for_this_level)} vendors.")
        else:
            logger.info(f"Level {level}: Grouping {len(current_vendors_for_this_level)} vendors based on Level {level-1} results.")
            grouped_vendors_names = group_by_parent_category(results, level - 1, current_vendors_for_this_level)
            logger.info(f"Level {level}: Created {len(grouped_vendors_names)} groups for processing.")
            # Reduced verbosity:
            # for parent_id, names in grouped_vendors_names.items():
            #         logger.debug(f"  Group Parent ID '{parent_id}': {len(names)} vendors")

        processed_in_level_count = 0
        batch_counter_for_level = 0
        total_batches_for_level = sum( (len(names) + settings.BATCH_SIZE - 1) // settings.BATCH_SIZE for names in grouped_vendors_names.values() )
        logger.info(f"Level {level}: Total batches to process: {total_batches_for_level}")

        for parent_category_id, group_vendor_names in grouped_vendors_names.items():
            if not group_vendor_names:
                logger.debug(f"Skipping empty group for parent '{parent_category_id}' at Level {level}.")
                continue

            logger.info(f"Processing Level {level} group",
                        extra={"parent_category_id": parent_category_id, "vendor_count": len(group_vendor_names)})

            group_vendor_data = [unique_vendors_map[name] for name in group_vendor_names if name in unique_vendors_map]
            level_batches_data = create_batches(group_vendor_data, batch_size=settings.BATCH_SIZE)
            logger.debug(f"Created {len(level_batches_data)} batches for group '{parent_category_id}' at Level {level}.")

            for i, batch_data in enumerate(level_batches_data):
                batch_counter_for_level += 1
                batch_names = [vd['vendor_name'] for vd in batch_data] # Get names for logging
                logger.info(f"Processing Level {level} batch {i+1}/{len(level_batches_data)} for parent '{parent_category_id or 'None'}'",
                            extra={"batch_size": len(batch_data), "first_vendor": batch_names[0] if batch_names else 'N/A', "batch_num": batch_counter_for_level, "total_batches": total_batches_for_level})
                try:
                    # Process batch WITHOUT search context initially
                    batch_results = await process_batch(batch_data, level, parent_category_id, taxonomy, llm_service, stats, search_context=None)
                    logger.debug(f"Level {level} batch {i+1} results received. Count: {len(batch_results)}.")

                    for vendor_name, classification in batch_results.items():
                        if vendor_name in results:
                            # Ensure classification_source is set (should be 'Initial' here)
                            classification["classification_source"] = "Initial"
                            results[vendor_name][f"level{level}"] = classification
                            processed_in_level_count += 1

                            if not classification.get("classification_not_possible", True):
                                vendors_successfully_classified_in_level_names.add(vendor_name)
                                # logger.debug(f"Vendor '{vendor_name}' successfully classified at Level {level} (ID: {classification.get('category_id')}). Added for L{level+1}.") # Reduced verbosity
                                # Update stats based on the actual level completed
                                if level == 4:
                                    initial_l4_success_count += 1
                                    # logger.debug(f"Incremented initial_l4_success_count for {vendor_name}. Current L4 count: {initial_l4_success_count}") # Reduced verbosity
                                if level == 5:
                                    initial_l5_success_count += 1
                                    # logger.debug(f"Incremented initial_l5_success_count for {vendor_name}. Current L5 count: {initial_l5_success_count}") # Reduced verbosity
                            # else: # Reduced verbosity
                                # logger.debug(f"Vendor '{vendor_name}' not successfully classified at Level {level}. Reason: {classification.get('classification_not_possible_reason', 'Unknown')}. Will not proceed.")
                        else:
                                logger.warning(f"Vendor '{vendor_name}' from batch result not found in main results dictionary.", extra={"level": level})

                except Exception as batch_error:
                    logger.error(f"Error during initial batch processing logic (Level {level}, parent '{parent_category_id or 'None'}')", exc_info=True,
                                    extra={"batch_vendors": batch_names, "error": str(batch_error)})
                    for vendor_name in batch_names:
                            if vendor_name in results:
                                if f"level{level}" not in results[vendor_name]:
                                    results[vendor_name][f"level{level}"] = {
                                        "category_id": "ERROR", "category_name": "ERROR", "confidence": 0.0,
                                        "classification_not_possible": True,
                                        "classification_not_possible_reason": f"Batch processing logic error: {str(batch_error)[:100]}",
                                        "vendor_name": vendor_name,
                                        "classification_source": "Initial"
                                    }
                                    processed_in_level_count += 1
                            else:
                                logger.warning(f"Vendor '{vendor_name}' from failed batch not found in main results dictionary.", extra={"level": level})


                # Update progress within the level (based on batches completed)
                level_progress_fraction = batch_counter_for_level / total_batches_for_level if total_batches_for_level > 0 else 1
                job.progress = min(0.8, 0.1 + ((level - 1) * progress_per_level) + (progress_per_level * level_progress_fraction))
                try:
                    # logger.info(f"[process_vendors] Committing progress update after batch {batch_counter_for_level}/{total_batches_for_level} (Level {level}): {job.progress:.3f}") # Reduced verbosity
                    db.commit()
                except Exception as db_err:
                        logger.error("Failed to commit progress update during batch processing", exc_info=True)
                        db.rollback()

        logger.info(f"===== Initial Level {level} Classification Completed =====")
        logger.info(f"  Processed {processed_in_level_count} vendor results at Level {level}.")
        logger.info(f"  {len(vendors_successfully_classified_in_level_names)} vendors successfully classified and validated at Level {level}, proceeding to L{level+1}.")
        # logger.debug(f"Vendors proceeding to Level {level+1}: {list(vendors_successfully_classified_in_level_names)[:10]}...") # Reduced verbosity
        vendors_to_process_next_level_names = vendors_successfully_classified_in_level_names

    # --- End of Initial Level Loop ---
    logger.info(f"===== Finished Initial Hierarchical Classification Loop (Up to Level {target_level}) =====")

    # --- Identify vendors needing search (those not successfully classified at target_level) ---
    unknown_vendors_data_to_search = []
    for vendor_name in unique_vendor_names:
        is_classified_target = False
        if vendor_name in results:
            target_level_result = results[vendor_name].get(f"level{target_level}")
            if target_level_result and isinstance(target_level_result, dict) and not target_level_result.get("classification_not_possible", True):
                    # Also check for valid ID just in case
                    if target_level_result.get("category_id") and target_level_result.get("category_id") not in ["N/A", "ERROR"]:
                        is_classified_target = True

        if not is_classified_target:
            logger.debug(f"Vendor '{vendor_name}' did not initially reach/pass target Level {target_level} classification. Adding to search list.")
            if vendor_name in unique_vendors_map:
                unknown_vendors_data_to_search.append(unique_vendors_map[vendor_name])
            else:
                logger.warning(f"Vendor '{vendor_name}' marked for search but not found in unique_vendors_map.")
                unknown_vendors_data_to_search.append({'vendor_name': vendor_name}) # Add basic entry

    stats["classification_not_possible_initial"] = len(unknown_vendors_data_to_search)
    stats["successfully_classified_l4"] = initial_l4_success_count # Store initial L4 count
    stats["successfully_classified_l5"] = initial_l5_success_count # Store initial L5 count

    logger.info(f"Initial Classification Summary (Target L{target_level}): {total_unique_vendors - stats['classification_not_possible_initial']} reached target, {stats['classification_not_possible_initial']} did not.")
    if target_level >= 4: logger.info(f"  Ref: {initial_l4_success_count} reached L4 initially.")
    if target_level >= 5: logger.info(f"  Ref: {initial_l5_success_count} reached L5 initially.")

    # --- Search and Recursive Classification for Unknown Vendors (up to target_level) ---
    if unknown_vendors_data_to_search:
        job.current_stage = ProcessingStage.SEARCH.value
        job.progress = 0.8 # Progress after initial classification attempts
        logger.info(f"[process_vendors] Committing status update before Search stage: {job.status}, {job.current_stage}, {job.progress}")
        try:
            db.commit()
        except Exception as db_err:
            logger.error("Failed to commit status update before Search stage", exc_info=True)
            db.rollback()

        logger.info(f"===== Starting Search and Recursive Classification for {stats['classification_not_possible_initial']} Unclassified Vendors (Up to Level {target_level}) =====")

        stats["search_attempts"] = len(unknown_vendors_data_to_search)

        search_tasks = []
        if MAX_CONCURRENT_SEARCHES <= 0:
            logger.error(f"MAX_CONCURRENT_SEARCHES is {MAX_CONCURRENT_SEARCHES}. Cannot proceed with search tasks.")
            raise ValueError("MAX_CONCURRENT_SEARCHES must be positive.")
        search_semaphore = asyncio.Semaphore(MAX_CONCURRENT_SEARCHES)
        logger.info(f"Created search semaphore with concurrency limit: {MAX_CONCURRENT_SEARCHES}")

        for vendor_data in unknown_vendors_data_to_search:
            task = asyncio.create_task(
                search_and_classify_recursively(
                    vendor_data, taxonomy, llm_service, search_service, stats, search_semaphore, target_level # Pass target_level
                )
            )
            search_tasks.append(task)

        logger.info(f"Gathering results for {len(search_tasks)} search & recursive classification tasks...")
        logger.debug(f"Starting asyncio.gather for {len(search_tasks)} tasks.")
        gather_start_time = time.monotonic()
        search_and_recursive_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        gather_duration = time.monotonic() - gather_start_time
        logger.info(f"Search & recursive classification tasks completed (asyncio.gather finished). Duration: {gather_duration:.3f}s")

        job.progress = 0.95 # Indicate search phase is done, before result processing/generation
        logger.info(f"[process_vendors] Committing progress update after search gather: {job.progress:.3f}")
        try:
            db.commit()
        except Exception as db_err:
            logger.error("Failed to commit progress update after search gather", exc_info=True)
            db.rollback()

        successful_l1_searches = 0
        successful_l5_searches = 0 # Track L5 success via search
        processed_search_count = 0

        logger.info(f"Processing {len(search_and_recursive_results)} results from search/recursive tasks.")
        for i, result_or_exc in enumerate(search_and_recursive_results):
            processed_search_count += 1
            if i >= len(unknown_vendors_data_to_search):
                    logger.error(f"Search result index {i} out of bounds for unknown_vendors list.")
                    continue
            vendor_data = unknown_vendors_data_to_search[i]
            vendor_name = vendor_data.get('vendor_name', f'UnknownVendor_{i}')

            if vendor_name not in results:
                    logger.warning(f"Vendor '{vendor_name}' from search task not found in main results dict. Initializing.")
                    results[vendor_name] = {}

            results[vendor_name]["search_attempted"] = True # Add flag

            if isinstance(result_or_exc, Exception):
                logger.error(f"Error during search_and_classify_recursively for vendor {vendor_name}", exc_info=result_or_exc)
                results[vendor_name]["search_results"] = {"error": f"Search/Recursive task error: {str(result_or_exc)}"}
                # Mark L1 as failed if not already marked or if it was successful initially
                logger.info(f"OVERWRITE_LOG: Search task failed for '{vendor_name}'. Marking L1 as failed due to search error.")
                results[vendor_name]["level1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": f"Search task error: {str(result_or_exc)[:100]}",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed", "classification_source": "Search"
                }
                # Clear higher levels as search failed
                for lvl in range(2, target_level + 1):
                    if f"level{lvl}" in results[vendor_name]:
                        logger.info(f"OVERWRITE_LOG: Clearing L{lvl} for '{vendor_name}' due to search task error.")
                        results[vendor_name].pop(f"level{lvl}", None)

            elif isinstance(result_or_exc, dict):
                search_data = result_or_exc
                results[vendor_name]["search_results"] = search_data # Store raw search info

                l1_classification = search_data.get("classification_l1")

                if l1_classification: # Check if L1 result exists from search path
                    # --- START OVERWRITE LOGIC ---
                    logger.info(f"OVERWRITE_LOG: Processing search results for '{vendor_name}'. Target Level: {target_level}.")
                    results[vendor_name]["classified_via_search"] = True # Add flag

                    # Overwrite Level 1 unconditionally with the search result
                    logger.info(f"OVERWRITE_LOG: Overwriting L1 for '{vendor_name}' with search result. Possible: {not l1_classification.get('classification_not_possible', True)}")
                    results[vendor_name]["level1"] = l1_classification # This is the L1 overwrite

                    if not l1_classification.get("classification_not_possible", True):
                        successful_l1_searches += 1
                        logger.debug(f"Vendor '{vendor_name}' successfully classified at L1 via search (ID: {l1_classification.get('category_id')}).")

                        # Overwrite or clear higher levels based on recursive search results
                        for lvl in range(2, target_level + 1):
                            search_lvl_key = f"classification_l{lvl}"
                            main_lvl_key = f"level{lvl}"

                            if search_lvl_key in search_data and search_data[search_lvl_key]:
                                # Overwrite with the result from the recursive search
                                lvl_result_data = search_data[search_lvl_key]
                                logger.info(f"OVERWRITE_LOG: Overwriting L{lvl} for '{vendor_name}' with search result. Possible: {not lvl_result_data.get('classification_not_possible', True)}")
                                results[vendor_name][main_lvl_key] = lvl_result_data

                                # Track L5 success specifically if reached via search
                                if lvl == 5 and not lvl_result_data.get("classification_not_possible", True):
                                    successful_l5_searches += 1
                                    logger.info(f"Vendor '{vendor_name}' reached L5 classification via search.")
                            else:
                                # If search path didn't yield a result for this level (stopped early/failed), remove any initial result
                                if main_lvl_key in results[vendor_name]:
                                    logger.info(f"OVERWRITE_LOG: Clearing L{lvl} for '{vendor_name}' as search path did not provide a result for this level.")
                                    results[vendor_name].pop(main_lvl_key, None)
                                else:
                                    logger.debug(f"OVERWRITE_LOG: No initial L{lvl} result to clear for '{vendor_name}' and no search result provided.")

                    else: # L1 classification via search failed or wasn't possible
                        reason = l1_classification.get("classification_not_possible_reason", "Search did not yield L1 classification")
                        logger.info(f"Vendor '{vendor_name}' could not be classified via search at L1. Reason: {reason}. Clearing higher levels.")
                        # Clear higher levels as L1 failed post-search
                        for lvl in range(2, target_level + 1):
                            if f"level{lvl}" in results[vendor_name]:
                                logger.info(f"OVERWRITE_LOG: Clearing L{lvl} for '{vendor_name}' due to L1 failure post-search.")
                                results[vendor_name].pop(f"level{lvl}", None)
                    # --- END OVERWRITE LOGIC ---
                else:
                     # This case should ideally not happen if search_and_classify returns correctly, but handle defensively
                     logger.error(f"Search task for '{vendor_name}' returned dict but missing 'classification_l1'. Marking L1 as failed.")
                     results[vendor_name]["level1"] = { "classification_not_possible": True, "classification_not_possible_reason": "Internal search error (missing L1 result)", "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Error", "classification_source": "Search" }
                     for lvl in range(2, target_level + 1): results[vendor_name].pop(f"level{lvl}", None)


            else: # Handle unexpected return type from gather
                logger.error(f"Unexpected result type for vendor {vendor_name} search task: {type(result_or_exc)}")
                results[vendor_name]["search_results"] = {"error": f"Unexpected search result type: {type(result_or_exc)}"}
                # Mark L1 as failed
                logger.info(f"OVERWRITE_LOG: Unexpected search result type for '{vendor_name}'. Marking L1 as failed.")
                results[vendor_name]["level1"] = { "classification_not_possible": True, "classification_not_possible_reason": "Internal search error (unexpected type)", "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Error", "classification_source": "Search" }
                # Clear higher levels
                for lvl in range(2, target_level + 1):
                    if f"level{lvl}" in results[vendor_name]:
                        logger.info(f"OVERWRITE_LOG: Clearing L{lvl} for '{vendor_name}' due to unexpected search result type.")
                        results[vendor_name].pop(f"level{lvl}", None)

        stats["search_successful_classifications_l1"] = successful_l1_searches
        stats["search_successful_classifications_l5"] = successful_l5_searches # Updated stat
        # Update total L5 success count (if target was >= 5)
        # Recalculate final L5 count based on the *final* state of results dict
        final_l5_success_count = 0
        if target_level >= 5:
            for vendor_name in unique_vendor_names:
                l5_res = results.get(vendor_name, {}).get("level5")
                if l5_res and isinstance(l5_res, dict) and not l5_res.get("classification_not_possible", True):
                    final_l5_success_count += 1
            stats["successfully_classified_l5"] = final_l5_success_count
            logger.info(f"Final recalculation: Total vendors successfully classified at L5: {stats['successfully_classified_l5']}")
        else:
            stats["successfully_classified_l5"] = 0 # Ensure it's 0 if target level was lower

        logger.info(f"===== Unknown Vendor Search & Recursive Classification Completed =====")
        logger.info(f"  Attempted search for {stats['search_attempts']} vendors.")
        logger.info(f"  Successfully classified {successful_l1_searches} at L1 via search.")
        if target_level >= 5:
            logger.info(f"  Successfully classified {successful_l5_searches} at L5 via search path.") # L5 count specifically from search path
            logger.info(f"  Total vendors successfully classified at L5 (final state): {stats['successfully_classified_l5']}")
    else:
        logger.info("No unknown vendors required search.")
        job.progress = 0.95 # Set progress high if search wasn't needed
        logger.info(f"[process_vendors] Committing status update as search was skipped: {job.progress:.3f}")
        try:
            db.commit()
        except Exception as db_err:
            logger.error("Failed to commit status update when skipping search", exc_info=True)
            db.rollback()
    logger.info("process_vendors function is returning.")
</file>

<file path='app/tasks/classification_prompts.py'>

# app/prompts/classification_prompts.py
import json
import logging
from typing import List, Dict, Any, Optional

from models.taxonomy import Taxonomy, TaxonomyCategory

# Configure logger for this module
logger = logging.getLogger("vendor_classification.prompts")

def generate_batch_prompt(
    vendors_data: List[Dict[str, Any]],
    level: int,
    taxonomy: Taxonomy,
    parent_category_id: Optional[str] = None,
    batch_id: str = "unknown-batch",
    search_context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Create an appropriate prompt for the current classification level (1-5),
    optionally including search context for post-search classification.
    """
    context_type = "Search Context" if search_context else "Initial Data"
    logger.debug(f"generate_batch_prompt: Generating prompt for Level {level} using {context_type}",
                extra={ "vendor_count": len(vendors_data), "parent_category_id": parent_category_id, "batch_id": batch_id, "has_search_context": bool(search_context) })

    # --- Build Vendor Data Section ---
    vendor_data_xml = "<vendor_data>\n"
    for i, vendor_entry in enumerate(vendors_data):
        vendor_name = vendor_entry.get('vendor_name', f'UnknownVendor_{i}')
        example = vendor_entry.get('example')
        address = vendor_entry.get('vendor_address')
        website = vendor_entry.get('vendor_website')
        internal_cat = vendor_entry.get('internal_category')
        parent_co = vendor_entry.get('parent_company')
        spend_cat = vendor_entry.get('spend_category')

        vendor_data_xml += f"  <vendor index=\"{i+1}\">\n"
        vendor_data_xml += f"    <name>{vendor_name}</name>\n"
        if example: vendor_data_xml += f"    <example_goods_services>{str(example)[:200]}</example_goods_services>\n"
        if address: vendor_data_xml += f"    <address>{str(address)[:200]}</address>\n"
        if website: vendor_data_xml += f"    <website>{str(website)[:100]}</website>\n"
        if internal_cat: vendor_data_xml += f"    <internal_category>{str(internal_cat)[:100]}</internal_category>\n"
        if parent_co: vendor_data_xml += f"    <parent_company>{str(parent_co)[:100]}</parent_company>\n"
        if spend_cat: vendor_data_xml += f"    <spend_category>{str(spend_cat)[:100]}</spend_category>\n"
        vendor_data_xml += f"  </vendor>\n"
    vendor_data_xml += "</vendor_data>"

    # --- Build Search Context Section ---
    search_context_xml = ""
    if search_context and level > 1:
        logger.debug(f"Including search context in prompt for Level {level}", extra={"batch_id": batch_id})
        search_context_xml += "<search_context>\n"
        summary = search_context.get("summary")
        sources = search_context.get("sources")
        if summary:
            search_context_xml += f"  <summary>{str(summary)[:1000]}</summary>\n" # Limit length
        if sources and isinstance(sources, list):
            search_context_xml += "  <sources>\n"
            for j, source in enumerate(sources[:3]): # Limit to top 3 sources for brevity
                title = source.get('title', 'N/A')
                url = source.get('url', 'N/A')
                content_preview = str(source.get('content', ''))[:500] # Limit length
                search_context_xml += f"    <source index=\"{j+1}\">\n"
                search_context_xml += f"      <title>{title}</title>\n"
                search_context_xml += f"      <url>{url}</url>\n"
                search_context_xml += f"      <content_snippet>{content_preview}...</content_snippet>\n"
                search_context_xml += f"    </source>\n"
            search_context_xml += "  </sources>\n"
        else:
             search_context_xml += "  <message>No relevant search results sources were provided.</message>\n"
        search_context_xml += "</search_context>\n"

    # --- Get Category Options ---
    categories: List[TaxonomyCategory] = []
    parent_category_name = "N/A"
    category_lookup_successful = True
    try:
        logger.debug(f"generate_batch_prompt: Retrieving categories via taxonomy methods for Level {level}, Parent: {parent_category_id}")
        parent_obj = None
        if level == 1:
            categories = taxonomy.get_level1_categories()
        elif parent_category_id:
            if level == 2:
                categories = taxonomy.get_level2_categories(parent_category_id)
                parent_obj = taxonomy.categories.get(parent_category_id)
            elif level == 3:
                categories = taxonomy.get_level3_categories(parent_category_id)
                l1_id, l2_id = parent_category_id.split('.') if '.' in parent_category_id else (None, parent_category_id)
                if not l1_id:
                    for l1_key, l1_node in taxonomy.categories.items():
                        if l2_id in getattr(l1_node, 'children', {}): l1_id = l1_key; break
                if l1_id: parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id)
            elif level == 4:
                categories = taxonomy.get_level4_categories(parent_category_id)
                l1_id, l2_id, l3_id = parent_category_id.split('.') if parent_category_id.count('.') == 2 else (None, None, parent_category_id)
                if not l1_id:
                     found = False
                     for l1k, l1n in taxonomy.categories.items():
                         for l2k, l2n in getattr(l1n, 'children', {}).items():
                             if l3_id in getattr(l2n, 'children', {}): l1_id = l1k; l2_id = l2k; found = True; break
                         if found: break
                if l1_id and l2_id: parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id, {}).children.get(l3_id)
            elif level == 5:
                categories = taxonomy.get_level5_categories(parent_category_id)
                l1_id, l2_id, l3_id, l4_id = parent_category_id.split('.') if parent_category_id.count('.') == 3 else (None, None, None, parent_category_id)
                if not l1_id:
                    found = False
                    for l1k, l1n in taxonomy.categories.items():
                        for l2k, l2n in getattr(l1n, 'children', {}).items():
                            for l3k, l3n in getattr(l2n, 'children', {}).items():
                                if l4_id in getattr(l3n, 'children', {}): l1_id = l1k; l2_id = l2k; l3_id = l3k; found = True; break
                            if found: break
                        if found: break
                if l1_id and l2_id and l3_id:
                    parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id, {}).children.get(l3_id, {}).children.get(l4_id)

            if parent_obj: parent_category_name = parent_obj.name
        else: # level > 1 and no parent_category_id
            logger.error(f"Parent category ID is required for level {level} prompt generation but was not provided.")
            category_lookup_successful = False

        if not categories and level > 1 and parent_category_id:
             logger.warning(f"No subcategories found for Level {level}, Parent '{parent_category_id}'.")
             if level == 1: category_lookup_successful = False
        elif not categories and level == 1:
             logger.error(f"No Level 1 categories found in taxonomy!")
             category_lookup_successful = False

        logger.debug(f"generate_batch_prompt: Retrieved {len(categories)} categories for Level {level}, Parent '{parent_category_id}' ('{parent_category_name}').")

    except Exception as e:
        logger.error(f"Error retrieving categories for prompt (Level {level}, Parent: {parent_category_id})", exc_info=True)
        category_lookup_successful = False

    # --- Build Category Options Section ---
    category_options_xml = "<category_options>\n"
    if category_lookup_successful:
        category_options_xml += f"  <level>{level}</level>\n"
        if level > 1 and parent_category_id:
            category_options_xml += f"  <parent_id>{parent_category_id}</parent_id>\n"
            category_options_xml += f"  <parent_name>{parent_category_name}</parent_name>\n"
        category_options_xml += "  <categories>\n"
        if categories: # Check if categories list is not empty
            for cat in categories:
                category_options_xml += f"    <category id=\"{cat.id}\" name=\"{cat.name}\"/>\n"
        else:
             category_options_xml += f"    <message>No subcategories available for this level and parent.</message>\n"
        category_options_xml += "  </categories>\n"
    else:
        category_options_xml += f"  <error>Could not retrieve valid categories for Level {level}, Parent '{parent_category_id}'. Classification is not possible.</error>\n"
    category_options_xml += "</category_options>"

    # --- Define Output Format Section ---
    output_format_xml = f"""<output_format>
Respond *only* with a valid JSON object matching this exact schema. Do not include any text before or after the JSON object.

json
{{
  "level": {level},
  "batch_id": "{batch_id}",
  "parent_category_id": {json.dumps(parent_category_id)},
  "classifications": [
    {{
      "vendor_name": "string", // Exact vendor name from input <vendor_data>
      "category_id": "string", // ID from <category_options> or "N/A" if not possible
      "category_name": "string", // Name corresponding to category_id or "N/A"
      "confidence": "float", // 0.0 to 1.0. MUST be 0.0 if classification_not_possible is true.
      "classification_not_possible": "boolean", // true if classification cannot be confidently made from options, false otherwise.
      "classification_not_possible_reason": "string | null", // Brief reason if true (e.g., "Ambiguous", "Insufficient info"), null if false.
      "notes": "string | null" // Optional brief justification or reasoning, especially if confidence is low or not possible.
    }}
    // ... one entry for EACH vendor in <vendor_data>
  ]
}}

</output_format>"""

    # --- Assemble Final Prompt ---
    prompt_base = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>

<task>Classify each vendor provided in `<vendor_data>` into **ONE** appropriate NAICS category from the `<category_options>` for Level {level}. {f"Consider that these vendors belong to the parent category '{parent_category_id}: {parent_category_name}'. " if level > 1 and parent_category_id else ""}</task>"""

    if search_context_xml:
        prompt_base += f"""
<search_context_instruction>You have been provided with additional context from a web search in `<search_context>`. Use this information, along with the original `<vendor_data>`, to make the most accurate classification decision for Level {level}.</search_context_instruction>"""

    prompt_base += f"""
<instructions>
1.  Analyze each vendor's details in `<vendor_data>` {f"and the supplementary information in `<search_context>`" if search_context_xml else ""}.
2.  Compare the vendor's likely primary business activity against the available categories in `<category_options>`.
3.  Assign the **single most specific and appropriate** category ID and name from the list.
4.  Provide a confidence score (0.0 to 1.0).
5.  **CRITICAL:** If the vendor's primary activity is genuinely ambiguous, cannot be determined from the provided information, or does not fit well into *any* of the specific categories listed in `<category_options>`, **DO NOT GUESS**. Instead: Set `classification_not_possible` to `true`, `confidence` to `0.0`, provide a brief `classification_not_possible_reason`, and set `category_id`/`category_name` to "N/A".
6.  If classification *is* possible (`classification_not_possible: false`), ensure `confidence` > 0.0 and `category_id`/`category_name` are populated correctly from `<category_options>`.
7.  Provide brief optional `notes` for reasoning, especially if confidence is low or classification was not possible.
8.  Ensure the `batch_id` in the final JSON output matches the `batch_id` specified in `<output_format>`.
9.  Ensure the output contains an entry for **every** vendor listed in `<vendor_data>`.
10. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>

{vendor_data_xml}
{search_context_xml if search_context_xml else ""}
{category_options_xml}
{output_format_xml}
"""
    prompt = prompt_base

    if not category_lookup_successful:
         prompt = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>
<task>Acknowledge that classification is not possible for the vendors in `<vendor_data>` at Level {level} because the necessary subcategories could not be provided.</task>
<instructions>
1. For **every** vendor listed in `<vendor_data>`, create a classification entry in the final JSON output.
2. In each entry, set `classification_not_possible` to `true`.
3. Set `confidence` to `0.0`.
4. Set `category_id` and `category_name` to "N/A".
5. Set `classification_not_possible_reason` to "No subcategories defined or retrievable for parent {parent_category_id} at Level {level}".
6. Ensure the `batch_id` in the final JSON output matches the `batch_id` specified in `<output_format>`.
7. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>
{vendor_data_xml}
{category_options_xml}
{output_format_xml}
"""

    return prompt


def generate_search_prompt(
    vendor_data: Dict[str, Any],
    search_results: Dict[str, Any],
    taxonomy: Taxonomy,
    attempt_id: str = "unknown-attempt"
) -> str:
    """
    Create a prompt for processing search results, aiming for Level 1 classification.
    """
    logger.debug(f"Entering generate_search_prompt for vendor: {vendor_data.get('vendor_name', 'Unknown')}")
    vendor_name = vendor_data.get('vendor_name', 'UnknownVendor')
    example = vendor_data.get('example')
    address = vendor_data.get('vendor_address')
    website = vendor_data.get('vendor_website')
    internal_cat = vendor_data.get('internal_category')
    parent_co = vendor_data.get('parent_company')
    spend_cat = vendor_data.get('spend_category')

    logger.debug(f"Creating search results prompt for vendor",
                extra={ "vendor": vendor_name, "source_count": len(search_results.get("sources", [])), "attempt_id": attempt_id })

    # --- Build Vendor Data Section ---
    vendor_data_xml = "<vendor_data>\n"
    vendor_data_xml += f"  <name>{vendor_name}</name>\n"
    if example: vendor_data_xml += f"  <example_goods_services>{str(example)[:300]}</example_goods_services>\n"
    if address: vendor_data_xml += f"  <address>{str(address)[:200]}</address>\n"
    if website: vendor_data_xml += f"  <website>{str(website)[:100]}</website>\n"
    if internal_cat: vendor_data_xml += f"  <internal_category>{str(internal_cat)[:100]}</internal_category>\n"
    if parent_co: vendor_data_xml += f"  <parent_company>{str(parent_co)[:100]}</parent_company>\n"
    if spend_cat: vendor_data_xml += f"  <spend_category>{str(spend_cat)[:100]}</spend_category>\n"
    vendor_data_xml += "</vendor_data>"

    # --- Build Search Results Section ---
    search_results_xml = "<search_results>\n"
    sources = search_results.get("sources")
    if sources and isinstance(sources, list):
        search_results_xml += "  <sources>\n"
        for i, source in enumerate(sources):
            content_preview = str(source.get('content', ''))[:1500] # Limit length
            search_results_xml += f"    <source index=\"{i+1}\">\n"
            search_results_xml += f"      <title>{source.get('title', 'N/A')}</title>\n"
            search_results_xml += f"      <url>{source.get('url', 'N/A')}</url>\n"
            search_results_xml += f"      <content_snippet>{content_preview}...</content_snippet>\n"
            search_results_xml += f"    </source>\n"
        search_results_xml += "  </sources>\n"
    else:
        search_results_xml += "  <message>No relevant search results sources were found.</message>\n"

    summary_str = search_results.get("summary", "")
    if summary_str:
        search_results_xml += f"  <summary>{summary_str}</summary>\n"
    search_results_xml += "</search_results>"

    # --- Get Level 1 Category Options ---
    categories = taxonomy.get_level1_categories()
    category_options_xml = "<category_options>\n"
    category_options_xml += "  <level>1</level>\n" # Explicitly state Level 1
    category_options_xml += "  <categories>\n"
    for cat in categories:
        category_options_xml += f"    <category id=\"{cat.id}\" name=\"{cat.name}\"/>\n" # Omit description
    category_options_xml += "  </categories>\n"
    category_options_xml += "</category_options>"

    # --- Define Output Format Section ---
    output_format_xml = f"""<output_format>
Respond *only* with a valid JSON object matching this exact schema. Do not include any text before or after the JSON object.

json
{{
  "attempt_id": "{attempt_id}", // ID for this specific attempt
  "vendor_name": "{vendor_name}", // Exact vendor name from input <vendor_data>
  "category_id": "string", // Level 1 ID from <category_options> or "N/A" if not possible
  "category_name": "string", // Name corresponding to category_id or "N/A"
  "confidence": "float", // 0.0 to 1.0. MUST be 0.0 if classification_not_possible is true.
  "classification_not_possible": "boolean", // true if classification cannot be confidently made from options based *only* on provided info, false otherwise.
  "classification_not_possible_reason": "string | null", // Brief reason if true (e.g., "Insufficient info", "Conflicting sources"), null if false.
  "notes": "string | null" // Brief explanation of decision based *only* on the provided context and search results. Reference specific sources if helpful.
}}

</output_format>"""

    # --- Assemble Final Prompt ---
    prompt = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>

<task>Analyze the vendor details in `<vendor_data>` and the web search information in `<search_results>` to classify the vendor into **ONE** appropriate **Level 1** NAICS category from `<category_options>`. Base your decision *only* on the provided information.</task>

<instructions>
1.  Carefully review the vendor details in `<vendor_data>` (name, examples, address, website, internal category, parent company, spend category).
2.  Carefully review the search results in `<search_results>` (sources and summary).
3.  Synthesize all provided information to understand the vendor's **primary business activity**. Focus on what the company *does*, not just what it might resell.
4.  Compare this primary activity against the **Level 1** categories listed in `<category_options>`.
5.  Assign the **single most appropriate** Level 1 category ID and name.
6.  Provide a confidence score (0.0 to 1.0) based on the clarity, consistency, and relevance of the provided information.
7.  **CRITICAL:** If the provided information (vendor data + search results) is insufficient, contradictory, irrelevant, focuses only on products sold rather than the business activity, or does not allow for confident determination of the primary business activity *from the listed L1 categories*, **DO NOT GUESS**. Instead: Set `classification_not_possible` to `true`, `confidence` to `0.0`, provide a brief `classification_not_possible_reason`, and set `category_id`/`category_name` to "N/A".
8.  If classification *is* possible (`classification_not_possible: false`), ensure `confidence` > 0.0 and `category_id`/`category_name` are populated correctly from `<category_options>`.
9.  Provide brief optional `notes` explaining your reasoning, referencing specific details from `<vendor_data>` or `<search_results>`.
10. Ensure the `vendor_name` in the final JSON output matches the name in `<vendor_data>`.
11. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>

{vendor_data_xml}

{search_results_xml}

{category_options_xml}

{output_format_xml}
"""
    return prompt
</file>

<file path='app/tasks/classification_tasks.py'>

# app/tasks/classification_tasks.py
import os
import asyncio
import logging
from datetime import datetime
from celery import shared_task
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from typing import Dict, Any # <<< ADDED IMPORTS

from core.database import SessionLocal
from core.config import settings
from core.logging_config import get_logger
# Import context functions from the new module
from core.log_context import set_correlation_id, set_job_id, set_log_context, clear_all_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_duration

from models.job import Job, JobStatus, ProcessingStage
from services.file_service import read_vendor_file, normalize_vendor_data, generate_output_file
from services.llm_service import LLMService
from services.search_service import SearchService
from utils.taxonomy_loader import load_taxonomy

# Import the refactored logic
from .classification_logic import process_vendors

# Configure logger
logger = get_logger("vendor_classification.tasks")
# --- ADDED: Log confirmation ---
logger.debug("Successfully imported Dict and Any from typing for classification tasks.")
# --- END ADDED ---

@shared_task(bind=True)
# --- UPDATED: Added target_level parameter ---
def process_vendor_file(self, job_id: str, file_path: str, target_level: int):
# --- END UPDATED ---
    """
    Celery task entry point for processing a vendor file.
    Orchestrates the overall process by calling the main async helper.

    Args:
        job_id: Job ID
        file_path: Path to vendor file
        target_level: The desired maximum classification level (1-5)
    """
    task_id = self.request.id if self.request and self.request.id else "UnknownTaskID"
    logger.info(f"***** process_vendor_file TASK RECEIVED *****",
                extra={
                    "celery_task_id": task_id,
                    "job_id_arg": job_id,
                    "file_path_arg": file_path,
                    "target_level_arg": target_level # Log received target level
                })

    set_correlation_id(job_id) # Set correlation ID early
    set_job_id(job_id)
    set_log_context({"target_level": target_level}) # Add target level to context
    logger.info(f"Starting vendor file processing task (inside function)",
                extra={"job_id": job_id, "file_path": file_path, "target_level": target_level})

    # Validate target_level
    if not 1 <= target_level <= 5:
        logger.error(f"Invalid target_level received: {target_level}. Must be between 1 and 5.")
        # Fail the job immediately if level is invalid
        db_fail = SessionLocal()
        try:
            job_fail = db_fail.query(Job).filter(Job.id == job_id).first()
            if job_fail:
                job_fail.fail(f"Invalid target level specified: {target_level}. Must be 1-5.")
                db_fail.commit()
        except Exception as db_err:
            logger.error("Failed to mark job as failed due to invalid target level", exc_info=db_err)
            db_fail.rollback()
        finally:
            db_fail.close()
        return # Stop task execution

    # Initialize loop within the task context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    logger.debug(f"Created and set new asyncio event loop for job {job_id}")

    db = SessionLocal()
    job = None # Initialize job to None

    try:
        job = db.query(Job).filter(Job.id == job_id).first()
        if job:
            # Verify the target level matches the job record (optional sanity check)
            if job.target_level != target_level:
                logger.warning(f"Task received target_level {target_level} but job record has {job.target_level}. Using task value: {target_level}.")
                # Optionally update job record here if desired, or just proceed with task value

            set_log_context({
                "company_name": job.company_name,
                "creator": job.created_by,
                "file_name": job.input_file_name
                # target_level already set above
            })
            logger.info(f"Processing file for company",
                        extra={"company": job.company_name})
        else:
            logger.error("Job not found in database at start of task!", extra={"job_id": job_id})
            loop.close() # Close loop if job not found
            db.close() # Close db session
            return # Exit task if job doesn't exist

        logger.info(f"About to run async processing for job {job_id}")
        with LogTimer(logger, "Complete file processing", level=logging.INFO, include_in_stats=True):
            # Run the async function within the loop created for this task
            # --- UPDATED: Pass target_level to async helper ---
            loop.run_until_complete(_process_vendor_file_async(job_id, file_path, db, target_level))
            # --- END UPDATED ---

        logger.info(f"Vendor file processing completed successfully (async part finished)")

    except Exception as e:
        logger.error(f"Error processing vendor file task (in main try block)", exc_info=True, extra={"job_id": job_id})
        try:
            # Re-query the job within this exception handler if it wasn't fetched initially or became None
            db_error_session = SessionLocal()
            try:
                job_in_error = db_error_session.query(Job).filter(Job.id == job_id).first()
                if job_in_error:
                    if job_in_error.status != JobStatus.COMPLETED.value:
                        err_msg = f"Task failed: {type(e).__name__}: {str(e)}"
                        job_in_error.fail(err_msg[:2000]) # Limit error message length
                        db_error_session.commit()
                        logger.info(f"Job status updated to failed due to task error",
                                    extra={"error": str(e)})
                    else:
                        logger.warning(f"Task error occurred after job was marked completed, status not changed.",
                                        extra={"error": str(e)})
                else:
                    logger.error("Job not found when trying to mark as failed.", extra={"job_id": job_id})
            except Exception as db_error:
                logger.error(f"Error updating job status during task failure handling", exc_info=True,
                            extra={"original_error": str(e), "db_error": str(db_error)})
                db_error_session.rollback()
            finally:
                    db_error_session.close()
        except Exception as final_db_error:
                logger.critical(f"CRITICAL: Failed even to handle database update in task error handler.", exc_info=final_db_error)

    finally:
        if db: # Close the main session used by the async function
            db.close()
            logger.debug(f"Main database session closed for task.")
        if loop and not loop.is_closed():
            loop.close()
            logger.debug(f"Event loop closed for task.")
        clear_all_context()
        logger.info(f"***** process_vendor_file TASK FINISHED *****", extra={"job_id": job_id})


# --- UPDATED: Added target_level parameter ---
async def _process_vendor_file_async(job_id: str, file_path: str, db: Session, target_level: int):
# --- END UPDATED ---
    """
    Asynchronous part of the vendor file processing.
    Sets up services, initializes stats, calls the core processing logic,
    and handles final result generation and job status updates.
    """
    logger.info(f"[_process_vendor_file_async] Starting async processing for job {job_id} to target level {target_level}")

    llm_service = LLMService()
    search_service = SearchService()

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.error(f"[_process_vendor_file_async] Job not found in database", extra={"job_id": job_id})
        return

    # --- Initialize stats (Updated for L5) ---
    start_time = datetime.now()
    # --- MODIFIED: Type hints added ---
    stats: Dict[str, Any] = {
        "job_id": job.id,
        "company_name": job.company_name,
        "target_level": target_level, # Store target level in stats
        "start_time": start_time.isoformat(),
        "end_time": None,
        "processing_duration_seconds": None,
        "total_vendors": 0,
        "unique_vendors": 0,
        "successfully_classified_l4": 0, # Keep L4 count for reference
        "successfully_classified_l5": 0, # Count successful classifications reaching L5 (if target >= 5)
        "classification_not_possible_initial": 0, # Count initially unclassifiable before search
        "invalid_category_errors": 0, # Track validation errors
        "search_attempts": 0, # Count how many vendors needed search
        "search_successful_classifications_l1": 0, # Count successful L1 classifications *after* search
        "search_successful_classifications_l5": 0, # Count successful L5 classifications *after* search (if target >= 5)
        "api_usage": {
            "openrouter_calls": 0,
            "openrouter_prompt_tokens": 0,
            "openrouter_completion_tokens": 0,
            "openrouter_total_tokens": 0,
            "tavily_search_calls": 0,
            "cost_estimate_usd": 0.0
        }
    }
    # --- END MODIFIED ---
    # --- End Initialize stats ---

    try:
        job.status = JobStatus.PROCESSING.value
        job.current_stage = ProcessingStage.INGESTION.value
        job.progress = 0.05
        logger.info(f"[_process_vendor_file_async] Committing initial status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"status": job.status, "stage": job.current_stage, "progress": job.progress})

        logger.info(f"Reading vendor file")
        with log_duration(logger, "Reading vendor file"):
            vendors_data = read_vendor_file(file_path)
        logger.info(f"Vendor file read successfully",
                    extra={"vendor_count": len(vendors_data)})

        job.current_stage = ProcessingStage.NORMALIZATION.value
        job.progress = 0.1
        logger.info(f"[_process_vendor_file_async] Committing status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        logger.info(f"Normalizing vendor data")
        with log_duration(logger, "Normalizing vendor data"):
            normalized_vendors_data = normalize_vendor_data(vendors_data)
        logger.info(f"Vendor data normalized",
                    extra={"normalized_count": len(normalized_vendors_data)})

        logger.info(f"Identifying unique vendors")
        # --- MODIFIED: Type hints added ---
        unique_vendors_map: Dict[str, Dict[str, Any]] = {}
        # --- END MODIFIED ---
        for entry in normalized_vendors_data:
            name = entry.get('vendor_name')
            if name and name not in unique_vendors_map:
                unique_vendors_map[name] = entry
        logger.info(f"Unique vendors identified",
                    extra={"unique_count": len(unique_vendors_map)})

        stats["total_vendors"] = len(normalized_vendors_data)
        stats["unique_vendors"] = len(unique_vendors_map)

        logger.info(f"Loading taxonomy")
        with log_duration(logger, "Loading taxonomy"):
            taxonomy = load_taxonomy() # Can raise exceptions
        logger.info(f"Taxonomy loaded",
                    extra={"taxonomy_version": taxonomy.version})

        # --- MODIFIED: Type hints added ---
        results: Dict[str, Dict] = {vendor_name: {} for vendor_name in unique_vendors_map.keys()}
        # --- END MODIFIED ---

        logger.info(f"Starting vendor classification process by calling classification_logic.process_vendors up to Level {target_level}")
        # --- Call the refactored logic, passing target_level ---
        await process_vendors(
            unique_vendors_map=unique_vendors_map,
            taxonomy=taxonomy,
            results=results,
            stats=stats,
            job=job,
            db=db,
            llm_service=llm_service,
            search_service=search_service,
            target_level=target_level # Pass the target level
        )
        # --- End call to refactored logic ---
        logger.info(f"Vendor classification process completed (returned from classification_logic.process_vendors)")

        logger.info("Starting result generation phase.")

        job.current_stage = ProcessingStage.RESULT_GENERATION.value
        job.progress = 0.98 # Progress after all classification/search
        logger.info(f"[_process_vendor_file_async] Committing status update before result generation: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        output_file_name = None # Initialize
        try:
                logger.info(f"Generating output file")
                with log_duration(logger, "Generating output file"):
                    output_file_name = generate_output_file(normalized_vendors_data, results, job_id) # Can raise IOError
                logger.info(f"Output file generated", extra={"output_file": output_file_name})
        except Exception as gen_err:
                logger.error("Failed during output file generation", exc_info=True)
                job.fail(f"Failed to generate output file: {str(gen_err)}")
                db.commit()
                return # Stop processing

        # --- Finalize stats ---
        end_time = datetime.now()
        processing_duration = (end_time - datetime.fromisoformat(stats["start_time"])).total_seconds()
        stats["end_time"] = end_time.isoformat()
        stats["processing_duration_seconds"] = round(processing_duration, 2)
        # Cost calculation remains the same
        cost_input_per_1k = 0.0005
        cost_output_per_1k = 0.0015
        estimated_cost = (stats["api_usage"]["openrouter_prompt_tokens"] / 1000) * cost_input_per_1k + \
                            (stats["api_usage"]["openrouter_completion_tokens"] / 1000) * cost_output_per_1k
        estimated_cost += (stats["api_usage"]["tavily_search_calls"] / 1000) * 4.0
        stats["api_usage"]["cost_estimate_usd"] = round(estimated_cost, 4)
        # --- End Finalize stats ---

        # --- Final Commit Block ---
        try:
            logger.info("Attempting final job completion update in database.")
            job.complete(output_file_name, stats)
            job.progress = 1.0 # Ensure progress is 1.0 on completion
            logger.info(f"[_process_vendor_file_async] Committing final job completion status.")
            db.commit()
            logger.info(f"Job completed successfully",
                        extra={
                            "processing_duration": processing_duration,
                            "output_file": output_file_name,
                            "target_level": target_level,
                            "openrouter_calls": stats["api_usage"]["openrouter_calls"],
                            "tokens_used": stats["api_usage"]["openrouter_total_tokens"],
                            "tavily_calls": stats["api_usage"]["tavily_search_calls"],
                            "estimated_cost": stats["api_usage"]["cost_estimate_usd"],
                            "invalid_category_errors": stats.get("invalid_category_errors", 0),
                            "successfully_classified_l5_total": stats.get("successfully_classified_l5", 0)
                        })
        except Exception as final_commit_err:
            logger.error("CRITICAL: Failed to commit final job completion status!", exc_info=True)
            db.rollback()
            try:
                err_msg = f"Failed during final commit: {type(final_commit_err).__name__}: {str(final_commit_err)}"
                job.fail(err_msg[:2000])
                db.commit()
            except Exception as fail_err:
                logger.error("CRITICAL: Also failed to mark job as failed after final commit error.", exc_info=fail_err)
                db.rollback()
        # --- End Final Commit Block ---

    except (ValueError, FileNotFoundError, IOError) as file_err:
        logger.error(f"[_process_vendor_file_async] File reading or writing error", exc_info=True,
                    extra={"error": str(file_err)})
        if job:
            err_msg = f"File processing error: {type(file_err).__name__}: {str(file_err)}"
            job.fail(err_msg[:2000])
            db.commit()
        else:
            logger.error("Job object was None during file error handling.")
    except SQLAlchemyError as db_err:
        logger.error(f"[_process_vendor_file_async] Database error during processing", exc_info=True,
                    extra={"error": str(db_err)})
        if job:
            if job.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                    err_msg = f"Database error: {type(db_err).__name__}: {str(db_err)}"
                    job.fail(err_msg[:2000])
                    db.commit()
            else:
                    logger.warning(f"Database error occurred but job status was already {job.status}. Error: {db_err}")
                    db.rollback()
        else:
            logger.error("Job object was None during database error handling.")
    except Exception as async_err:
        logger.error(f"[_process_vendor_file_async] Unexpected error during async processing", exc_info=True,
                    extra={"error": str(async_err)})
        if job:
            if job.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                err_msg = f"Unexpected error: {type(async_err).__name__}: {str(async_err)}"
                job.fail(err_msg[:2000])
                db.commit()
            else:
                logger.warning(f"Unexpected error occurred but job status was already {job.status}. Error: {async_err}")
                db.rollback()
        else:
            logger.error("Job object was None during unexpected error handling.")
    finally:
        logger.info(f"[_process_vendor_file_async] Finished async processing for job {job_id}")
</file>

<file path='app/utils/taxonomy_loader.py'>
# <file path='app/utils/taxonomy_loader.py'>
# --- file path='app/utils/taxonomy_loader.py' ---
import os
import json
import pandas as pd
import re # <<< Added import
from typing import Dict, Any

# --- ADDED: Import logging for the main script part ---
import logging
# --- END ADDED ---

from core.config import settings
# --- MODIFIED IMPORT: Import all level classes including L5 ---
from models.taxonomy import Taxonomy, TaxonomyLevel1, TaxonomyLevel2, TaxonomyLevel3, TaxonomyLevel4, TaxonomyLevel5
# --- END MODIFIED IMPORT ---
from core.logging_config import get_logger

# Configure logger
logger = get_logger("vendor_classification.taxonomy_loader")

# --- Global cache for taxonomy ---
_taxonomy_cache: Taxonomy | None = None

def load_taxonomy(force_reload: bool = False) -> Taxonomy:
    """
    Load taxonomy data, using cache if available unless forced.
    Tries JSON first, then Excel as fallback.

    Args:
        force_reload: If True, bypass cache and reload from file.

    Returns:
        Taxonomy object

    Raises:
        FileNotFoundError: If neither JSON nor Excel file can be found.
        ValueError: If both JSON and Excel loading fail or result in empty taxonomy.
    """
    global _taxonomy_cache
    if _taxonomy_cache is not None and not force_reload:
        logger.info("Returning cached taxonomy.")
        return _taxonomy_cache

    excel_path = os.path.join(settings.TAXONOMY_DATA_DIR, "2022_NAICS_Codes.xlsx")
    json_path = os.path.join(settings.TAXONOMY_DATA_DIR, "naics_taxonomy.json")

    taxonomy = None

    # --- Try JSON first ---
    if os.path.exists(json_path):
        try:
            logger.info(f"Attempting to load taxonomy from JSON: {json_path}")
            with open(json_path, "r") as f:
                taxonomy_data = json.load(f)

            # Validate structure before creating Taxonomy object
            if not taxonomy_data.get("categories"):
                raise ValueError("JSON data is missing the 'categories' key.")

            taxonomy = Taxonomy(**taxonomy_data)
            logger.info(f"Taxonomy loaded successfully from JSON with {len(taxonomy.categories)} top-level categories.")
            _taxonomy_cache = taxonomy # Update cache
            return taxonomy
        except json.JSONDecodeError as json_err:
            logger.error(f"Failed to decode JSON from {json_path}: {json_err}", exc_info=False)
            logger.warning(f"JSON parsing failed. Will attempt fallback to Excel if available.")
        except Exception as e:
            logger.error(f"Error loading taxonomy from JSON: {e}", exc_info=True)
            logger.warning(f"Unexpected error loading JSON. Will attempt fallback to Excel if available.")
            taxonomy = None # Ensure taxonomy is None if JSON loading fails

    # --- Fallback to Excel if JSON failed or didn't exist ---
    if taxonomy is None and os.path.exists(excel_path):
        try:
            logger.warning(f"JSON load failed or file missing, attempting to load taxonomy from Excel: {excel_path}")
            taxonomy = load_taxonomy_from_excel(excel_path)

            # Save as JSON for future use IF successful
            logger.info(f"Saving newly loaded taxonomy to JSON: {json_path}")
            os.makedirs(settings.TAXONOMY_DATA_DIR, exist_ok=True)
            with open(json_path, "w") as f:
                # Use model_dump for Pydantic v2
                json.dump(taxonomy.model_dump(exclude_none=True, mode='json'), f, indent=2) # Added mode='json'

            logger.info(f"Taxonomy loaded successfully from Excel with {len(taxonomy.categories)} top-level categories and saved to JSON.")
            _taxonomy_cache = taxonomy # Update cache
            return taxonomy
        except Exception as e:
            logger.error(f"Error loading taxonomy from Excel after JSON failure: {e}", exc_info=True)
            taxonomy = None # Ensure taxonomy is None if Excel loading also fails

    # --- If both failed, raise error ---
    if taxonomy is None:
        error_msg = f"Failed to load taxonomy. Neither JSON ({json_path}) nor Excel ({excel_path}) file could be loaded or found."
        logger.critical(error_msg)
        raise FileNotFoundError(error_msg)

    # This part should theoretically not be reached if errors are raised correctly
    logger.critical("Reached unexpected end of load_taxonomy function.")
    raise RuntimeError("Taxonomy loading finished in an unexpected state.")


def load_taxonomy_from_excel(file_path: str) -> Taxonomy:
    """
    Load taxonomy from Excel file, building the hierarchical structure up to 5 levels.
    Correctly handles sector range titles.

    Args:
        file_path: Path to Excel file

    Returns:
        Taxonomy object

    Raises:
        FileNotFoundError: If the file_path does not exist.
        ValueError: If the file cannot be parsed or required columns are missing.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Taxonomy Excel file not found at {file_path}")

    try:
        # Read Excel file
        logger.debug(f"Reading Excel file: {file_path}")

        try:
            # Try reading as Excel first
            df = pd.read_excel(file_path, dtype=str) # Read all as string initially
            logger.debug(f"Successfully read Excel file with {len(df)} rows")
        except Exception as excel_error:
            # Fallback to CSV with pipe delimiter if Excel read fails
            logger.warning(f"Failed to read as Excel ({str(excel_error)}), trying as pipe-delimited text...")
            try:
                # Assuming the Excel file provided might actually be pipe-delimited
                df = pd.read_csv(file_path, delimiter='|', quotechar='"', dtype=str, skipinitialspace=True)
                logger.debug(f"Successfully read pipe-delimited file with {len(df)} rows")
            except Exception as csv_error:
                logger.error(f"Failed to read as pipe-delimited text: {str(csv_error)}")
                raise ValueError(f"Could not parse taxonomy file '{file_path}'. Tried Excel and Pipe-Delimited CSV.") from excel_error

        # --- Column Identification ---
        code_col_options = ['naics code', '2022 naics us code', 'code']
        title_col_options = ['naics title', '2022 naics us title', 'title']
        desc_col_options = ['description', 'desc']

        df_cols_lower = {col.lower().strip(): col for col in df.columns}

        code_column = next((df_cols_lower[opt] for opt in code_col_options if opt in df_cols_lower), None)
        title_column = next((df_cols_lower[opt] for opt in title_col_options if opt in df_cols_lower), None)
        desc_column = next((df_cols_lower[opt] for opt in desc_col_options if opt in df_cols_lower), None)

        if not code_column or not title_column:
            error_msg = f"Could not identify required 'code' and 'title' columns in the taxonomy file. Found columns: {list(df.columns)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        logger.info(f"Identified taxonomy columns - Code: '{code_column}', Title: '{title_column}', Desc: '{desc_column or 'None'}'")

        # --- Data Cleaning ---
        df = df.dropna(subset=[code_column])
        df[code_column] = df[code_column].astype(str).str.strip()
        df[title_column] = df[title_column].astype(str).str.strip().str.replace(r'\s*T$', '', regex=True) # Remove trailing 'T'

        if desc_column:
            df[desc_column] = df[desc_column].fillna('').astype(str).str.strip()
        else:
            desc_column = 'Description' # Assign a name
            df[desc_column] = ''
            logger.warning("No description column found, descriptions will be empty.")

        # --- Filter out rows with non-standard codes BEFORE processing ranges ---
        valid_code_pattern = r'^(\d{2}-\d{2}|\d+)$'
        original_row_count = len(df)
        df = df[df[code_column].str.match(valid_code_pattern)]
        filtered_row_count = len(df)
        if original_row_count != filtered_row_count:
            logger.warning(f"Filtered out {original_row_count - filtered_row_count} rows with invalid code formats (neither ##-## nor numeric).")

        # --- Build taxonomy structure ---
        categories_level1: Dict[str, TaxonomyLevel1] = {}
        logger.info("Building taxonomy hierarchy...")

        rows_processed = 0
        skipped_rows = 0
        for index, row in df.iterrows():
            code_raw = row[code_column].strip()
            title = row[title_column].strip()
            description = row[desc_column].strip()

            code = code_raw
            original_code = code_raw
            is_range = '-' in code

            # --- Range Handling ---
            if is_range:
                range_match = re.match(r'^(\d{2})-\d{2}$', code)
                if range_match:
                    start_code = range_match.group(1)
                    range_title = title
                    logger.debug(f"Row {index}: Processing range code '{original_code}' ('{range_title}') - Associating title with start code '{start_code}'")
                    if start_code not in categories_level1:
                        categories_level1[start_code] = TaxonomyLevel1(id=start_code, name=range_title, description=description, children={})
                        logger.info(f"Row {index}: Added L1 '{start_code}' using range title '{range_title}'.")
                    else:
                        existing_name = categories_level1[start_code].name
                        if existing_name != range_title:
                            logger.warning(f"Row {index}: L1 '{start_code}' already exists with name '{existing_name}'. Overwriting with range title '{range_title}'.")
                            categories_level1[start_code].name = range_title
                        if not categories_level1[start_code].description and description:
                             categories_level1[start_code].description = description
                             logger.debug(f"Row {index}: Updated empty L1 '{start_code}' description.")
                    rows_processed += 1
                    continue # Skip rest of hierarchy logic for this row
                else:
                    logger.warning(f"Row {index}: Skipping row with unhandled range format code: '{original_code}' ('{title}')")
                    skipped_rows += 1
                    continue
            elif not code.isdigit():
                logger.warning(f"Row {index}: Skipping row with non-numeric/non-range code: '{original_code}' ('{title}')")
                skipped_rows += 1
                continue
            # --- End Range Handling ---

            # --- Hierarchy Building for Numeric Codes ---
            code_length = len(code)
            try:
                if code_length == 2:  # Level 1 (Specific code)
                    if code not in categories_level1:
                        categories_level1[code] = TaxonomyLevel1(id=code, name=title, description=description, children={})
                        logger.debug(f"Row {index}: Added L1: {code} - {title}")
                    elif not categories_level1[code].description and description:
                        categories_level1[code].description = description
                        logger.debug(f"Row {index}: Updated L1 '{code}' description (already existed).")

                elif code_length == 3: # Level 2
                    l1_code = code[:2]
                    if l1_code in categories_level1:
                        l1_cat = categories_level1[l1_code]
                        if code not in l1_cat.children:
                            l1_cat.children[code] = TaxonomyLevel2(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:   Added L2: {code} under {l1_code}")
                    else:
                        logger.warning(f"Row {index}: L1 parent '{l1_code}' not found for L2 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 4: # Level 3
                    l1_code = code[:2]; l2_code = code[:3]
                    if l1_code in categories_level1 and l2_code in categories_level1[l1_code].children:
                        l2_cat = categories_level1[l1_code].children[l2_code]
                        if code not in l2_cat.children:
                            l2_cat.children[code] = TaxonomyLevel3(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:     Added L3: {code} under {l2_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2 parent '{l1_code}/{l2_code}' not found for L3 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 5: # Level 4
                    l1_code = code[:2]; l2_code = code[:3]; l3_code = code[:4]
                    if l1_code in categories_level1 and \
                       l2_code in categories_level1[l1_code].children and \
                       l3_code in categories_level1[l1_code].children[l2_code].children:
                        l3_cat = categories_level1[l1_code].children[l2_code].children[l3_code]
                        if code not in l3_cat.children:
                            # Create L4 with empty children dict for potential L5
                            l3_cat.children[code] = TaxonomyLevel4(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:       Added L4: {code} under {l3_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2/L3 parent '{l1_code}/{l2_code}/{l3_code}' not found for L4 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 6: # Level 5
                    l1_code = code[:2]; l2_code = code[:3]; l3_code = code[:4]; l4_code = code[:5]
                    if l1_code in categories_level1 and \
                       l2_code in categories_level1[l1_code].children and \
                       l3_code in categories_level1[l1_code].children[l2_code].children and \
                       l4_code in categories_level1[l1_code].children[l2_code].children[l3_code].children:
                        l4_cat = categories_level1[l1_code].children[l2_code].children[l3_code].children[l4_code]
                        if code not in l4_cat.children:
                            l4_cat.children[code] = TaxonomyLevel5(id=code, name=title, description=description)
                            logger.debug(f"Row {index}:         Added L5: {code} under {l4_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2/L3/L4 parent '{l1_code}/{l2_code}/{l3_code}/{l4_code}' not found for L5 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue
                else:
                     logger.warning(f"Row {index}: Code '{code}' has unhandled length {code_length}. Skipping.")
                     skipped_rows += 1; continue

                rows_processed += 1

            except Exception as hierarchy_error:
                 logger.error(f"Row {index}: Error building hierarchy for code '{code}' ('{title}')", exc_info=True)
                 skipped_rows += 1

        if not categories_level1:
            error_msg = f"No valid Level 1 categories found after processing {rows_processed + skipped_rows} rows from the file."
            logger.error(error_msg)
            raise ValueError(error_msg)

        # Create final taxonomy object
        taxonomy = Taxonomy(
            name="NAICS Taxonomy",
            version="2022", # Or extract from filename/content if possible
            description="North American Industry Classification System", # Or extract
            categories=categories_level1
        )

        # Log final stats including L5
        l1_count = len(taxonomy.categories)
        l2_count = sum(len(getattr(l1, 'children', {})) for l1 in taxonomy.categories.values())
        l3_count = sum(len(getattr(l2, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values())
        l4_count = sum(len(getattr(l3, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values() for l3 in getattr(l2, 'children', {}).values())
        l5_count = sum(len(getattr(l4, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values() for l3 in getattr(l2, 'children', {}).values() for l4 in getattr(l3, 'children', {}).values())
        logger.info(f"Taxonomy hierarchy built with {l1_count} L1, {l2_count} L2, {l3_count} L3, {l4_count} L4, {l5_count} L5 categories from {rows_processed} processed rows ({skipped_rows} skipped).")

        return taxonomy

    except FileNotFoundError: # Raised explicitly above
        raise
    except ValueError as ve: # Raised explicitly above for parsing/column errors
         logger.error(f"Value error processing taxonomy Excel file '{file_path}': {ve}", exc_info=False)
         raise # Re-raise specific error
    except Exception as e:
        logger.error(f"Unexpected error processing taxonomy Excel file '{file_path}': {e}", exc_info=True)
        raise ValueError(f"Could not process taxonomy Excel file: {e}") from e


# --- create_sample_taxonomy function remains unchanged ---
# (It's a fallback and doesn't need L5 for this update)
def create_sample_taxonomy() -> Taxonomy:
    """
    Create a sample NAICS taxonomy for testing or fallback.
    (This function remains unchanged from the original provided code)
    """
    # Level 4 categories
    level4_categories_11 = {
        "111110": TaxonomyLevel4(id="111110", name="Soybean Farming", description="...", children={}), # Add empty children
        "111120": TaxonomyLevel4(id="111120", name="Oilseed (except Soybean) Farming", description="...", children={}),
    }
    # ... other sample L4 categories ...
    level4_categories_23 = { "236115": TaxonomyLevel4(id="236115", name="New Single-Family Housing Construction", description="...", children={}) }
    level4_categories_51 = { "513210": TaxonomyLevel4(id="513210", name="Software Publishers", description="...", children={}) }

    # Create level 3 categories
    level3_categories_11 = {
        "1111": TaxonomyLevel3(id="1111", name="Oilseed and Grain Farming", description="...", children=level4_categories_11),
    }
    level3_categories_23 = { "2361": TaxonomyLevel3(id="2361", name="Residential Building Construction", description="...", children=level4_categories_23) }
    level3_categories_51 = { "5132": TaxonomyLevel3(id="5132", name="Software Publishers", description="...", children=level3_categories_51) }


    # Create level 2 categories
    level2_categories_11 = {
        "111": TaxonomyLevel2(id="111", name="Crop Production", description="...", children=level3_categories_11),
    }
    level2_categories_23 = { "236": TaxonomyLevel2(id="236", name="Construction of Buildings", description="...", children=level3_categories_23) }
    level2_categories_51 = { "513": TaxonomyLevel2(id="513", name="Publishing Industries", description="...", children=level3_categories_51) }

    # Create level 1 categories
    level1_categories = {
        "11": TaxonomyLevel1(id="11", name="Agriculture, Forestry, Fishing and Hunting", description="...", children=level2_categories_11),
        "23": TaxonomyLevel1(id="23", name="Construction", description="...", children=level2_categories_23),
        "51": TaxonomyLevel1(id="51", name="Information", description="...", children=level2_categories_51)
    }

    # Create taxonomy
    taxonomy = Taxonomy(
        name="NAICS Taxonomy (Sample)",
        version="2022_Sample",
        description="Sample North American Industry Classification System",
        categories=level1_categories
    )
    logger.warning("Generated and using a sample taxonomy.")
    return taxonomy


if __name__ == "__main__":
     # --- ADDED: Import logging for test block ---
     import logging
     # --- END ADDED ---
     # Example usage for testing
     print("Testing taxonomy loader...")
     try:
         # Ensure settings are available or provide a default path
         if 'settings' not in locals() and 'settings' not in globals():
              class MockSettings:
                  TAXONOMY_DATA_DIR = os.path.join(os.path.dirname(__file__), '../../data/taxonomy')
              settings = MockSettings()
              print(f"Using mock settings for TAXONOMY_DATA_DIR: {settings.TAXONOMY_DATA_DIR}")
         else:
              if not hasattr(settings, 'TAXONOMY_DATA_DIR') or not settings.TAXONOMY_DATA_DIR:
                   settings.TAXONOMY_DATA_DIR = os.path.join(os.path.dirname(__file__), '../../data/taxonomy')
              print(f"Using settings.TAXONOMY_DATA_DIR: {settings.TAXONOMY_DATA_DIR}")

         # --- Ensure log directory exists for testing ---
         log_test_dir = "./logs_test"
         os.makedirs(log_test_dir, exist_ok=True)
         from core.logging_config import setup_logging
         setup_logging(log_level=logging.DEBUG, log_to_file=True, log_dir=log_test_dir, async_logging=False)
         # ---

         # Force reload from Excel (or CSV fallback)
         tax = load_taxonomy(force_reload=True)
         print(f"\nLoaded taxonomy: {tax.name} - Version: {tax.version}")
         print(f"Number of L1 categories: {len(tax.categories)}")

         # Verification for a specific L5 code (example)
         print("\n--- Verification for L5 Code (e.g., 311111) ---")
         try:
             l1 = tax.categories.get('31')
             l2 = l1.children.get('311') if l1 else None
             l3 = l2.children.get('3111') if l2 else None
             l4 = l3.children.get('31111') if l3 else None
             l5 = l4.children.get('311111') if l4 else None
             if l5:
                 print(f"L5 '311111' FOUND. Name: '{l5.name}'")
             else:
                 print("L5 '311111' NOT FOUND (or parent missing).")
         except Exception as e:
             print(f"Error during L5 verification: {e}")

     except Exception as e:
         print(f"\nError during testing: {e}")
         import traceback
         traceback.print_exc()
</file>

<file path='frontend/vue_frontend/src/components/JobHistory.vue'>
<template>
    <div class="mt-10 bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200">
        <h4 class="text-xl font-semibold mb-0">Job History</h4>
      </div>
      <div class="p-6 sm:p-8">
        <!-- Loading State -->
        <div v-if="historyLoading" class="text-center text-gray-500 py-8">
          <svg class="animate-spin inline-block h-6 w-6 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
          <span>Loading job history...</span>
        </div>
  
        <!-- Error State -->
        <div v-else-if="historyError" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
          <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
          <span>Error loading history: {{ historyError }}</span>
        </div>
  
        <!-- Empty State -->
        <div v-else-if="!jobHistory || jobHistory.length === 0" class="text-center text-gray-500 py-8">
          <p>No job history found.</p>
          <p class="text-sm">Upload a file to start your first job.</p>
        </div>
  
        <!-- History Table -->
        <div v-else class="overflow-x-auto">
          <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
              <tr>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Job ID
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Company
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Status
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Created
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Actions
                </th>
              </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
              <tr v-for="job in jobHistory" :key="job.id" class="hover:bg-gray-50 cursor-pointer" @click="selectJob(job.id)">
                <td class="px-4 py-3 whitespace-nowrap text-xs font-mono text-gray-700">
                  {{ job.id.substring(0, 8) }}...
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-800 font-medium">
                  {{ job.company_name }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap">
                  <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="getStatusBadgeClass(job.status)">
                    {{ job.status }}
                  </span>
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-500">
                  {{ formatDateTime(job.created_at) }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-right text-sm font-medium">
                  <button
                    v-if="job.status === 'completed'"
                    @click.stop="downloadResults(job.id, $event)"
                    :disabled="isDownloadLoading(job.id)"
                    class="text-primary hover:text-primary-hover disabled:opacity-50 disabled:cursor-not-allowed inline-flex items-center"
                    title="Download Results"
                  >
                     <svg v-if="isDownloadLoading(job.id)" class="animate-spin h-4 w-4 text-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                     </svg>
                     <ArrowDownTrayIcon v-else class="h-4 w-4" />
                    <!-- <span class="ml-1">Download</span> -->
                  </button>
                  <span v-else-if="job.status === 'failed'" class="text-red-500 text-xs italic" title="Job Failed">Failed</span>
                  <span v-else class="text-gray-400 text-xs italic" title="Processing or Pending">In Progress</span>
                  <!-- Add View Details button if needed -->
                  <!-- <button @click.stop="selectJob(job.id)" class="text-indigo-600 hover:text-indigo-900 ml-3">View</button> -->
                </td>
              </tr>
            </tbody>
          </table>
        </div>
  
        <!-- TODO: Add Pagination Controls if needed -->
  
      </div>
    </div>
  </template>
  
  <script setup lang="ts">
  import { computed, onMounted, ref } from 'vue';
  import { useJobStore } from '@/stores/job';
  import { ExclamationTriangleIcon, ArrowDownTrayIcon } from '@heroicons/vue/20/solid';
  import apiService from '@/services/api';
  
  const jobStore = useJobStore();
  
  const jobHistory = computed(() => jobStore.jobHistory);
  const historyLoading = computed(() => jobStore.historyLoading);
  const historyError = computed(() => jobStore.historyError);
  
  // State for managing individual download button loading
  const downloadingJobs = ref<Set<string>>(new Set());
  const downloadErrors = ref<Record<string, string | null>>({});
  
  const isDownloadLoading = (jobId: string) => downloadingJobs.value.has(jobId);
  
  const fetchHistory = async () => {
    await jobStore.fetchJobHistory({ limit: 100 }); // Fetch latest 100 jobs on mount
  };
  
  const selectJob = (jobId: string) => {
    console.log(`JobHistory: Selecting job ${jobId}`);
    jobStore.setCurrentJobId(jobId);
    // Optional: Scroll to the top or to the JobStatus component
    window.scrollTo({ top: 0, behavior: 'smooth' });
  };
  
  const formatDateTime = (isoString: string | null | undefined): string => {
    if (!isoString) return 'N/A';
    try {
      return new Date(isoString).toLocaleString(undefined, {
        year: 'numeric', month: 'short', day: 'numeric',
        hour: 'numeric', minute: '2-digit', hour12: true
      });
    } catch {
      return 'Invalid Date';
    }
  };
  
  const getStatusBadgeClass = (status: string | undefined) => {
    switch (status) {
      case 'completed': return 'bg-green-100 text-green-800';
      case 'failed': return 'bg-red-100 text-red-800';
      case 'processing': return 'bg-blue-100 text-blue-800';
      default: return 'bg-gray-100 text-gray-800';
    }
  };
  
  const downloadResults = async (jobId: string, event: Event) => {
     event.stopPropagation(); // Prevent row click when clicking button
     if (!jobId || downloadingJobs.value.has(jobId)) return;
  
     downloadingJobs.value.add(jobId);
     downloadErrors.value[jobId] = null;
  
    try {
      const { blob, filename } = await apiService.downloadResults(jobId);
      const url = window.URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.style.display = 'none';
      a.href = url;
      a.download = filename;
      document.body.appendChild(a);
      a.click();
      window.URL.revokeObjectURL(url);
      document.body.removeChild(a);
    } catch (error: any) {
      console.error(`Download failed for job ${jobId}:`, error);
      downloadErrors.value[jobId] = `Download failed: ${error.message || 'Error'}`;
      // Optionally clear the error message after a delay
      setTimeout(() => { downloadErrors.value[jobId] = null; }, 5000);
    } finally {
      downloadingJobs.value.delete(jobId);
    }
  };
  
  
  onMounted(() => {
    fetchHistory();
  });
  </script>
  
  <style scoped>
  /* Add specific styles if needed */
  tbody tr:hover {
    background-color: #f9fafb; /* Tailwind gray-50 */
  }
  </style>
</file>

<file path='frontend/vue_frontend/src/components/JobStats.vue'>
    <template>
        <div class="mt-6 bg-gray-50 rounded-lg p-6 border border-gray-200 shadow-inner">
          <h5 class="text-lg font-semibold text-gray-700 mb-5 border-b border-gray-200 pb-3">
              Processing Statistics
              <!-- ADDED: Display Target Level -->
              <span v-if="jobTargetLevel" class="text-sm font-normal text-gray-500 ml-2">(Target Level: {{ jobTargetLevel }})</span>
          </h5>
          <div v-if="isLoading" class="text-center text-gray-500 py-4">
                <svg class="animate-spin inline-block h-5 w-5 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                   <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                   <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                </svg>
                <span>Loading stats...</span>
          </div>
          <div v-else-if="error" class="p-3 bg-yellow-100 border border-yellow-300 text-yellow-800 rounded-md text-sm">
              {{ error }}
          </div>
          <div v-else-if="stats" class="grid grid-cols-1 sm:grid-cols-2 gap-x-8 gap-y-4 text-sm">
              <!-- Column 1: Vendor & Classification Stats -->
              <div class="space-y-2.5">
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Total Vendors:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.total_vendors?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Unique Vendors:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.unique_vendors?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- UPDATED: Display L5 Success (Keep label static for now, but value is correct) -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Successfully Classified (L5):</strong>
                      <span class="text-green-700 font-semibold">{{ stats.successfully_classified_l5?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- UPDATED: Display L5 Search Success (Corrected field name) -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Search Assisted (L5):</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.search_successful_classifications_l5?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- Keep L4 for reference if desired -->
                   <p class="flex justify-between text-xs text-gray-500">
                      <strong class="font-normal">Ref: Classified (L4):</strong>
                      <span class="font-normal">{{ stats.successfully_classified_l4?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Invalid Category Errors:</strong>
                      <span :class="(stats.invalid_category_errors ?? 0) > 0 ? 'text-red-600 font-semibold' : 'text-gray-800 font-semibold'">
                          {{ stats.invalid_category_errors?.toLocaleString() ?? 'N/A' }}
                      </span>
                  </p>
              </div>
               <!-- Column 2: API & Time Stats -->
               <div class="space-y-2.5">
                  <!-- UPDATED: Access nested fields -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">LLM API Calls:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.openrouter_calls?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">LLM Tokens Used:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.openrouter_total_tokens?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Search API Calls:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.tavily_search_calls?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between pt-2 mt-1 border-t border-gray-200">
                      <strong class="text-gray-600 font-medium">Processing Time:</strong>
                      <!-- UPDATED: Use correct field name -->
                      <span class="text-gray-800 font-semibold">{{ formattedTime }}</span>
                  </p>
               </div>
          </div>
           <div v-else class="text-gray-500 text-center py-4 text-sm">No statistics available for this job.</div>
        </div>
      </template>
    
    <script setup lang="ts">
    import { ref, onMounted, watch, computed } from 'vue';
    import apiService, { type JobStatsData } from '@/services/api';
    // ADDED: Import job store to access target level
    import { useJobStore } from '@/stores/job';
    
    // Define the component props
    interface Props {
        jobId: string | null | undefined; // Allow jobId to be potentially null or undefined
    }
    const props = defineProps<Props>();
    
    // ADDED: Access job store
    const jobStore = useJobStore();
    
    // Reactive state variables
    const stats = ref<JobStatsData | null>(null); // Use the imported type
    const isLoading = ref(false);
    const error = ref<string | null>(null);
    
    // ADDED: Computed property to get target level from store
    const jobTargetLevel = computed(() => jobStore.jobDetails?.target_level);
    
    // Computed property to format processing time nicely
    const formattedTime = computed(() => {
        // UPDATED: Access correct field name
        if (stats.value?.processing_duration_seconds == null) return 'N/A'; // Check for null or undefined
        const seconds = stats.value.processing_duration_seconds;
        if (seconds < 0) return 'N/A'; // Handle potential negative values if they occur
        if (seconds < 1) return `${(seconds * 1000).toFixed(0)} ms`;
        if (seconds < 60) return `${seconds.toFixed(1)} seconds`;
        const minutes = Math.floor(seconds / 60);
        const remainingSeconds = (seconds % 60).toFixed(0);
        return `${minutes} min ${remainingSeconds} sec`;
    });
    
    /**
     * Fetches job statistics from the API for the given job ID.
     * @param {string | null | undefined} id - The job ID to fetch stats for.
     */
    const fetchStats = async (id: string | null | undefined) => {
      // Only proceed if id is a non-empty string
      if (!id) {
          console.log("JobStats: fetchStats called with no ID, skipping."); // Logging
          stats.value = null; // Clear previous stats if ID is null/undefined
          error.value = null;
          isLoading.value = false;
          return;
      }
    
      isLoading.value = true;
      error.value = null;
      // Don't clear stats immediately, only on successful fetch or error
      // stats.value = null;
    
      try {
          console.log(`JobStats: Fetching stats for job ID: ${id}`); // Logging
          // The API service now returns the updated structure
          stats.value = await apiService.getJobStats(id);
          // LOGGING: Log the received stats structure after fetch
          console.log(`JobStats: Received and assigned stats:`, JSON.parse(JSON.stringify(stats.value)));
      } catch (err: any) {
          console.error(`JobStats: Error fetching stats for ${id}:`, err); // Logging
          error.value = err.message || 'Failed to load statistics.';
          stats.value = null; // Clear stats on error
      } finally {
          isLoading.value = false;
      }
    };
    
    // Fetch stats when the component mounts
    onMounted(() => {
        console.log(`JobStats: Mounted with initial jobId: ${props.jobId}`); // Logging
        fetchStats(props.jobId);
    });
    
    // Watch for changes in the jobId prop and refetch stats
    watch(() => props.jobId,
      (newJobId: string | null | undefined) => {
        console.log(`JobStats: Watched jobId changed to: ${newJobId}`); // Logging
        fetchStats(newJobId); // fetchStats handles null/undefined check internally
      },
      { immediate: false } // Don't run immediately, onMounted handles initial fetch
    );
    </script>
    
    <style scoped>
      .shadow-inner {
          box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.05);
      }
      /* Add any other specific styles if needed */
    </style>
</file>

<file path='frontend/vue_frontend/src/components/JobStatus.vue'>
<template>
    <div v-if="jobStore.currentJobId" class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200">
        <h4 class="text-xl font-semibold mb-0">Job Status</h4>
      </div>
      <div class="p-6 sm:p-8 space-y-6"> <!-- Increased spacing -->

        <!-- Error Message -->
        <div v-if="errorMessage" class="p-3 bg-yellow-100 border border-yellow-300 text-yellow-800 rounded-md text-sm flex items-center">
            <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-yellow-600 flex-shrink-0"/>
            <span>{{ errorMessage }}</span>
        </div>

        <!-- Job ID & Status Row -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm border-b border-gray-100 pb-4">
          <div>
             <strong class="text-gray-600 block mb-1">Job ID:</strong>
             <!-- Display the full ID for clarity during debugging -->
             <span class="text-gray-900 font-mono text-xs bg-gray-100 px-2 py-1 rounded break-all">{{ jobStore.currentJobId }}</span>
          </div>
           <div class="flex items-center space-x-2">
             <strong class="text-gray-600">Status:</strong>
             <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="statusBadgeClass">
                 <!-- Use jobDetails.status directly -->
                 {{ jobDetails?.status || 'Loading...' }}
             </span>
           </div>
        </div>

        <!-- Stage & Error (if failed) -->
        <div class="text-sm">
            <strong class="text-gray-600 block mb-1">Current Stage:</strong>
            <span class="text-gray-800 font-medium">{{ formattedStage }}</span>
            <!-- Use jobDetails.error_message -->
            <div v-if="jobDetails?.status === 'failed' && jobDetails?.error_message" class="mt-3 p-4 bg-red-50 border border-red-200 text-red-800 rounded-md text-xs shadow-sm">
              <strong class="block mb-1 font-semibold">Error Details:</strong>
              <p class="whitespace-pre-wrap">{{ jobDetails.error_message }}</p> <!-- Preserve whitespace -->
            </div>
        </div>

        <!-- Progress Bar -->
        <div>
          <label class="block text-sm font-medium text-gray-600 mb-1.5">Progress:</label>
          <div class="w-full bg-gray-200 rounded-full h-3 overflow-hidden"> <!-- Slimmer progress bar -->
            <div
              class="h-3 rounded-full transition-all duration-500 ease-out"
              :class="progressColorClass"
              :style="{ width: progressPercent + '%' }"
              ></div>
          </div>
          <div class="text-right text-xs text-gray-500 mt-1">{{ progressPercent }}% Complete</div>
        </div>

        <!-- Timestamps Row -->
        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-xs text-gray-500 border-t border-gray-100 pt-5">
            <div>
                 <strong class="block text-gray-600 mb-0.5">Created:</strong>
                 <span>{{ formattedCreatedAt }}</span>
            </div>
            <div>
                 <strong class="block text-gray-600 mb-0.5">Updated:</strong>
                 <span>{{ formattedUpdatedAt }}</span>
            </div>
             <div>
                <strong class="block text-gray-600 mb-0.5">Est. Completion:</strong>
                <span>{{ formattedEstimatedCompletion }}</span>
            </div>
        </div>

        <!-- Notification Section -->
        <div v-if="canRequestNotify" class="pt-5 border-t border-gray-100">
            <label for="notificationEmail" class="block text-sm font-medium text-gray-700 mb-1.5">Get Notified Upon Completion</label>
            <div class="flex flex-col sm:flex-row sm:space-x-2">
                <input type="email"
                       class="mb-2 sm:mb-0 flex-grow block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100"
                       id="notificationEmail"
                       placeholder="Enter your email"
                       v-model="notificationEmail"
                       :disabled="isNotifyLoading" />
                <button
                    type="button"
                    @click="requestNotification"
                    :disabled="isNotifyLoading || !notificationEmail"
                    class="w-full sm:w-auto inline-flex justify-center items-center px-4 py-2 border border-gray-300 rounded-md shadow-sm text-sm font-medium text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed"
                >
                    <svg v-if="isNotifyLoading" class="animate-spin -ml-1 mr-2 h-4 w-4 text-gray-700" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                       <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                       <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                     <EnvelopeIcon v-else class="h-4 w-4 mr-2 -ml-1 text-gray-500"/>
                    {{ isNotifyLoading ? 'Sending...' : 'Notify Me' }}
                </button>
            </div>
            <!-- Notification Feedback -->
            <p v-if="notifyMessage" :class="notifyMessageIsError ? 'text-red-600' : 'text-green-600'" class="mt-2 text-xs">{{ notifyMessage }}</p>
        </div>

        <!-- Download Section -->
        <div v-if="jobDetails?.status === 'completed'" class="pt-5 border-t border-gray-100">
          <button @click="downloadResults"
                  class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-green-600 hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-green-500 disabled:opacity-50 disabled:cursor-not-allowed"
                  :disabled="isDownloadLoading">
             <!-- Spinner SVG -->
             <svg v-if="isDownloadLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
             </svg>
             <!-- Download Icon -->
             <ArrowDownTrayIcon v-else class="h-5 w-5 mr-2 -ml-1" />
            {{ isDownloadLoading ? ' Preparing Download...' : 'Download Results' }}
          </button>
          <p v-if="downloadError" class="mt-2 text-xs text-red-600 text-center">{{ downloadError }}</p>
        </div>

        <!-- Stats Section (Rendered within JobStatus when complete) -->
         <!-- Use jobDetails.id -->
         <JobStats v-if="jobDetails?.status === 'completed' && jobDetails?.id" :job-id="jobDetails.id" />

      </div>
    </div>
      <div v-else class="text-center py-10 text-gray-500">
        <!-- Message shown when no job is selected -->
        <!-- Select a job from the history or upload a new file. -->
      </div>
  </template>

  <script setup lang="ts">
  import { ref, computed, onMounted, onUnmounted, watch } from 'vue';
  import apiService from '@/services/api';
  import { useJobStore, type JobDetails } from '@/stores/job';
  import JobStats from './JobStats.vue';
  import { EnvelopeIcon, ArrowDownTrayIcon, ExclamationTriangleIcon } from '@heroicons/vue/20/solid';

  const POLLING_INTERVAL = 5000; // Poll every 5 seconds
  const jobStore = useJobStore();
  // Use jobDetails directly from the store
  const jobDetails = computed(() => jobStore.jobDetails);
  const isLoading = ref(false); // Tracks if a poll request is currently in flight
  const errorMessage = ref<string | null>(null); // Stores polling or general errors
  const pollingIntervalId = ref<number | null>(null); // Stores the ID from setInterval

  // --- Notification State ---
  const notificationEmail = ref('');
  const isNotifyLoading = ref(false);
  const notifyMessage = ref<string | null>(null);
  const notifyMessageIsError = ref(false);

  // --- Download State ---
  const isDownloadLoading = ref(false);
  const downloadError = ref<string | null>(null);

  // --- Computed Properties ---
  const formattedStage = computed(() => {
    const stage = jobDetails.value?.current_stage;
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'Completed';
    if (status === 'failed') return 'Failed';
    if (status === 'pending') return 'Pending Start';
    if (!stage) return 'Loading...';

    // Map internal stage names to user-friendly display names
    const stageNames: { [key: string]: string } = {
      'ingestion': 'Ingesting File',
      'normalization': 'Normalizing Data',
      'classification_level_1': 'Classifying (L1)',
      'classification_level_2': 'Classifying (L2)',
      'classification_level_3': 'Classifying (L3)',
      'classification_level_4': 'Classifying (L4)',
      'search_unknown_vendors': 'Researching Vendors',
      'result_generation': 'Generating Results',
    };
    return stageNames[stage] || stage; // Fallback to raw stage name if not mapped
  });

  const progressPercent = computed(() => {
    const status = jobDetails.value?.status;
    const progress = jobDetails.value?.progress ?? 0;
    if (status === 'completed') return 100;
    if (status === 'pending') return 0; // Show 0% for pending
    // Ensure progress is between 0 and 100
    return Math.max(0, Math.min(100, Math.round(progress * 100)));
  });

  const statusBadgeClass = computed(() => {
      switch (jobDetails.value?.status) {
          case 'completed': return 'bg-green-100 text-green-800';
          case 'failed': return 'bg-red-100 text-red-800';
          case 'processing': return 'bg-blue-100 text-blue-800 animate-pulse'; // Add pulse for processing
          default: return 'bg-gray-100 text-gray-800'; // Pending or loading
      }
  });

  const progressColorClass = computed(() => {
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'bg-green-500';
    if (status === 'failed') return 'bg-red-500';
    // Use primary color and pulse animation for processing/pending
    return 'bg-primary animate-pulse';
  });

  const formatDateTime = (isoString: string | null | undefined): string => {
      if (!isoString) return 'N/A';
      try {
          // Use user's locale and common options
          return new Date(isoString).toLocaleString(undefined, {
              year: 'numeric', month: 'short', day: 'numeric',
              hour: 'numeric', minute: '2-digit', hour12: true
          });
      } catch {
          return 'Invalid Date';
      }
  };

  const formattedCreatedAt = computed(() => formatDateTime(jobDetails.value?.created_at));
  const formattedUpdatedAt = computed(() => formatDateTime(jobDetails.value?.updated_at));

  const formattedEstimatedCompletion = computed(() => {
      const status = jobDetails.value?.status;
      if (status === 'completed' && jobDetails.value?.completed_at) {
          return formatDateTime(jobDetails.value.completed_at);
      }
      // Assuming backend provides 'estimated_completion' during processing
      // --- CHECK FIELD NAME ---
      // Check if the field is 'estimated_completion' or something else in JobDetails
      const estCompletion = jobDetails.value?.estimated_completion; // Use the actual field name
      if (status === 'processing' && estCompletion) {
          return `${formatDateTime(estCompletion)} (est.)`;
      }
      // --- END CHECK ---
      if (status === 'processing') return 'Calculating...';
      if (status === 'failed') return 'N/A';
      if (status === 'pending') return 'Pending Start';
      return 'N/A';
  });

  const canRequestNotify = computed(() => {
      const status = jobDetails.value?.status;
      return status === 'pending' || status === 'processing';
  });

  // --- Methods ---
  const pollJobStatus = async (jobId: string | null | undefined) => {
     // Check if we should still be polling this job
     if (!jobId || jobStore.currentJobId !== jobId) {
         console.log(`JobStatus: [pollJobStatus] Stopping polling because jobId (${jobId}) doesn't match store (${jobStore.currentJobId}) or is null.`); // LOGGING
         stopPolling();
         return;
     }

     // Avoid concurrent polls
     if (isLoading.value) {
         console.log(`JobStatus: [pollJobStatus] Skipping poll for ${jobId} as another poll is already in progress.`); // LOGGING
         return;
     }

     isLoading.value = true;
     console.log(`JobStatus: [pollJobStatus] Polling status for job ${jobId}...`); // LOGGING
    try {
        const data = await apiService.getJobStatus(jobId);
        // IMPORTANT: Check if the job ID is still the current one *after* the API call returns
        if (jobStore.currentJobId === jobId) {
            console.log(`JobStatus: [pollJobStatus] Received status data for ${jobId}: Status=${data.status}, Progress=${data.progress}, Stage=${data.current_stage}`); // LOGGING
            jobStore.updateJobDetails(data); // Update the store
            errorMessage.value = null; // Clear previous errors on successful poll

            // Stop polling if job is completed or failed
            if (data.status === 'completed' || data.status === 'failed') {
                console.log(`JobStatus: [pollJobStatus] Job ${jobId} reached terminal state (${data.status}). Stopping polling.`); // LOGGING
                stopPolling();
            }
        } else {
             console.log(`JobStatus: [pollJobStatus] Job ID changed from ${jobId} to ${jobStore.currentJobId} during API call. Ignoring stale data.`); // LOGGING
             // Don't update the store with stale data
        }
    } catch (error: any) {
        console.error(`JobStatus: [pollJobStatus] Error polling status for ${jobId}:`, error); // LOGGING
        // Only set error if the failed poll was for the *current* job ID
        if (jobStore.currentJobId === jobId) {
            errorMessage.value = `Polling Error: ${error.message || 'Failed to fetch status.'}`;
        }
        // Consider retrying after a longer interval before stopping completely
        stopPolling(); // Stop polling on error for now
    } finally {
        // Only set isLoading to false if the poll was for the current job
        if (jobStore.currentJobId === jobId) {
            isLoading.value = false;
        }
    }
  };

  const startPolling = (jobId: string | null | undefined) => {
    if (!jobId) {
        console.log("JobStatus: [startPolling] Cannot start polling, no jobId provided."); // LOGGING
        return;
    }
    stopPolling(); // Ensure any existing polling is stopped first
    console.log(`JobStatus: [startPolling] Starting polling for job ${jobId}.`); // LOGGING
    pollJobStatus(jobId); // Poll immediately

    pollingIntervalId.value = window.setInterval(() => {
        console.log(`JobStatus: [setInterval] Checking poll condition for ${jobId}. Current store ID: ${jobStore.currentJobId}, Status: ${jobStore.jobDetails?.status}`); // LOGGING
        // Check condition inside interval as well
        if (jobStore.currentJobId === jobId && jobStore.jobDetails?.status !== 'completed' && jobStore.jobDetails?.status !== 'failed') {
            pollJobStatus(jobId);
        } else {
            console.log(`JobStatus: [setInterval] Condition not met, stopping polling.`); // LOGGING
            // Stop if job ID changed or job finished between polls
            stopPolling();
        }
    }, POLLING_INTERVAL);
  };

  const stopPolling = () => {
    if (pollingIntervalId.value !== null) {
        console.log(`JobStatus: [stopPolling] Stopping polling interval ID ${pollingIntervalId.value}.`); // LOGGING
        clearInterval(pollingIntervalId.value);
        pollingIntervalId.value = null;
    } else {
        // console.log(`JobStatus: [stopPolling] No active polling interval to stop.`); // LOGGING (Optional)
    }
  };

  const requestNotification = async () => {
     // Use jobDetails.id
     const currentId = jobDetails.value?.id;
     if (!currentId || !notificationEmail.value) return;
     isNotifyLoading.value = true;
     notifyMessage.value = null;
     notifyMessageIsError.value = false;
     console.log(`JobStatus: Requesting notification for ${currentId} to ${notificationEmail.value}`); // LOGGING
    try {
        const response = await apiService.requestNotification(currentId, notificationEmail.value);
        console.log(`JobStatus: Notification request successful:`, response); // LOGGING
        notifyMessage.value = response.message || 'Notification request sent!';
        notificationEmail.value = ''; // Clear input on success
    } catch (error: any) {
        console.error(`JobStatus: Notification request failed:`, error); // LOGGING
        notifyMessage.value = `Error: ${error.message || 'Failed to send request.'}`;
        notifyMessageIsError.value = true;
    } finally {
        isNotifyLoading.value = false;
        // Clear message after a delay
        setTimeout(() => { notifyMessage.value = null; }, 5000);
    }
  };

  const downloadResults = async () => {
     // Use jobDetails.id
     const currentId = jobDetails.value?.id;
     if (!currentId) return;
     isDownloadLoading.value = true;
     downloadError.value = null;
     console.log(`JobStatus: Attempting download for ${currentId}`); // LOGGING
    try {
        const { blob, filename } = await apiService.downloadResults(currentId);
        console.log(`JobStatus: Download blob received, filename: ${filename}`); // LOGGING
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
        console.log(`JobStatus: Download triggered for ${filename}`); // LOGGING
    } catch (error: any) {
        console.error(`JobStatus: Download failed:`, error); // LOGGING
        downloadError.value = `Download failed: ${error.message || 'Could not download results.'}`;
    } finally {
        isDownloadLoading.value = false;
    }
  };

  // --- Lifecycle Hooks ---
  onMounted(() => {
      console.log(`JobStatus: Mounted. Current job ID from store: ${jobStore.currentJobId}`); // LOGGING
      if (jobStore.currentJobId) {
          errorMessage.value = null;
          // Fetch initial details if not already present or if status is unknown/stale
          // Use jobDetails.id for comparison
          if (!jobDetails.value || jobDetails.value.id !== jobStore.currentJobId || (jobDetails.value.status !== 'completed' && jobDetails.value.status !== 'failed')) {
              console.log(`JobStatus: Fetching initial details or starting polling for ${jobStore.currentJobId}`); // LOGGING
              startPolling(jobStore.currentJobId);
          } else {
               console.log(`JobStatus: Job ${jobStore.currentJobId} already in terminal state (${jobDetails.value.status}), not polling.`); // LOGGING
          }
      }
  });

  onUnmounted(() => {
      console.log("JobStatus: Unmounted, stopping polling."); // LOGGING
      stopPolling();
  });

  // --- Watchers ---
  // Watch for changes in the store's currentJobId
  watch(() => jobStore.currentJobId, (newJobId: string | null | undefined) => {
      console.log(`JobStatus: Watched currentJobId changed to: ${newJobId}`); // LOGGING
      if (newJobId) {
          // Reset component state when job ID changes
          errorMessage.value = null;
          downloadError.value = null;
          notifyMessage.value = null;
          notificationEmail.value = '';
          isDownloadLoading.value = false;
          isNotifyLoading.value = false;

          // Fetch details or start polling if needed for the new job
          // Use jobDetails.id for comparison
          if (!jobStore.jobDetails || jobStore.jobDetails.id !== newJobId || (jobStore.jobDetails.status !== 'completed' && jobStore.jobDetails.status !== 'failed')) {
               console.log(`JobStatus: Starting polling due to job ID change to ${newJobId}`); // LOGGING
               startPolling(newJobId);
          } else {
               // If the new job is already completed/failed, don't poll
               console.log(`JobStatus: Job ${newJobId} already in terminal state (${jobStore.jobDetails.status}), not polling.`); // LOGGING
               stopPolling(); // Ensure polling is stopped
          }
      } else {
          // Job ID was cleared
          console.log("JobStatus: Job ID cleared, stopping polling."); // LOGGING
          stopPolling();
      }
  });

  // Watch for the job status changing to a terminal state
  watch(() => jobStore.jobDetails?.status, (newStatus: JobDetails['status'] | undefined) => {
      console.log(`JobStatus: Watched job status changed to: ${newStatus}`); // LOGGING
      if (newStatus === 'completed' || newStatus === 'failed') {
          console.log(`JobStatus: Job reached terminal state (${newStatus}), stopping polling.`); // LOGGING
          stopPolling();
      }
  });

  </script>
</file>

<file path='frontend/vue_frontend/src/components/UploadForm.vue'>

<template>
  <div class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
    <div class="bg-primary text-white p-4 sm:p-5">
      <h4 class="text-xl font-semibold mb-0">Upload Vendor File</h4>
    </div>
    <div class="p-6 sm:p-8">
      <!-- Success Alert -->
      <div v-if="successMessage" class="mb-5 p-3 bg-green-100 border border-green-300 text-green-800 rounded-md text-sm flex items-center">
          <CheckCircleIcon class="h-5 w-5 mr-2 text-green-600 flex-shrink-0"/>
          <span>{{ successMessage }}</span>
      </div>
       <!-- Error Alert -->
      <div v-if="errorMessage" class="mb-5 p-3 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
           <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
          <span>{{ errorMessage }}</span>
      </div>

      <form @submit.prevent="handleUpload" enctype="multipart/form-data">
        <div class="mb-5">
          <label for="companyName" class="block text-sm font-medium text-gray-700 mb-1.5">
              Company Name <span class="text-red-500">*</span>
          </label>
          <input
            type="text"
            class="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100 disabled:cursor-not-allowed"
            id="companyName"
            v-model="companyName"
            required
            :disabled="isLoading"
            placeholder="e.g., Your Company Inc."
          />
        </div>

        <!-- ADDED: Target Level Selection -->
        <div class="mb-5">
          <label for="targetLevel" class="block text-sm font-medium text-gray-700 mb-1.5">
              Target Classification Level <span class="text-red-500">*</span>
          </label>
          <select
            id="targetLevel"
            v-model.number="selectedLevel"
            required
            :disabled="isLoading"
            class="block w-full px-3 py-2 border border-gray-300 bg-white rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100 disabled:cursor-not-allowed"
          >
            <option value="1">Level 1 (Sector)</option>
            <option value="2">Level 2 (Subsector)</option>
            <option value="3">Level 3 (Industry Group)</option>
            <option value="4">Level 4 (NAICS Industry)</option>
            <option value="5">Level 5 (National Industry)</option>
          </select>
          <p class="mt-1 text-xs text-gray-500">Select the maximum NAICS level you want the classification to reach.</p>
        </div>
        <!-- END ADDED -->

        <div class="mb-6">
          <label for="vendorFile" class="block text-sm font-medium text-gray-700 mb-1.5">
              Vendor Excel File <span class="text-red-500">*</span>
          </label>
          <input
            type="file"
            class="block w-full text-sm text-gray-500 border border-gray-300 rounded-md cursor-pointer bg-gray-50 focus:outline-none focus:ring-primary focus:border-primary file:mr-4 file:py-2 file:px-4 file:rounded-l-md file:border-0 file:text-sm file:font-semibold file:bg-gray-100 file:text-gray-700 hover:file:bg-gray-200 disabled:opacity-60 disabled:cursor-not-allowed"
            id="vendorFile"
            ref="fileInputRef"
            @change="handleFileChange"
            accept=".xlsx,.xls"
            required
            :disabled="isLoading"
          />
          <p class="mt-2 text-xs text-gray-500">
              Requires '.xlsx' or '.xls'. Must contain 'vendor_name' column.
              <br/>Optional context columns enhance accuracy (address, website, example, etc.).
          </p>
        </div>
        <button type="submit" class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-primary hover:bg-primary-hover focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed" :disabled="isLoading || !selectedFile">
           <svg v-if="isLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
              <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
              <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
           </svg>
           <ArrowUpTrayIcon v-else class="h-5 w-5 mr-2 -ml-1" />
          {{ isLoading ? ' Uploading & Processing...' : 'Upload and Process' }}
        </button>
      </form>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref } from 'vue';
import apiService from '@/services/api';
import { useJobStore } from '@/stores/job';
import { ArrowUpTrayIcon, CheckCircleIcon, ExclamationTriangleIcon } from '@heroicons/vue/20/solid'; // Using solid icons

const jobStore = useJobStore();
const companyName = ref('');
const selectedFile = ref<File | null>(null);
const fileInputRef = ref<HTMLInputElement | null>(null);
const isLoading = ref(false);
const successMessage = ref<string | null>(null);
const errorMessage = ref<string | null>(null);
// --- ADDED: State for selected level ---
const selectedLevel = ref<number>(5); // Default to Level 5
// --- END ADDED ---

const emit = defineEmits(['upload-successful']);

const handleFileChange = (event: Event) => {
  const target = event.target as HTMLInputElement;
  if (target.files && target.files.length > 0) {
    selectedFile.value = target.files[0];
    errorMessage.value = null;
    successMessage.value = null;
  } else {
    selectedFile.value = null;
  }
};

const handleUpload = async () => {
  if (!selectedFile.value || !companyName.value) {
    errorMessage.value = 'Please provide a company name and select a file.';
    successMessage.value = null;
    return;
  }
  // --- ADDED: Validate selected level ---
  if (selectedLevel.value < 1 || selectedLevel.value > 5) {
    errorMessage.value = 'Please select a valid target level (1-5).';
    successMessage.value = null;
    return;
  }
  // --- END ADDED ---

  isLoading.value = true;
  successMessage.value = null;
  errorMessage.value = null;
  jobStore.clearJob();
  const formData = new FormData();
  formData.append('company_name', companyName.value);
  formData.append('file', selectedFile.value);
  // --- ADDED: Append selected level to form data ---
  formData.append('target_level', selectedLevel.value.toString());
  // --- END ADDED ---

  try {
    const response = await apiService.uploadFile(formData); // apiService.uploadFile now handles FormData directly
    successMessage.value = `Upload successful! Job ${response.job_id} started (Target Level: ${selectedLevel.value}). Monitoring status below...`;
    emit('upload-successful', response.job_id);
    // Reset form fields
    companyName.value = '';
    selectedFile.value = null;
    selectedLevel.value = 5; // Reset level to default
    if (fileInputRef.value) {
        fileInputRef.value.value = '';
    }
  } catch (error: any) {
    errorMessage.value = error.message || 'An unexpected error occurred during upload.';
    successMessage.value = null;
  } finally {
    isLoading.value = false;
  }
};
</script>

<style scoped>
/* Style the file input button more effectively */
input[type="file"]::file-selector-button {
    /* Tailwind handles most, but you can add custom tweaks */
    cursor: pointer;
}
</style>
</file>

<file path='frontend/vue_frontend/src/services/api.ts'>
// <file path='frontend/vue_frontend/src/services/api.ts'>
import axios, {
    type AxiosInstance,
    type InternalAxiosRequestConfig,
    type AxiosError // Import AxiosError type
} from 'axios';
import { useAuthStore } from '@/stores/auth'; // Adjust path as needed
import type { JobDetails } from '@/stores/job'; // Adjust path as needed

// --- Define API Response Interfaces ---

// Matches backend schemas/user.py -> UserResponse
export interface UserResponse {
    email: string;
    full_name: string | null;
    is_active: boolean | null;
    is_superuser: boolean | null;
    username: string;
    id: string; // UUID as string
    created_at: string; // ISO Date string
    updated_at: string; // ISO Date string
}

// Matches backend schemas/user.py -> UserCreate (for request body)
export interface UserCreateData {
    email: string;
    full_name?: string | null;
    is_active?: boolean | null;
    is_superuser?: boolean | null;
    username: string;
    password?: string; // Password required on create
}

// Matches backend schemas/user.py -> UserUpdate (for request body)
export interface UserUpdateData {
    email?: string | null;
    full_name?: string | null;
    password?: string | null; // Optional password update
    is_active?: boolean | null;
    is_superuser?: boolean | null;
}


// Matches backend response for /token (modified to include user object)
interface AuthResponse {
    access_token: string;
    token_type: string;
    user: UserResponse; // Include the user details
}

// Matches backend response for /api/v1/upload
interface UploadResponse {
    job_id: string;
    status: string;
    message: string;
    created_at: string;
    progress: number;
    current_stage: string;
}

// Matches backend response for /api/v1/jobs/{job_id}/notify
interface NotifyResponse {
    success: boolean;
    message: string;
}

// Matches backend response for /api/v1/jobs/ (list endpoint)
// Should align with app/schemas/job.py -> JobResponse
export interface JobResponse {
    id: string;
    company_name: string;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string;
    created_at: string; // ISO Date string
    updated_at?: string | null;
    completed_at?: string | null;
    output_file_name?: string | null;
    input_file_name: string;
    created_by: string;
    error_message?: string | null;
    target_level: number; // Ensure target_level is included here
}

// --- UPDATED JobStatsData Interface ---
// Matches backend models/classification.py -> ProcessingStats and console log
export interface JobStatsData {
    job_id: string;
    company_name: string;
    start_time: string | null; // Assuming ISO string
    end_time: string | null; // Assuming ISO string
    processing_duration_seconds: number | null; // Renamed from processing_time
    total_vendors: number | null; // Added
    unique_vendors: number | null; // Added (was present in console)
    successfully_classified_l4: number | null; // Keep for reference
    successfully_classified_l5: number | null; // Keep L5 count
    classification_not_possible_initial: number | null; // Added
    invalid_category_errors: number | null; // Added (was present in console)
    search_attempts: number | null; // Added
    search_successful_classifications_l1: number | null; // Added
    search_successful_classifications_l5: number | null; // Renamed from search_assisted_l5
    api_usage: { // Nested structure
        openrouter_calls: number | null;
        openrouter_prompt_tokens: number | null;
        openrouter_completion_tokens: number | null;
        openrouter_total_tokens: number | null;
        tavily_search_calls: number | null;
        cost_estimate_usd: number | null;
    } | null; // Allow api_usage itself to be null if not populated
}
// --- END UPDATED JobStatsData Interface ---


// Structure for download result helper
interface DownloadResult {
    blob: Blob;
    filename: string;
}

// Parameters for the job history list endpoint
interface GetJobsParams {
    status?: string;
    start_date?: string; // ISO string format
    end_date?: string; // ISO string format
    skip?: number;
    limit?: number;
}

// --- Axios Instance Setup ---

const axiosInstance: AxiosInstance = axios.create({
    baseURL: '/api/v1', // Assumes Vite dev server proxies /api/v1 to your backend
    timeout: 60000, // 60 seconds timeout
    headers: {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
    },
});

// --- Request Interceptor (Add Auth Token) ---
axiosInstance.interceptors.request.use(
    (config: InternalAxiosRequestConfig) => {
        const authStore = useAuthStore();
        const token = authStore.getToken();
        // No need for noAuthUrls here as login uses base axios
        if (token && config.url) {
            // LOGGING: Log token presence and target URL
            console.log(`[api.ts Request Interceptor] Adding token for URL: ${config.url}`);
            config.headers.Authorization = `Bearer ${token}`;
        } else {
            console.log(`[api.ts Request Interceptor] No token found or no URL for config: ${config.url}`);
        }
        return config;
    },
    (error: AxiosError) => {
        console.error('[api.ts Request Interceptor] Error:', error);
        return Promise.reject(error);
    }
);

// --- Response Interceptor (Handle Errors) ---
axiosInstance.interceptors.response.use(
    (response) => {
        // LOGGING: Log successful response status and URL
        console.log(`[api.ts Response Interceptor] Success for URL: ${response.config.url} | Status: ${response.status}`);
        return response;
    },
    (error: AxiosError) => {
        console.error('[api.ts Response Interceptor] Error:', error.config?.url, error.response?.status, error.message);
        const authStore = useAuthStore();

        if (error.response) {
            const { status, data } = error.response;

            // Handle 401 Unauthorized (except for login attempts)
            // Login uses base axios, so this interceptor won't catch its 401
            if (status === 401) {
                console.warn('[api.ts Response Interceptor] Received 401 Unauthorized. Logging out.');
                authStore.logout(); // Trigger logout action
                // No reload here, let the component handle redirection or UI change
                return Promise.reject(new Error('Session expired. Please log in again.'));
            }

            // Extract detailed error message from response data
            let detailMessage = 'An error occurred.';
            const responseData = data as any;

            if (responseData?.detail) {
                    if (Array.isArray(responseData.detail)) {
                        detailMessage = `Validation Error: ${responseData.detail.map((err: any) => `${err.loc?.join('.') ?? 'field'}: ${err.msg}`).join('; ')}`;
                    } else if (typeof responseData.detail === 'string') {
                        detailMessage = responseData.detail;
                    } else {
                        try { detailMessage = JSON.stringify(responseData.detail); } catch { /* ignore */ }
                    }
            } else if (typeof data === 'string' && data.length > 0 && data.length < 300) {
                detailMessage = data;
            } else if (error.message) {
                detailMessage = error.message;
            }

            const errorMessage = `Error ${status}: ${detailMessage}`;
            console.error(`[api.ts Response Interceptor] Rejecting with error: ${errorMessage}`); // LOGGING
            return Promise.reject(new Error(errorMessage));

        } else if (error.request) {
            console.error('[api.ts Response Interceptor] Network error or no response received:', error.request);
            return Promise.reject(new Error('Network error or server did not respond. Please check connection.'));
        } else {
            console.error('[api.ts Response Interceptor] Axios setup error:', error.message);
            return Promise.reject(new Error(`Request setup error: ${error.message}`));
        }
    }
);


// --- API Service Object ---

const apiService = {
    /**
        * Logs in a user. Uses base axios for specific headers.
        */
    async login(usernameInput: string, passwordInput: string): Promise<AuthResponse> {
        const params = new URLSearchParams();
        params.append('username', usernameInput);
        params.append('password', passwordInput);
        console.log(`[api.ts login] Attempting login for user: ${usernameInput}`); // LOGGING
        // Use base axios to avoid default JSON headers and ensure correct Content-Type
        const response = await axios.post<AuthResponse>('/token', params, {
            headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
        });
        console.log(`[api.ts login] Login successful for user: ${usernameInput}`); // LOGGING
        return response.data;
    },

    /**
        * Uploads the vendor file.
        */
    async uploadFile(formData: FormData): Promise<UploadResponse> {
        console.log('[api.ts uploadFile] Attempting file upload...'); // LOGGING
        // This uses axiosInstance, so /api/v1 prefix is added automatically
        const response = await axiosInstance.post<UploadResponse>('/upload', formData, {
                headers: { 'Content-Type': undefined } // Let browser set Content-Type for FormData
        });
        console.log(`[api.ts uploadFile] Upload successful, job ID: ${response.data.job_id}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches the status and details of a specific job.
        */
    async getJobStatus(jobId: string): Promise<JobDetails> {
        console.log(`[api.ts getJobStatus] Fetching status for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobDetails>(`/jobs/${jobId}`);
        console.log(`[api.ts getJobStatus] Received status for job ${jobId}:`, response.data.status); // LOGGING
        return response.data;
    },

    /**
        * Fetches statistics for a specific job.
        */
    async getJobStats(jobId: string): Promise<JobStatsData> { // Use the updated interface here
        console.log(`[api.ts getJobStats] Fetching stats for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobStatsData>(`/jobs/${jobId}/stats`);
        // LOGGING: Log the received stats structure
        console.log(`[api.ts getJobStats] Received stats for job ${jobId}:`, JSON.parse(JSON.stringify(response.data)));
        return response.data;
    },

    /**
        * Requests email notification for a job completion.
        */
    async requestNotification(jobId: string, email: string): Promise<NotifyResponse> {
        console.log(`[api.ts requestNotification] Requesting notification for job ${jobId} to email ${email}`); // LOGGING
        const response = await axiosInstance.post<NotifyResponse>(`/jobs/${jobId}/notify`, { email });
        console.log(`[api.ts requestNotification] Notification request response:`, response.data.success); // LOGGING
        return response.data;
    },

    /**
        * Downloads the results file for a completed job.
        */
    async downloadResults(jobId: string): Promise<DownloadResult> {
        console.log(`[api.ts downloadResults] Requesting download for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get(`/jobs/${jobId}/download`, {
            responseType: 'blob',
        });
        const disposition = response.headers['content-disposition'];
        let filename = `results_${jobId}.xlsx`;
        if (disposition?.includes('attachment')) {
            const filenameMatch = disposition.match(/filename\*?=(?:(?:"((?:[^"\\]|\\.)*)")|(?:([^;\n]*)))/i);
            if (filenameMatch?.[1]) { filename = filenameMatch[1].replace(/\\"/g, '"'); }
            else if (filenameMatch?.[2]) {
                    const utf8Match = filenameMatch[2].match(/^UTF-8''(.*)/i);
                    if (utf8Match?.[1]) { try { filename = decodeURIComponent(utf8Match[1]); } catch (e) { filename = utf8Match[1]; } }
                    else { filename = filenameMatch[2]; }
            }
        }
        console.log(`[api.ts downloadResults] Determined download filename: ${filename}`); // LOGGING
        return { blob: response.data as Blob, filename };
    },

    /**
        * Fetches a list of jobs for the current user, with optional filtering/pagination.
        */
    async getJobs(params: GetJobsParams = {}): Promise<JobResponse[]> {
        const cleanedParams = Object.fromEntries(
            Object.entries(params).filter(([, value]) => value !== undefined && value !== null)
        );
        console.log('[api.ts getJobs] Fetching job list with params:', cleanedParams); // LOGGING
        const response = await axiosInstance.get<JobResponse[]>('/jobs/', { params: cleanedParams });
        console.log(`[api.ts getJobs] Received ${response.data.length} jobs.`); // LOGGING
        return response.data;
    },

    // --- User Management API Methods ---

    /**
        * Fetches the current logged-in user's details.
        */
    async getCurrentUser(): Promise<UserResponse> {
        console.log('[api.ts getCurrentUser] Fetching current user details...'); // LOGGING
        const response = await axiosInstance.get<UserResponse>('/users/me');
        console.log(`[api.ts getCurrentUser] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches a list of all users (admin only).
        */
    async getUsers(skip: number = 0, limit: number = 100): Promise<UserResponse[]> {
        console.log(`[api.ts getUsers] Fetching user list (skip: ${skip}, limit: ${limit})...`); // LOGGING
        const response = await axiosInstance.get<UserResponse[]>('/users/', { params: { skip, limit } });
         console.log(`[api.ts getUsers] Received ${response.data.length} users.`); // LOGGING
        return response.data;
    },

        /**
        * Fetches a specific user by ID (admin or self).
        */
        async getUserById(userId: string): Promise<UserResponse> {
        console.log(`[api.ts getUserById] Fetching user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.get<UserResponse>(`/users/${userId}`);
        console.log(`[api.ts getUserById] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Creates a new user (admin only).
        */
    async createUser(userData: UserCreateData): Promise<UserResponse> {
        console.log(`[api.ts createUser] Attempting to create user: ${userData.username}`); // LOGGING
        const response = await axiosInstance.post<UserResponse>('/users/', userData);
        console.log(`[api.ts createUser] User created successfully: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Updates a user (admin or self).
        */
    async updateUser(userId: string, userData: UserUpdateData): Promise<UserResponse> {
        console.log(`[api.ts updateUser] Attempting to update user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.put<UserResponse>(`/users/${userId}`, userData);
        console.log(`[api.ts updateUser] User updated successfully: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Deletes a user (admin only).
        */
    async deleteUser(userId: string): Promise<{ message: string }> {
        console.log(`[api.ts deleteUser] Attempting to delete user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.delete<{ message: string }>(`/users/${userId}`);
        console.log(`[api.ts deleteUser] User delete response: ${response.data.message}`); // LOGGING
        return response.data;
    },
    // --- END User Management API Methods ---
};

export default apiService;
</file>

<file path='frontend/vue_frontend/src/stores/job.ts'>
// <file path='frontend/vue_frontend/src/stores/job.ts'>
import { defineStore } from 'pinia';
import { ref } from 'vue';
import apiService, { type JobResponse } from '@/services/api'; // Import JobResponse type

// Define the structure of the job details object based on your API response
export interface JobDetails {
    id: string; // Changed from job_id to match JobResponse schema
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string; // Consider using specific stage literals if known
    created_at: string | null;
    updated_at: string | null;
    completed_at?: string | null; // Optional completion time
    estimated_completion?: string | null; // Added optional field (backend doesn't provide this explicitly yet)
    error_message: string | null;
    target_level: number; // ADDED: Ensure target_level is part of the details
    // Add other fields returned by /api/v1/jobs/{job_id} if needed
    // Match JobResponse fields where applicable
    company_name?: string;
    input_file_name?: string;
    output_file_name?: string | null;
    created_by?: string;
}

export const useJobStore = defineStore('job', () => {
    // --- State ---
    const currentJobId = ref<string | null>(null);
    const jobDetails = ref<JobDetails | null>(null);
    const isLoading = ref(false); // For tracking polling/loading state for CURRENT job
    const error = ref<string | null>(null); // For storing errors related to fetching CURRENT job status

    // --- ADDED: Job History State ---
    const jobHistory = ref<JobResponse[]>([]);
    const historyLoading = ref(false);
    const historyError = ref<string | null>(null);
    // --- END ADDED ---

    // --- Actions ---
    function setCurrentJobId(jobId: string | null): void {
        console.log(`JobStore: Setting currentJobId from '${currentJobId.value}' to '${jobId}'`); // LOGGING
        if (currentJobId.value !== jobId) {
            currentJobId.value = jobId;
            // Clear details when ID changes (to null or a new ID) to force refresh
            jobDetails.value = null;
            console.log(`JobStore: Cleared jobDetails due to ID change.`); // LOGGING
            error.value = null; // Clear errors
            isLoading.value = false; // Reset loading state

            // Update URL to reflect the current job ID or clear it
            try {
                 const url = new URL(window.location.href);
                 if (jobId) {
                     url.searchParams.set('job_id', jobId);
                     console.log(`JobStore: Updated URL searchParam 'job_id' to ${jobId}`); // LOGGING
                 } else {
                     url.searchParams.delete('job_id');
                     console.log(`JobStore: Removed 'job_id' from URL searchParams.`); // LOGGING
                 }
                 // Use replaceState to avoid polluting history
                 window.history.replaceState({}, '', url.toString());
            } catch (e) {
                 console.error("JobStore: Failed to update URL:", e);
            }
        }
         // If the same job ID is set again, force a refresh of details
         else if (jobId !== null) {
             console.log(`JobStore: Re-setting same job ID ${jobId}, clearing details to force refresh.`); // LOGGING
             jobDetails.value = null;
             error.value = null;
             isLoading.value = false;
         }
    }

    function updateJobDetails(details: JobDetails): void {
        // Only update if the details are for the currently tracked job
        if (details && details.id === currentJobId.value) { // Match 'id' field from JobResponse/JobDetails
            // LOGGING: Include target_level in log
            console.log(`JobStore: Updating jobDetails for ${currentJobId.value} with status ${details.status}, progress ${details.progress}, target_level ${details.target_level}`);
            jobDetails.value = { ...details }; // Create new object for reactivity
            error.value = null; // Clear error on successful update
        } else if (details) {
            console.warn(`JobStore: Received details for job ${details.id}, but currently tracking ${currentJobId.value}. Ignoring update.`); // LOGGING
        } else {
            console.warn(`JobStore: updateJobDetails called with invalid details object.`); // LOGGING
        }
    }

    function setLoading(loading: boolean): void {
        isLoading.value = loading;
    }

    function setError(errorMessage: string | null): void {
        error.value = errorMessage;
    }

    function clearJob(): void {
        console.log('JobStore: Clearing job state.'); // LOGGING
        setCurrentJobId(null); // This also clears details, error, loading and URL param
        // --- ADDED: Clear history too on full clear? Optional. ---
        // jobHistory.value = [];
        // historyLoading.value = false;
        // historyError.value = null;
        // --- END ADDED ---
    }

    // --- ADDED: Job History Actions ---
    async function fetchJobHistory(params = {}): Promise<void> {
        console.log('JobStore: Fetching job history with params:', params); // LOGGING
        historyLoading.value = true;
        historyError.value = null;
        try {
            const jobs = await apiService.getJobs(params);
            jobHistory.value = jobs;
            console.log(`JobStore: Fetched ${jobs.length} jobs.`); // LOGGING
        } catch (err: any) {
            console.error('JobStore: Failed to fetch job history:', err); // LOGGING
            historyError.value = err.message || 'Failed to load job history.';
            jobHistory.value = []; // Clear history on error
        } finally {
            historyLoading.value = false;
        }
    }
    // --- END ADDED ---


    return {
        currentJobId,
        jobDetails,
        isLoading,
        error,
        // History state & actions
        jobHistory,
        historyLoading,
        historyError,
        fetchJobHistory,
        // Existing actions
        setCurrentJobId,
        updateJobDetails,
        setLoading,
        setError,
        clearJob,
    };
});
</file>

</Concatenated Source Code>