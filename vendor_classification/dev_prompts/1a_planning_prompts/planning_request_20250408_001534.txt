--- Goal Description ---

when we reseearch a vendor via Tavily, we need to overwrite any level that might have been been assigned already. We need consistency.

--- Instructions for LLM ---

Based on the goal described above and the complete project source code provided below, please identify the files that would likely need to be:
a) Modified to implement the goal.
b) Provided as essential context for understanding the relevant parts of the codebase.

Please list the relevant file paths **relative to the project root**.

**Output Format:** Provide the list as a single line of comma-separated file paths. Use forward slashes `/` as path separators.

Example Output:
src/core/feature.py, src/utils/helpers.py, tests/test_feature.py, config/settings.yaml

--- End Instructions ---

<Project File Tree>
- Dockerfile.web
- Dockerfile.worker
- app/api/__init__.py
- app/api/auth.py
- app/api/jobs.py
- app/api/main.py
- app/api/users.py
- app/core/config.py
- app/core/database.py
- app/core/initialize_db.py
- app/core/log_context.py
- app/core/log_formatters.py
- app/core/log_handlers.py
- app/core/logging_config.py
- app/middleware/__init__.py
- app/middleware/db_logging_middleware.py
- app/middleware/logging_middleware.py
- app/models/classification.py
- app/models/job.py
- app/models/taxonomy.py
- app/models/user.py
- app/schemas/job.py
- app/schemas/user.py
- app/services/file_service.py
- app/services/llm_service.py
- app/services/search_service.py
- app/services/user_service.py
- app/tasks/celery_app.py
- app/tasks/classification_logic.py
- app/tasks/classification_prompts.py
- app/tasks/classification_tasks.py
- app/utils/log_utils.py
- app/utils/taxonomy_loader.py
- app/utils/text_processing.py
- docker-compose.yml
- docs/1_response_solution design.md
- frontend/vue_frontend/README.md
- frontend/vue_frontend/env.d.ts
- frontend/vue_frontend/index.html
- frontend/vue_frontend/postcss.config.js
- frontend/vue_frontend/src/App.vue
- frontend/vue_frontend/src/assets/base.css
- frontend/vue_frontend/src/assets/main.css
- frontend/vue_frontend/src/assets/styles.css
- frontend/vue_frontend/src/components/AppContent.vue
- frontend/vue_frontend/src/components/Footer.vue
- frontend/vue_frontend/src/components/JobHistory.vue
- frontend/vue_frontend/src/components/JobStats.vue
- frontend/vue_frontend/src/components/JobStatus.vue
- frontend/vue_frontend/src/components/LandingPage.vue
- frontend/vue_frontend/src/components/Login.vue
- frontend/vue_frontend/src/components/Navbar.vue
- frontend/vue_frontend/src/components/UploadForm.vue
- frontend/vue_frontend/src/components/UserFormModal.vue
- frontend/vue_frontend/src/components/UserManagement.vue
- frontend/vue_frontend/src/components/icons/IconCommunity.vue
- frontend/vue_frontend/src/components/icons/IconDocumentation.vue
- frontend/vue_frontend/src/components/icons/IconEcosystem.vue
- frontend/vue_frontend/src/components/icons/IconSupport.vue
- frontend/vue_frontend/src/components/icons/IconTooling.vue
- frontend/vue_frontend/src/main.ts
- frontend/vue_frontend/src/services/api.ts
- frontend/vue_frontend/src/stores/auth.ts
- frontend/vue_frontend/src/stores/counter.ts
- frontend/vue_frontend/src/stores/job.ts
- frontend/vue_frontend/src/stores/view.ts
- frontend/vue_frontend/tailwind.config.js
- frontend/vue_frontend/vite.config.ts
- frontend/vue_frontend/yarn.lock
- requirements.txt
- run_local_V2.sh
</Project File Tree>


<Project Source Code>

<file path='Dockerfile.web'>
# --- Stage 1: Build Frontend ---
# Using standard Debian-based image
FROM node:18 AS builder

WORKDIR /frontend

# Log contents of the context's frontend directories before attempting COPY
RUN echo ">>> [Builder Stage 1] Listing context contents:"
RUN echo ">>> [Builder Stage 1] Context Root:" && ls -la ../ || echo "Cannot list context root"
RUN echo ">>> [Builder Stage 1] Context 'frontend/':" && ls -la ../frontend/ || echo ">>> [Builder Stage 1] INFO: ../frontend/ not found or empty"
RUN echo ">>> [Builder Stage 1] Context 'frontend/vue_frontend/':" && ls -la ../frontend/vue_frontend/ || echo ">>> [Builder Stage 1] INFO: ../frontend/vue_frontend/ not found or empty"

# Copy package manifests first
COPY frontend/vue_frontend/package.json frontend/vue_frontend/package-lock.json* ./

# Clean cache and update npm
RUN echo ">>> [Builder Stage 1] Cleaning npm cache..." && \
    npm cache clean --force || echo ">>> [Builder Stage 1] WARNING: npm cache clean failed, continuing..."
# RUN echo ">>> [Builder Stage 1] Updating npm..." && \
#     npm install -g npm@latest || echo ">>> [Builder Stage 1] WARNING: Failed to update npm, continuing with default version..."
# RUN echo ">>> [Builder Stage 1] Current npm version:" && npm --version

# Install dependencies using the manifests copied earlier
# Keep --no-optional here, but we'll add the specific one later
RUN echo ">>> [Builder Stage 1] Running npm install --no-optional..." && \
    npm install --no-optional || { echo ">>> [Builder Stage 1] ERROR: npm install failed!"; exit 1; }
RUN echo ">>> [Builder Stage 1] npm install completed."

# Copy the rest of the frontend source code
COPY frontend/vue_frontend/ ./

# --- MODIFIED: Explicitly install the required optional Rollup binary ---
# Determine the target architecture (this assumes linux/arm64 based on the error)
# If building on a different host, Docker buildx might need platform flags
RUN echo ">>> [Builder Stage 1] Explicitly installing @rollup/rollup-linux-arm64-gnu..." && \
    npm install --no-save @rollup/rollup-linux-arm64-gnu || \
    { echo ">>> [Builder Stage 1] WARNING: Failed to explicitly install optional dependency, build might still fail."; }
# --- END MODIFIED ---


# --- ADDED LOGGING ---
# Verify the structure inside the container *before* running the build
RUN echo ">>> [Builder Stage 1] Listing WORKDIR (/frontend) contents BEFORE build:" && ls -la node_modules/@rollup || echo "Cannot list node_modules/@rollup"
RUN echo ">>> [Builder Stage 1] Checking for src/main.ts:" && ls -l src/main.ts || echo ">>> [Builder Stage 1] WARNING: src/main.ts not found!"
RUN echo ">>> [Builder Stage 1] Checking for vite.config.ts:" && ls -l vite.config.ts || echo ">>> [Builder Stage 1] WARNING: vite.config.ts not found!"
# --- END ADDED LOGGING ---

# Build the Vue application
# Ensure 'build' script is defined in your frontend/vue_frontend/package.json
RUN echo ">>> [Builder Stage 1] Running npm run build..." && \
    npm run build || { echo ">>> [Builder Stage 1] ERROR: npm run build failed!"; exit 1; }
RUN echo ">>> [Builder Stage 1] npm run build completed."

# --- ADDED LOGGING ---
# Verify the output of the build - THIS IS CRITICAL
RUN echo ">>> [Builder Stage 1] Listing contents of /frontend/dist AFTER build:" && \
    ls -lA dist/ || { echo ">>> [Builder Stage 1] ERROR: /frontend/dist directory NOT FOUND or empty after build!"; exit 1; }
RUN echo ">>> [Builder Stage 1] Checking for index.html in dist:" && \
    ls -l dist/index.html || { echo ">>> [Builder Stage 1] ERROR: dist/index.html NOT FOUND after build!"; exit 1; }
# --- END ADDED LOGGING ---


# --- Stage 2: Final Python Application ---
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (same as before)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    net-tools \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Create directories for data
RUN mkdir -p /data/input /data/output /data/taxonomy /data/logs

# Install Python dependencies
COPY requirements.txt .
# Consider adding --verbose to pip install for more detail if needed
RUN pip install --no-cache-dir numpy==1.24.4
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code (backend only)
COPY ./app /app

# --- ADDED DIAGNOSTIC ---
# Verify that __init__.py and users.py are present in /app/api/
RUN echo ">>> [Final Stage 2] Verifying contents of /app/api/:" && \
    ls -lA /app/api/ || echo ">>> [Final Stage 2] WARNING: Cannot list /app/api/"
# --- END ADDED DIAGNOSTIC ---

# Copy Built Frontend from Builder Stage
RUN echo ">>> [Final Stage 2] Copying built frontend from builder stage (/frontend/dist) to /app/frontend/dist..."
COPY --from=builder /frontend/dist /app/frontend/dist
# --- ADDED LOGGING ---
# Verify the copy - THIS IS CRITICAL
RUN echo ">>> [Final Stage 2] Listing contents of /app/frontend/dist AFTER COPY:" && \
    ls -lA /app/frontend/dist || { echo ">>> [Final Stage 2] ERROR: /app/frontend/dist directory NOT FOUND or empty after copy!"; exit 1; }
RUN echo ">>> [Final Stage 2] Checking for index.html in /app/frontend/dist:" && \
    ls -l /app/frontend/dist/index.html || { echo ">>> [Final Stage 2] ERROR: /app/frontend/dist/index.html NOT FOUND after copy!"; exit 1; }
# --- END ADDED LOGGING ---

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8000

# Diagnostic output (remains useful)
RUN echo ">>> [Final Stage 2] Python version:" && python --version
RUN echo ">>> [Final Stage 2] Checking Python app directory contents (/app):" && ls -la /app || echo "App directory not found"

# Add a healthcheck
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Run the application
CMD ["sh", "-c", "echo '>>> [CMD Start] Starting web server...' && \
                  echo '>>> [CMD Start] Final check of /app/frontend/dist:' && \
                  ls -lA /app/frontend/dist && \
                  echo '>>> [CMD Start] Running Uvicorn...' && \
                  uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload --log-level debug"]
</file>

<file path='Dockerfile.worker'>

# --- file path='Dockerfile.worker' ---

# Dockerfile.worker

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    net-tools \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Create directories for data
RUN mkdir -p /data/input /data/output /data/taxonomy

# Install Python dependencies in a specific order to avoid binary conflicts
COPY requirements.txt .

# Install NumPy first to ensure pandas works with it correctly
RUN pip install --no-cache-dir numpy==1.24.4
RUN pip install --no-cache-dir -r requirements.txt

# --- ADDED LOGGING ---
# Verify the installed version of pydantic-settings after installation
RUN echo "Checking pydantic-settings installation details:" && pip show pydantic-settings || echo "pydantic-settings not found after install attempt"
RUN echo "Checking Celery installation details:" && pip show celery || echo "Celery not found after install attempt"
# --- END ADDED LOGGING ---

# Copy application code
COPY ./app /app

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Diagnostic output for troubleshooting
RUN echo "Python version:" && python --version
RUN echo "NumPy version:" && python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
RUN echo "Pandas version:" && python -c "import pandas; print(f'Pandas: {pandas.__version__}')" || echo "Pandas not installed"
RUN echo "Checking pandas installation details:" && pip show pandas | grep Version || echo "Pandas package details not available"
RUN echo "Directory structure:" && find /app -type f -name "*.py" | sort

# --- UPDATED CMD ---
# Run Celery worker with corrected app path relative to WORKDIR/PYTHONPATH
# Use -A app.tasks.celery_app:celery_app if app module structure requires it
# Based on current imports, tasks.celery_app seems correct as PYTHONPATH=/app
CMD ["sh", "-c", "echo 'Worker starting...' && \
                  echo 'Current directory:' $(pwd) && \
                  echo 'PYTHONPATH:' $PYTHONPATH && \
                  echo 'Contents of /app/tasks:' && ls -la /app/tasks && \
                  echo 'Running command: celery -A tasks.celery_app worker --loglevel=debug' && \
                  celery -A tasks.celery_app worker --loglevel=debug"]

# --- END UPDATED CMD ---

</file>

<file path='app/api/__init__.py'>

</file>

<file path='app/api/auth.py'>

# app/api/auth.py
from fastapi import Depends, HTTPException, status, Request
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
from typing import Optional
import uuid

from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import get_logger
from core.log_context import set_user, get_user, get_correlation_id
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call

from models.user import User
from core.database import get_db

# Configure logging using our custom logger
logger = get_logger("vendor_classification.auth")

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

@log_function_call(logger, include_args=False) # Don't log passwords
def verify_password(plain_password, hashed_password):
    """Verify password against hashed version."""
    try:
        with LogTimer(logger, "Password verification", include_in_stats=True):
            hash_prefix = hashed_password[:5] if hashed_password else None
            logger.debug(f"Verifying password", extra={"hash_prefix": hash_prefix})
            result = pwd_context.verify(plain_password, hashed_password)
            logger.debug(f"Password verification result", extra={"result": result})
            return result
    except Exception as e:
        logger.error(f"Password verification error: {type(e).__name__}", exc_info=False)
        return False

@log_function_call(logger, include_args=False) # Don't log password
def get_password_hash(password):
    """Generate password hash."""
    try:
        with LogTimer(logger, "Password hashing", include_in_stats=True):
            hashed = pwd_context.hash(password)
            logger.debug(f"Generated password hash", extra={"hash_prefix": hashed[:5] if hashed else None})
            return hashed
    except Exception as e:
        logger.error(f"Password hashing error", exc_info=True)
        raise

@log_function_call(logger, include_args=False) # Don't log password
def authenticate_user(db, username: str, password: str):
    """Authenticate user."""
    try:
        logger.info(f"Authentication attempt", extra={"username": username})
        with LogTimer(logger, f"User authentication", include_in_stats=True):
            user = db.query(User).filter(User.username == username).first()
            if not user:
                logger.warning(f"Authentication failed: user not found", extra={"username": username})
                return None
            logger.debug(f"User found in database", extra={"username": user.username, "user_id": user.id})
            if not verify_password(password, user.hashed_password):
                logger.warning(f"Authentication failed: invalid password", extra={"username": username})
                return None
            logger.info(f"Authentication successful", extra={"username": username, "user_id": user.id})
            return user
    except Exception as e:
        logger.error(f"Authentication error", exc_info=True, extra={"username": username})
        return None

@log_function_call(logger)
def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Create JWT access token."""
    try:
        with LogTimer(logger, "JWT token creation", include_in_stats=True):
            subject = data.get("sub")
            logger.debug("Creating access token", extra={"subject": subject})
            to_encode = data.copy()
            if expires_delta:
                expire = datetime.utcnow() + expires_delta
            else:
                expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
            to_encode.update({"exp": expire})
            encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM)
            logger.debug(f"Access token created", extra={"subject": subject, "expires_at": expire.isoformat()})
            return encoded_jwt
    except Exception as e:
        logger.error(f"Token creation error", exc_info=True)
        raise

async def get_current_user(request: Request, db = Depends(get_db)):
    """Get current user from the JWT token by manually reading header."""
    correlation_id = get_correlation_id() or str(uuid.uuid4())
    logger.debug(f"===> Entered get_current_user function (manual header read)", extra={'correlation_id': correlation_id})

    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer", "X-Correlation-ID": correlation_id},
    )

    token: Optional[str] = None
    try:
        logger.debug("Attempting to manually get Authorization header...")
        auth_header: Optional[str] = request.headers.get("Authorization")
        if not auth_header:
            logger.warning("Authorization header missing")
            raise credentials_exception

        parts = auth_header.split()
        if len(parts) == 1 or parts[0].lower() != "bearer":
                logger.warning(f"Invalid Authorization header format. Header starts with: '{auth_header[:20]}...'")
                if len(parts) == 1 and len(parts[0]) > 20:
                    token = parts[0]
                    logger.warning("Assuming token was provided without 'Bearer ' prefix.")
                else:
                    raise credentials_exception
        elif len(parts) > 2:
                logger.warning(f"Authorization header has too many parts. Header starts with: '{auth_header[:40]}...'")
                raise credentials_exception
        else:
            token = parts[1]

        token_preview = token[:10] + "..." if token else "None"
        logger.debug(f"Manually extracted token: {token_preview}")

    except HTTPException:
        raise
    except Exception as header_err:
        logger.error(f"Error manually extracting token from header", exc_info=True, extra={"error_details": str(header_err)})
        raise credentials_exception

    payload = None
    username = None
    try:
        logger.debug("Attempting JWT decode...")
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        username: Optional[str] = payload.get("sub")
        logger.debug(f"Token decoded successfully", extra={"username": username, "payload_keys": list(payload.keys()) if payload else []})
        if username is None:
            logger.warning("JWT token missing 'sub' (username) claim after decode")
            raise credentials_exception
    except JWTError as jwt_err:
        logger.error(f"JWT decode error (JWTError): {str(jwt_err)}", exc_info=False, extra={"error_details": str(jwt_err), "token_preview": token_preview})
        detail = "Could not validate credentials"
        if "expired" in str(jwt_err).lower():
            detail = "Token has expired"
        elif "invalid signature" in str(jwt_err).lower():
            detail = "Invalid token signature"
        credentials_exception.detail = detail
        raise credentials_exception
    except Exception as decode_err:
        logger.error(f"Unexpected error during JWT decode", exc_info=True, extra={"error_details": str(decode_err)})
        raise credentials_exception

    user = None
    try:
        logger.debug(f"Looking up user in database", extra={"username": username})
        user = db.query(User).filter(User.username == username).first()
        if user is None:
            logger.warning(f"User '{username}' not found in database after token decode")
            raise credentials_exception

        # --- Set user context HERE after successful validation ---
        set_user(user) # Store the full user object in context
        # --- End set user context ---
        logger.debug(f"User found, returning user object.", extra={"username": user.username, "user_id": user.id})
        return user
    except HTTPException:
            raise
    except Exception as db_err:
        logger.error(f"Database error during user lookup in get_current_user", exc_info=True, extra={"error_details": str(db_err)})
        credentials_exception.detail = "Database error during authentication"
        raise credentials_exception

async def get_current_active_user(current_user: User = Depends(get_current_user)):
    """Dependency to get the current user and ensure they are active."""
    if not current_user.is_active:
        logger.warning(f"Authentication failed: User '{current_user.username}' is inactive.")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
    logger.debug(f"User '{current_user.username}' is active.")
    return current_user

async def get_current_active_superuser(current_user: User = Depends(get_current_active_user)):
    """Dependency to get the current active user and ensure they are a superuser."""
    if not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' is not a superuser.")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="The user doesn't have enough privileges"
        )
    logger.debug(f"User '{current_user.username}' is an active superuser.")
    return current_user
</file>

<file path='app/api/jobs.py'>
# <file path='app/api/jobs.py'>
# app/api/jobs.py
from fastapi import APIRouter, Depends, HTTPException, Query, status, Path # Added Path
from sqlalchemy.orm import Session
from typing import List, Optional, Dict
from datetime import datetime
import logging # Import logging

from core.database import get_db
from api.auth import get_current_user
from models.user import User
from models.job import Job, JobStatus
from schemas.job import JobResponse # Import the new schema
from core.logging_config import get_logger
from core.log_context import set_log_context
from core.config import settings # Need settings for file path construction


logger = get_logger("vendor_classification.api.jobs")
logger.debug("Successfully imported Dict from typing for jobs API.")

router = APIRouter()

@router.get("/", response_model=List[JobResponse])
async def list_jobs(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    status_filter: Optional[JobStatus] = Query(None, alias="status", description="Filter jobs by status"),
    start_date: Optional[datetime] = Query(None, description="Filter jobs created on or after this date (ISO format)"),
    end_date: Optional[datetime] = Query(None, description="Filter jobs created on or before this date (ISO format)"),
    skip: int = Query(0, ge=0, description="Number of jobs to skip for pagination"),
    limit: int = Query(100, ge=1, le=500, description="Maximum number of jobs to return"),
):
    """
    List jobs for the current user. Admins can see all jobs (optional enhancement).
    Supports filtering by status and date range, and pagination.
    """
    set_log_context({"username": current_user.username})
    logger.info("Fetching job history", extra={
        "status_filter": status_filter,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "skip": skip,
        "limit": limit,
    })

    query = db.query(Job)

    # Filter by user (Admins could potentially see all - add logic here if needed)
    # For now, all users only see their own jobs
    # if not current_user.is_superuser: # Example admin check
    query = query.filter(Job.created_by == current_user.username)

    # Apply filters
    if status_filter:
        query = query.filter(Job.status == status_filter.value)
    if start_date:
        query = query.filter(Job.created_at >= start_date)
    if end_date:
        # Add a day to end_date to make it inclusive of the whole day if time is not specified
        # Or adjust based on desired behavior (e.g., end_date < end_date + timedelta(days=1))
        query = query.filter(Job.created_at <= end_date)

    # Order by creation date (newest first)
    query = query.order_by(Job.created_at.desc())

    # Apply pagination
    jobs = query.offset(skip).limit(limit).all()

    logger.info(f"Retrieved {len(jobs)} jobs from history.")

    # Convert Job models to JobResponse schemas
    # Pydantic v2 handles this automatically with from_attributes=True
    return jobs

@router.get("/{job_id}", response_model=JobResponse)
async def read_job(
    # Use Path to ensure job_id is correctly extracted from the URL path
    job_id: str = Path(..., title="The ID of the job to get"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve details for a specific job by its ID.
    Ensures the current user owns the job (or is an admin - future enhancement).
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching details for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # --- Authorization Check ---
    # Ensure the user requesting the job is the one who created it
    # (Or add admin override logic here if needed)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access this job")
    # --- End Authorization Check ---

    # LOGGING: Log the job details being returned, especially target_level
    logger.info(f"Returning details for job ID: {job_id}", extra={"job_status": job.status, "target_level": job.target_level})
    return job # Pydantic will validate against JobResponse

# Use Dict for flexibility, or create a specific StatsResponse schema later if needed
@router.get("/{job_id}/stats", response_model=Dict)
async def read_job_stats(
    job_id: str = Path(..., title="The ID of the job to get stats for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Retrieve processing statistics for a specific job.
    """
    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Fetching statistics for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for stats", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check (same as read_job)
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted to access stats for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to access stats for this job")

    # LOGGING: Log the raw stats being returned from the database
    logger.info(f"Returning statistics for job ID: {job_id}")
    logger.debug(f"Raw stats from DB for job {job_id}: {job.stats}") # Log the actual stats dict

    # The stats are stored as JSON in the Job model
    return job.stats if job.stats else {}

@router.get("/{job_id}/download")
async def download_job_results(
    job_id: str = Path(..., title="The ID of the job to download results for"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Downloads the output Excel file for a completed job.
    """
    from fastapi.responses import FileResponse # Import here
    import os # Import os

    set_log_context({"username": current_user.username, "target_job_id": job_id})
    logger.info(f"Request to download results for job ID: {job_id}")

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.warning(f"Job not found for download", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Job not found")

    # Authorization Check
    if job.created_by != current_user.username: # and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' attempted download for job '{job_id}' owned by '{job.created_by}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized to download results for this job")

    if job.status != JobStatus.COMPLETED.value or not job.output_file_name:
        logger.warning(f"Download requested but job not completed or output file missing",
                       extra={"job_id": job_id, "status": job.status, "output_file": job.output_file_name})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Job not completed or output file not available.")

    # Construct the full path to the output file
    output_dir = os.path.join(settings.OUTPUT_DATA_DIR, job_id)
    file_path = os.path.join(output_dir, job.output_file_name)

    if not os.path.exists(file_path):
         logger.error(f"Output file record exists in DB but file not found on disk",
                      extra={"job_id": job_id, "expected_path": file_path})
         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Output file not found.")

    logger.info(f"Streaming output file for download",
                extra={"job_id": job_id, "file_path": file_path})
    return FileResponse(
        path=file_path,
        filename=job.output_file_name, # Suggest filename to browser
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )
</file>

<file path='app/api/main.py'>

# app/api/main.py
import socket
import sqlalchemy
import httpx
from fastapi import (
    FastAPI, Depends, HTTPException, UploadFile, File, Form,
    BackgroundTasks, status, Request
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from typing import Dict, Any, Optional, List
import uuid
import os
from datetime import datetime, timedelta
import logging
import time
from sqlalchemy.orm import Session

# --- Model Imports ---
from models.job import Job, JobStatus, ProcessingStage
from models.user import User

# --- Core Imports ---
from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import setup_logging, get_logger
from core.log_context import set_correlation_id, set_user, set_job_id, get_correlation_id
# Import middleware (which now uses updated context functions)
from middleware.logging_middleware import RequestLoggingMiddleware
from core.database import get_db, SessionLocal, engine
from core.initialize_db import initialize_database

# --- Service Imports ---
from services.file_service import save_upload_file

# --- Task Imports ---
from tasks.celery_app import celery_app
from tasks.classification_tasks import process_vendor_file

# --- Utility Imports ---
from utils.taxonomy_loader import load_taxonomy

# --- Auth Imports ---
from fastapi.security import OAuth2PasswordRequestForm
from api.auth import (
    get_current_user,
    authenticate_user,
    create_access_token,
    get_current_active_user
)

# --- Router Imports ---
from api import jobs as jobs_router
from api import users as users_router

# --- Schema Imports ---
from schemas.job import JobResponse
from schemas.user import UserResponse as UserResponseSchema

# --- Logging Setup ---
# Initialize logging BEFORE creating the FastAPI app instance
# This ensures loggers are ready when middleware/routers are attached
setup_logging(log_level=logging.DEBUG, log_to_file=True, log_dir=settings.TAXONOMY_DATA_DIR.replace('taxonomy', 'logs')) # Use settings for log dir
logger = get_logger("vendor_classification.api")

# --- FastAPI App Initialization ---
app = FastAPI(
    title="NAICS Vendor Classification API",
    description="API for classifying vendors according to NAICS taxonomy",
    version="1.0.0",
)

# --- Middleware ---
app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Allow all origins for now, restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Include Routers ---
logger.info("Including API routers...")
app.include_router(
    jobs_router.router,
    prefix="/api/v1/jobs",
    tags=["Jobs"],
    dependencies=[Depends(get_current_user)]
)
logger.info("Included jobs router with prefix /api/v1/jobs")

app.include_router(
    users_router.router,
    prefix="/api/v1/users",
    tags=["Users"],
)
logger.info("Included users router with prefix /api/v1/users")
# --- End Include Routers ---

# --- Vue.js Frontend Serving Setup ---
VUE_BUILD_DIR = "/app/frontend/dist"
VUE_INDEX_FILE = os.path.join(VUE_BUILD_DIR, "index.html")
logger.info(f"Attempting to serve Vue frontend from: {VUE_BUILD_DIR}")
if not os.path.exists(VUE_BUILD_DIR):
    logger.error(f"Vue build directory NOT FOUND at {VUE_BUILD_DIR}. Frontend will not be served.")
elif not os.path.exists(VUE_INDEX_FILE):
    logger.error(f"Vue index.html NOT FOUND at {VUE_INDEX_FILE}. Frontend serving might fail.")
else:
    logger.info(f"Vue build directory and index.html found. Static files will be mounted.")

# --- API ROUTES ---

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    hostname = socket.gethostname()
    local_ip = ""
    try:
        local_ip = socket.gethostbyname(hostname)
    except socket.gaierror:
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
            s.close()
        except Exception:
                local_ip = "Could not resolve IP"

    logger.info(f"Health check called", extra={"hostname": hostname, "ip": local_ip})
    db_status = "unknown"
    db = None
    try:
        db = SessionLocal()
        db.execute(sqlalchemy.text("SELECT 1"))
        db_status = "connected"
    except Exception as e:
        logger.error(f"Health Check: Database connection error", exc_info=True, extra={"error_details": str(e)})
        db_status = f"error: {str(e)[:100]}"
    finally:
        if db:
            db.close()

    vue_frontend_status = "found" if os.path.exists(VUE_INDEX_FILE) else "missing"

    celery_broker_status = "unknown"
    celery_connection = None
    try:
        celery_connection = celery_app.connection(heartbeat=2.0)
        celery_connection.ensure_connection(max_retries=1, timeout=2)
        celery_broker_status = "connected"
    except Exception as celery_e:
        logger.error(f"Celery broker connection error during health check: {str(celery_e)}", exc_info=False)
        celery_broker_status = f"error: {str(celery_e)[:100]}"
    finally:
            if celery_connection:
                try: celery_connection.close()
                except Exception as close_err: logger.warning(f"Error closing celery connection in health check: {close_err}")

    openrouter_status = "unknown"
    tavily_status = "unknown"
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
                or_url = f"{settings.OPENROUTER_API_BASE}/models"
                or_headers = {"Authorization": f"Bearer {settings.OPENROUTER_API_KEY}"}
                or_resp = await client.get(or_url, headers=or_headers)
                openrouter_status = "connected" if or_resp.status_code == 200 else f"error: {or_resp.status_code}"

                tv_url = "https://api.tavily.com/search"
                tv_payload = {"api_key": settings.TAVILY_API_KEY, "query": "test", "max_results": 1}
                tv_resp = await client.post(tv_url, json=tv_payload)
                tavily_status = "connected" if tv_resp.status_code == 200 else f"error: {tv_resp.status_code}"

    except httpx.RequestError as http_err:
            logger.warning(f"HTTPX RequestError during external API health check: {http_err}")
            openrouter_status = openrouter_status if openrouter_status != "unknown" else "connection_error"
            tavily_status = tavily_status if tavily_status != "unknown" else "connection_error"
    except Exception as api_err:
            logger.error(f"Error checking external APIs during health check: {api_err}")
            openrouter_status = openrouter_status if openrouter_status != "unknown" else "check_error"
            tavily_status = tavily_status if tavily_status != "unknown" else "check_error"

    return {
        "status": "healthy",
        "hostname": hostname,
        "ip": local_ip,
        "database": db_status,
        "celery_broker": celery_broker_status,
        "vue_frontend_index": vue_frontend_status,
        "external_api_openrouter": openrouter_status,
        "external_api_tavily": tavily_status,
        "timestamp": datetime.now().isoformat()
    }


# --- Exception Handlers ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    correlation_id = get_correlation_id() or str(uuid.uuid4())
    try: body_preview = str(await request.body())[:500]
    except Exception: body_preview = "[Could not read request body]"
    logger.error("Request validation failed (422)", extra={
        "error_details": exc.errors(), "request_body_preview": body_preview,
        "request_headers": dict(request.headers), "correlation_id": correlation_id,
        "path": request.url.path
    })
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": exc.errors()},
        headers={"X-Correlation-ID": correlation_id}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    correlation_id = get_correlation_id() or str(uuid.uuid4())
    logger.error(f"Unhandled exception during request to {request.url.path}", exc_info=True, extra={
        "correlation_id": correlation_id, "request_headers": dict(request.headers),
        "path": request.url.path, "method": request.method,
    })
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": "An internal server error occurred.", "correlation_id": correlation_id},
        headers={"X-Correlation-ID": correlation_id}
    )


# --- Authentication Endpoint ---
@app.post("/token", response_model=Dict[str, Any])
async def login_for_access_token(
    request: Request,
    form_data: OAuth2PasswordRequestForm = Depends(),
    db: Session = Depends(get_db)
):
    """Handles user login and returns JWT token and user details."""
    correlation_id = str(uuid.uuid4())
    set_correlation_id(correlation_id)
    client_host = request.client.host if request.client else "Unknown"
    logger.info(f"Login attempt", extra={"username": form_data.username, "ip": client_host})

    try:
        user = authenticate_user(db, form_data.username, form_data.password)
        if not user:
            logger.warning(f"Login failed: invalid credentials", extra={"username": form_data.username, "ip": client_host})
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password",
                headers={"WWW-Authenticate": "Bearer"},
            )

        if not user.is_active:
                logger.warning(f"Login failed: user '{user.username}' is inactive.", extra={"ip": client_host})
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Inactive user.",
                )

        set_user(user) # Set context for logging
        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
        access_token = create_access_token(
            data={"sub": user.username}, expires_delta=access_token_expires
        )

        logger.info(f"Login successful, token generated", extra={ "username": user.username, "ip": client_host, "token_expires_in_minutes": settings.ACCESS_TOKEN_EXPIRE_MINUTES})

        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": UserResponseSchema.model_validate(user)
        }

    except HTTPException as http_exc:
        if http_exc.status_code not in [status.HTTP_401_UNAUTHORIZED, status.HTTP_400_BAD_REQUEST]:
                logger.error(f"HTTP exception during login", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"Unexpected login error", exc_info=True, extra={"error": str(e), "username": form_data.username})
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An error occurred during the login process."
        )

# --- UPLOAD ROUTE (Updated) ---
@app.post("/api/v1/upload", response_model=JobResponse, status_code=status.HTTP_202_ACCEPTED)
async def upload_vendor_file(
    background_tasks: BackgroundTasks,
    company_name: str = Form(...),
    # --- ADDED: target_level parameter ---
    target_level: int = Form(..., ge=1, le=5, description="Target classification level (1-5)"),
    # --- END ADDED ---
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Accepts vendor file upload, creates a job, and queues it for processing.
    Allows specifying the target classification level.
    """
    job_id = str(uuid.uuid4())
    set_job_id(job_id)
    set_user(current_user)

    logger.info(f"Upload request received", extra={
        "job_id": job_id,
        "company_name": company_name,
        "target_level": target_level, # Log the target level
        "uploaded_filename": file.filename,
        "content_type": file.content_type,
        "username": current_user.username
    })

    if not file.filename:
        logger.warning("Upload attempt with no filename.", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="No filename provided.")
    if not file.filename.lower().endswith(('.xlsx', '.xls')):
        logger.warning(f"Invalid file type uploaded: {file.filename}", extra={"job_id": job_id})
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid file type. Please upload an Excel file (.xlsx or .xls).")

    saved_file_path = None
    try:
        logger.debug(f"Attempting to save uploaded file for job {job_id}")
        saved_file_path = save_upload_file(file=file, job_id=job_id)
        logger.info(f"File saved successfully for job {job_id}", extra={"saved_path": saved_file_path})
    except IOError as e:
        logger.error(f"Failed to save uploaded file for job {job_id}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not save file: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during file upload/saving for job {job_id}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error processing upload: {e}")

    job = None
    try:
        logger.debug(f"Creating database job record for job {job_id}")
        job = Job(
            id=job_id,
            company_name=company_name,
            input_file_name=os.path.basename(saved_file_path),
            status=JobStatus.PENDING.value,
            current_stage=ProcessingStage.INGESTION.value,
            created_by=current_user.username,
            target_level=target_level # Save the target level
        )
        db.add(job)
        db.commit()
        db.refresh(job)
        logger.info(f"Database job record created successfully for job {job_id}", extra={"target_level": job.target_level})
    except Exception as e:
        db.rollback()
        logger.error(f"Failed to create database job record for job {job_id}", exc_info=True)
        if saved_file_path and os.path.exists(saved_file_path):
            try: os.remove(saved_file_path)
            except OSError: logger.warning(f"Could not remove file {saved_file_path} after DB error.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create job record.")

    try:
        logger.info(f"Adding Celery task 'process_vendor_file' to background tasks for job {job_id}")
        # --- UPDATED: Pass target_level to Celery task ---
        background_tasks.add_task(process_vendor_file.delay, job_id=job_id, file_path=saved_file_path, target_level=target_level)
        # --- END UPDATED ---
        logger.info(f"Celery task queued successfully for job {job_id}")
    except Exception as e:
        logger.error(f"Failed to queue Celery task for job {job_id}", exc_info=True)
        job.fail(f"Failed to queue processing task: {str(e)}")
        db.commit()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to queue job for processing.")

    logger.info(f"Upload request for job {job_id} processed successfully, returning 202 Accepted.")
    # Use model_validate for Pydantic v2
    return JobResponse.model_validate(job)
# --- END UPLOAD ROUTE ---


# --- Mount Static Files (Vue App) ---
if os.path.exists(VUE_BUILD_DIR) and os.path.exists(VUE_INDEX_FILE):
    logger.info(f"Mounting Vue app from directory: {VUE_BUILD_DIR}")
    app.mount("/", StaticFiles(directory=VUE_BUILD_DIR, html=True), name="app")
else:
    logger.error(f"Cannot mount Vue app: Directory {VUE_BUILD_DIR} or index file {VUE_INDEX_FILE} not found.")
    @app.get("/")
    async def missing_frontend():
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": f"Frontend not found. Expected build files in {VUE_BUILD_DIR}"}
        )
# --- END VUE.JS FRONTEND SERVING SETUP ---
</file>

<file path='app/api/users.py'>
# app/api/users.py
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from typing import List, Any
import uuid

from core.database import get_db
from schemas.user import UserCreate, UserUpdate, UserResponse
from services import user_service
from api.auth import get_current_active_user, get_current_active_superuser
from models.user import User as UserModel # Import the model for type hinting
# --- MODIFIED IMPORT: Import set_log_context from core.log_context ---
from core.logging_config import get_logger
from core.log_context import set_log_context
# --- END MODIFIED IMPORT ---

logger = get_logger("vendor_classification.api.users")

router = APIRouter()

@router.post("/", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
def create_user(
    *,
    db: Session = Depends(get_db),
    user_in: UserCreate,
    current_user: UserModel = Depends(get_current_active_superuser) # Require admin
):
    """
    Create new user. Requires superuser privileges.
    """
    set_log_context({"admin_user": current_user.username})
    logger.info(f"Admin '{current_user.username}' attempting to create user '{user_in.username}'")
    # Service layer handles potential duplicate username/email via HTTPException
    user = user_service.create_user(db=db, user_in=user_in)
    logger.info(f"User '{user.username}' created successfully by admin '{current_user.username}'")
    return user

@router.get("/", response_model=List[UserResponse])
def read_users(
    db: Session = Depends(get_db),
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=200),
    current_user: UserModel = Depends(get_current_active_superuser) # Require admin
):
    """
    Retrieve users. Requires superuser privileges.
    """
    set_log_context({"admin_user": current_user.username})
    logger.info(f"Admin '{current_user.username}' requesting user list", extra={"skip": skip, "limit": limit})
    users = user_service.get_users(db, skip=skip, limit=limit)
    logger.info(f"Returning {len(users)} users to admin '{current_user.username}'")
    return users

@router.get("/me", response_model=UserResponse)
def read_user_me(
    current_user: UserModel = Depends(get_current_active_user) # Just need active user
):
    """
    Get current user details.
    """
    set_log_context({"requesting_user": current_user.username})
    logger.info(f"User '{current_user.username}' requesting their own details.")
    # The dependency already fetches the user object
    return current_user

@router.get("/{user_id}", response_model=UserResponse)
def read_user_by_id(
    user_id: str,
    current_user: UserModel = Depends(get_current_active_user),
    db: Session = Depends(get_db)
):
    """
    Get a specific user by ID. Requires admin privileges or the user themselves.
    """
    set_log_context({"requesting_user": current_user.username, "target_user_id": user_id})
    logger.info(f"User '{current_user.username}' requesting details for user ID '{user_id}'")
    user = user_service.get_user(db, user_id=user_id)
    if not user:
        logger.warning(f"Target user not found", extra={"target_user_id": user_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")

    # Check permissions
    if user.id != current_user.id and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' cannot access details for user '{user.username}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not enough permissions")

    logger.info(f"Returning details for user '{user.username}'")
    return user

@router.put("/{user_id}", response_model=UserResponse)
def update_user(
    *,
    db: Session = Depends(get_db),
    user_id: str,
    user_in: UserUpdate,
    current_user: UserModel = Depends(get_current_active_user)
):
    """
    Update a user. Requires admin privileges or the user themselves.
    Non-admins cannot change their own is_superuser status or activate/deactivate themselves.
    """
    set_log_context({"requesting_user": current_user.username, "target_user_id": user_id})
    logger.info(f"User '{current_user.username}' attempting to update user ID '{user_id}'")
    db_user = user_service.get_user(db, user_id=user_id)
    if not db_user:
        logger.warning(f"Target user not found for update", extra={"target_user_id": user_id})
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")

    # Check permissions
    if db_user.id != current_user.id and not current_user.is_superuser:
        logger.warning(f"Authorization failed: User '{current_user.username}' cannot update user '{db_user.username}'")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not enough permissions to update this user")

    # Non-admins cannot make themselves superuser or change their active status
    if not current_user.is_superuser:
        if user_in.is_superuser is not None and user_in.is_superuser != db_user.is_superuser:
                logger.warning(f"User '{current_user.username}' attempted to change their own superuser status.")
                raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Cannot change superuser status")
        if user_in.is_active is not None and user_in.is_active != db_user.is_active:
                logger.warning(f"User '{current_user.username}' attempted to change their own active status.")
                raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Cannot change own active status")

    # Service layer handles update logic and potential integrity errors
    updated_user = user_service.update_user(db=db, user_id=user_id, user_in=user_in)
    if not updated_user: # Should be handled by service, but double check
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found after update attempt")

    logger.info(f"User '{db_user.username}' updated successfully by '{current_user.username}'")
    return updated_user


@router.delete("/{user_id}", status_code=status.HTTP_200_OK)
def delete_user(
    *,
    db: Session = Depends(get_db),
    user_id: str,
    current_user: UserModel = Depends(get_current_active_superuser) # Require admin
):
    """
    Delete a user. Requires superuser privileges.
    """
    set_log_context({"admin_user": current_user.username, "target_user_id": user_id})
    logger.info(f"Admin '{current_user.username}' attempting to delete user ID '{user_id}'")

    # Prevent admin from deleting themselves?
    if str(current_user.id) == user_id:
            logger.error(f"Admin '{current_user.username}' attempted to delete themselves.")
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Admins cannot delete themselves.")

    deleted = user_service.delete_user(db=db, user_id=user_id)
    if not deleted:
        # Service layer already logged warning
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")

    logger.info(f"User ID '{user_id}' deleted successfully by admin '{current_user.username}'")
    return {"message": "User deleted successfully"}
</file>

<file path='app/core/config.py'>
import os
import logging
from pydantic_settings import BaseSettings  # Updated import from pydantic-settings package
from typing import Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("vendor_classification")

logger.info("Initializing application settings")

class Settings(BaseSettings):
    """Application settings."""
    
    # API Configuration
    API_V1_STR: str = "/api/v1"
    PROJECT_NAME: str = "NAICS Vendor Classification"
    
    # Security
    SECRET_KEY: str = os.getenv("SECRET_KEY", "supersecretkey")  # Change in production
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    
    # Database
    DATABASE_URL: str = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/vendor_classification")
    
    # Redis
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://localhost:6379/0")
    
    # File Storage
    INPUT_DATA_DIR: str = "/data/input"
    OUTPUT_DATA_DIR: str = "/data/output"
    TAXONOMY_DATA_DIR: str = "/data/taxonomy"
    
    # OpenRouter - HARDCODED API KEY FOR TESTING
    OPENROUTER_API_KEY: str = "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"  #"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" # "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" 
    OPENROUTER_API_BASE: str = "https://openrouter.ai/api/v1"
    OPENROUTER_MODEL: str = "deepseek/deepseek-chat-v3-0324:free"  # Default model to use
    
    # Tavily Search API - HARDCODED API KEY FOR TESTING
    TAVILY_API_KEY: str = "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" #"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" #"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"
    
    # Processing Configuration
    BATCH_SIZE: int = 5
    MAX_RETRIES: int = 3
    RETRY_DELAY: int = 1  # seconds
    
    class Config:
        env_file = ".env"
        case_sensitive = True

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        logger.info(f"Settings initialized with DATABASE_URL: {self.DATABASE_URL}")
        logger.info(f"OPENROUTER_MODEL: {self.OPENROUTER_MODEL}")
        # Log if keys are present but not their actual values for security
        logger.info(f"OPENROUTER_API_KEY present: {bool(self.OPENROUTER_API_KEY)}")
        logger.info(f"OPENROUTER_API_BASE present: {bool(self.OPENROUTER_API_BASE)}")
        logger.info(f"TAVILY_API_KEY present: {bool(self.TAVILY_API_KEY)}")

settings = Settings()
logger.info("Application settings loaded successfully")

</file>

<file path='app/core/database.py'>
import logging
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

from core.config import settings

# Configure logging
logger = logging.getLogger("vendor_classification.database")

logger.info("Initializing database connection")

# Create SQLAlchemy engine
try:
    engine = create_engine(settings.DATABASE_URL)
    logger.info(f"Database engine created successfully for {settings.DATABASE_URL.split('@')[-1]}")
except Exception as e:
    logger.error(f"Failed to create database engine: {e}")
    raise

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
logger.info("Database session factory created")

# Create base class for models
Base = declarative_base()
logger.info("SQLAlchemy Base declarative_base initialized")

def get_db():
    """Get database session."""
    db = SessionLocal()
    logger.debug("Database session created")
    try:
        yield db
    finally:
        db.close()
        logger.debug("Database session closed")
</file>

<file path='app/core/initialize_db.py'>
# --- file path='app/core/initialize_db.py' ---
import logging
from sqlalchemy import create_engine, inspect as sql_inspect # Added inspect
from sqlalchemy.exc import SQLAlchemyError # Added for specific exception handling
from core.database import Base, SessionLocal, engine # Import engine directly
from core.config import settings
import sys
import uuid
from api.auth import get_password_hash

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("vendor_classification.db_init")

# Import all models to ensure they're registered with SQLAlchemy
logger.info("Importing models for database initialization")
try:
    from models.user import User
    from models.job import Job
    # Import other models here if they exist
    logger.info("Models imported successfully.")
except ImportError as e:
    logger.critical(f"Failed to import models for DB initialization: {e}", exc_info=True)
    sys.exit(1) # Exit if models can't be imported

def create_or_update_admin_user():
    """
    Ensure the 'admin' user exists.
    If no user with username=admin, create one with default password 'password'.
    Otherwise, optionally update its password to ensure a consistent known credential.
    """
    db = None # Initialize db to None
    try:
        db = SessionLocal()
        logger.info("Checking for existing admin user...")
        admin_user = db.query(User).filter(User.username == "admin").first()
        if admin_user:
            logger.info("Admin user already exists.")
            # OPTIONAL: Force-update the password each time:
            # If you do NOT want to re-hash or overwrite the password, remove below lines.
            try:
                logger.info("Attempting to update admin password to default 'password'...")
                admin_user.hashed_password = get_password_hash("password")
                db.commit()
                logger.info("Admin password was updated/reset to default: 'password'.")
            except Exception as hash_err:
                logger.error(f"Failed to update admin password: {hash_err}", exc_info=True)
                db.rollback() # Rollback password change on error
        else:
            logger.info("Admin user not found, creating default admin user...")
            admin_user = User(
                id=str(uuid.uuid4()),
                username="admin",
                email="admin@example.com",
                full_name="Admin User",
                hashed_password=get_password_hash("password"),
                is_active=True,
                is_superuser=True
            )
            db.add(admin_user)
            db.commit()
            logger.info("Created default admin user: admin / password")
    except SQLAlchemyError as e: # Catch specific DB errors
        logger.error(f"Database error during admin user check/creation: {e}", exc_info=True)
        if db: db.rollback()
    except Exception as e: # Catch other errors like hashing
        logger.error(f"Unexpected error ensuring admin user: {e}", exc_info=True)
        if db: db.rollback()
    finally:
        if db:
            db.close()
            logger.info("Admin user check/creation DB session closed.")

def initialize_database():
    """Initialize database by creating all tables, then ensuring 'admin' user exists."""
    max_retries = 5
    retry_delay = 5 # seconds
    for attempt in range(max_retries):
        try:
            logger.info(f"Attempting database initialization (Attempt {attempt + 1}/{max_retries})...")
            # Use the imported engine
            logger.info(f"Using database engine for URL: {settings.DATABASE_URL.split('@')[-1]}")

            # Check connection before creating tables
            with engine.connect() as connection:
                logger.info("Successfully connected to the database.")

            logger.info("Inspecting existing tables...")
            inspector = sql_inspect(engine)
            existing_tables = inspector.get_table_names()
            logger.info(f"Existing tables found: {existing_tables}")

            logger.info("Attempting to create all tables defined in Base.metadata...")
            # Log the tables Base knows about
            logger.info(f"Base.metadata knows about tables: {list(Base.metadata.tables.keys())}")
            Base.metadata.create_all(engine)
            logger.info("Base.metadata.create_all(engine) executed successfully.")

            # Verify tables were created
            logger.info("Re-inspecting tables after create_all...")
            inspector = sql_inspect(engine) # Re-inspect
            new_existing_tables = inspector.get_table_names()
            logger.info(f"Tables found after create_all: {new_existing_tables}")

            # Specifically check for 'users' and 'jobs' tables
            if 'users' not in new_existing_tables:
                logger.error("'users' table NOT FOUND after create_all call!")
            else:
                logger.info("'users' table found after create_all call.")
            if 'jobs' not in new_existing_tables:
                logger.warning("'jobs' table NOT FOUND after create_all call!") # Maybe optional?
            else:
                logger.info("'jobs' table found after create_all call.")

            # Create or update the admin user
            create_or_update_admin_user()

            logger.info("Database initialization appears successful.")
            return True # Success

        except SQLAlchemyError as e:
            logger.error(f"Database connection or table creation error on attempt {attempt + 1}: {e}", exc_info=False) # Don't need full trace every retry
            if attempt + 1 == max_retries:
                logger.critical("Max retries reached. Database initialization failed.", exc_info=True)
                raise # Raise the last exception
            logger.info(f"Retrying in {retry_delay} seconds...")
            time.sleep(retry_delay)
        except Exception as e:
            logger.error(f"Unexpected error during database initialization attempt {attempt + 1}: {e}", exc_info=True)
            # Decide if retry makes sense for unexpected errors, maybe not
            raise # Re-raise immediately for unexpected errors

    logger.error("Database initialization failed after all retries.")
    return False # Should not be reached if exceptions are raised

if __name__ == "__main__":
    import time # Import time for retries
    logger.info("Starting database initialization script directly...")
    try:
        success = initialize_database()
        if success:
            logger.info("Database initialization script completed successfully.")
            sys.exit(0)
        else:
            logger.error("Database initialization script failed.")
            sys.exit(1)
    except Exception as e:
        logger.critical(f"Unhandled exception during database initialization script: {e}", exc_info=True)
        sys.exit(1)
</file>

<file path='app/core/log_context.py'>

# app/core/log_context.py
import threading
import uuid
import json
from typing import Optional, List, Dict, Any

# Create a thread-local storage for correlation IDs and context
local_storage = threading.local()

def set_correlation_id(correlation_id: Optional[str] = None) -> str:
    """Set a correlation ID for the current thread/context."""
    if correlation_id is None:
        correlation_id = str(uuid.uuid4())
    local_storage.correlation_id = correlation_id
    return correlation_id

def get_correlation_id() -> Optional[str]:
    """Get the current correlation ID."""
    return getattr(local_storage, 'correlation_id', None)

def set_request_id(request_id: Optional[str] = None) -> str:
    """Set a request ID for the current thread/context."""
    if request_id is None:
        request_id = str(uuid.uuid4())
    local_storage.request_id = request_id
    return request_id

def get_request_id() -> Optional[str]:
    """Get the current request ID."""
    return getattr(local_storage, 'request_id', None)

def set_job_id(job_id: str) -> str:
    """Set a job ID for the current thread/context."""
    local_storage.job_id = job_id
    return job_id

def get_job_id() -> Optional[str]:
    """Get the current job ID."""
    return getattr(local_storage, 'job_id', None)

def set_user(user: Any) -> Dict[str, Any]:
    """Set user information for the current thread/context, ensuring serializability."""
    user_info = {}
    if isinstance(user, dict):
        user_info = user.copy() # Avoid modifying original dict
    elif hasattr(user, 'username'):
        user_info['username'] = user.username
        user_info['id'] = getattr(user, 'id', None)
    elif user is not None:
        user_info['username'] = str(user)
    else:
        user_info = {'username': 'anonymous'} # Default for None

    # Ensure serializability
    serializable_user_info = {}
    for k, v in user_info.items():
        try:
            json.dumps({k: v})
            serializable_user_info[k] = v
        except TypeError:
            serializable_user_info[k] = str(v)

    local_storage.user = serializable_user_info
    return local_storage.user

def get_user() -> Optional[Dict[str, Any]]:
    """Get the current user information."""
    return getattr(local_storage, 'user', None)

def set_log_context(context_dict: Dict[str, Any]):
    """Add or update context data to logs for the current thread/context."""
    if not hasattr(local_storage, 'context'):
        local_storage.context = {}
    if isinstance(context_dict, dict):
        local_storage.context.update(context_dict)

def get_log_context() -> Dict[str, Any]:
    """Get the current log context dictionary."""
    return getattr(local_storage, 'context', {}).copy()

def clear_log_context(keys: Optional[List[str]] = None):
    """Clear specific or all log context for the current thread/context."""
    if hasattr(local_storage, 'context'):
        if keys:
            for key in keys:
                local_storage.context.pop(key, None)
        else:
            local_storage.context = {}

def set_performance_stats(stats_dict: Dict[str, Any]):
    """Set performance stats data for the current thread/context."""
    local_storage.performance_stats = stats_dict

def get_performance_stats() -> Dict[str, Any]:
    """Get the current performance statistics."""
    return getattr(local_storage, 'performance_stats', {}).copy()

def update_performance_stat(stat_name: str, value: float):
    """Update a single performance stat (e.g., duration)."""
    if not hasattr(local_storage, 'performance_stats'):
        local_storage.performance_stats = {}
    # Simple update, LogTimer in log_utils provides more detail
    local_storage.performance_stats[stat_name] = value

def clear_performance_stats():
    """Clear the performance statistics."""
    if hasattr(local_storage, 'performance_stats'):
        delattr(local_storage, 'performance_stats')

def clear_all_context():
    """Clear all context data (IDs, user, context dict, stats) for the current thread/context."""
    # Iterate through known attributes and remove them
    for attr in ['correlation_id', 'request_id', 'job_id', 'user', 'context', 'performance_stats']:
        if hasattr(local_storage, attr):
            try:
                delattr(local_storage, attr)
            except AttributeError:
                pass # Should not happen with hasattr check, but belt-and-suspenders
</file>

<file path='app/core/log_formatters.py'>

# app/core/log_formatters.py
import logging
import json
import socket
from datetime import datetime
from typing import Dict, Any

# Import context functions from the new module
from .log_context import get_correlation_id, get_job_id, get_request_id, get_user, get_log_context

class JsonFormatter(logging.Formatter):
    """Format logs as JSON for better parsing and analysis."""

    def format(self, record: logging.LogRecord) -> str:
        timestamp = datetime.utcfromtimestamp(record.created).isoformat() + "Z"

        # Base log data
        log_data: Dict[str, Any] = {
            "timestamp": timestamp,
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(), # Ensure message is formatted
        }

        # --- Add Context Fields ---
        context_fields = {}
        correlation_id = get_correlation_id()
        if correlation_id:
            context_fields["correlation_id"] = correlation_id
        job_id = get_job_id()
        if job_id:
            context_fields["job_id"] = job_id
        request_id = get_request_id()
        if request_id:
             context_fields["request_id"] = request_id
        user_info = get_user()
        if user_info:
             # User info should already be serializable from set_user
             context_fields["user"] = user_info

        # Merge context fields
        log_data.update(context_fields)

        # --- Add Caller Info ---
        caller_info = {
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        log_data.update(caller_info)

        # --- Add Process/Thread Info ---
        process_info = {
            "process_id": record.process,
            "thread_id": record.thread,
            "host": socket.gethostname(),
        }
        log_data.update(process_info)

        # --- Add Exception Info ---
        if record.exc_info:
            exc_type, exc_value, exc_traceback = record.exc_info
            exception_info = {
                "type": exc_type.__name__ if exc_type else "Unknown",
                "message": str(exc_value) if exc_value else "",
                "traceback": self.formatException(record.exc_info) if exc_traceback else None
            }
            log_data["exception"] = exception_info

        if record.stack_info:
            log_data["stack_info"] = record.stack_info

        # --- Add Extra Attributes ---
        extra_data = {}
        standard_attrs = set(logging.LogRecord('', '', '', '', '', '', '', '').__dict__.keys()) | {'message', 'asctime', 'relativeCreated', 'exc_text', 'stack_info'} | set(log_data.keys())
        for key, value in record.__dict__.items():
            if key not in standard_attrs:
                extra_data[key] = value

        # Add the renamed function arguments if present in extra_data
        if 'function_args' in extra_data:
             log_data['function_args'] = extra_data.pop('function_args') # Move it to log_data
        if 'result' in extra_data:
             log_data['result'] = extra_data.pop('result') # Move result too

        # Merge remaining extra data, ensuring serializability
        if extra_data:
            for key, value in extra_data.items():
                if key not in log_data:
                    try:
                        json.dumps({key: value}) # Test serialization
                        log_data[key] = value
                    except TypeError:
                        log_data[key] = f"[Unserializable Extra: {type(value).__name__}] {str(value)[:100]}"

        # Add thread-local context dictionary items
        log_context_dict = get_log_context()
        if log_context_dict:
            for key, value in log_context_dict.items():
                if key not in log_data:
                     try:
                        json.dumps({key: value})
                        log_data[key] = value
                     except TypeError:
                        log_data[key] = f"[Unserializable Context: {type(value).__name__}] {str(value)[:100]}"

        try:
            return json.dumps(log_data, default=str) # Use default=str as fallback for non-serializable types
        except Exception as dump_err:
            # Fallback if JSON dumping fails completely
            error_log = {
                 "timestamp": timestamp,
                 "level": "ERROR",
                 "logger": "logging.JsonFormatter",
                 "message": "Failed to serialize log record to JSON.",
                 "original_logger": record.name,
                 "original_level": record.levelname,
                 "error": str(dump_err),
                 "record_preview": str(record.__dict__)[:500]
            }
            return json.dumps(error_log)
</file>

<file path='app/core/log_handlers.py'>
import os
import logging
import json
import gzip
import time
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
from typing import Dict, Any, Optional, List
from datetime import datetime

class GzipRotatingFileHandler(RotatingFileHandler):
    """
    Extended version of RotatingFileHandler that compresses logs when they're rotated.
    """
    
    def __init__(self, filename, mode='a', maxBytes=0, backupCount=0, encoding=None, delay=0):
        """Initialize with standard RotatingFileHandler params."""
        super().__init__(
            filename, mode, maxBytes, backupCount, encoding, delay
        )
    
    def doRollover(self):
        """
        Override doRollover to compress the rotated file.
        """
        # Close the current file
        if self.stream:
            self.stream.close()
            self.stream = None
            
        # Rotate the files as normal
        if self.backupCount > 0:
            # Remove the oldest log file if it exists
            oldest_log = f"{self.baseFilename}.{self.backupCount}.gz"
            if os.path.exists(oldest_log):
                os.remove(oldest_log)
                
            # Shift each log file to the next number
            for i in range(self.backupCount - 1, 0, -1):
                source = f"{self.baseFilename}.{i}"
                if os.path.exists(source):
                    target = f"{self.baseFilename}.{i + 1}.gz"
                    with open(source, 'rb') as f_in:
                        with gzip.open(target, 'wb') as f_out:
                            f_out.writelines(f_in)
                    os.remove(source)
                # Also check if there's already a gzipped version
                source = f"{self.baseFilename}.{i}.gz"
                if os.path.exists(source):
                    target = f"{self.baseFilename}.{i + 1}.gz"
                    os.rename(source, target)
            
            # Compress the most recently rotated file
            source = self.baseFilename
            target = f"{self.baseFilename}.1.gz"
            
            if os.path.exists(self.baseFilename):
                with open(source, 'rb') as f_in:
                    with gzip.open(target, 'wb') as f_out:
                        f_out.writelines(f_in)
        
        # Create a new log file
        if not self.delay:
            self.stream = self._open()

class SizedTimedRotatingFileHandler(TimedRotatingFileHandler):
    """
    A combination of TimedRotatingFileHandler and RotatingFileHandler.
    Rotates logs based on both time and size.
    """
    
    def __init__(
        self, filename, when='h', interval=1, backupCount=0, encoding=None, 
        delay=False, utc=False, atTime=None, maxBytes=0
    ):
        """
        Initialize with standard TimedRotatingFileHandler params plus maxBytes.
        
        Args:
            maxBytes: Maximum file size in bytes before rotation
        """
        super().__init__(
            filename, when, interval, backupCount, encoding, delay, utc, atTime
        )
        self.maxBytes = maxBytes
    
    def shouldRollover(self, record):
        """
        Check if rollover should occur based on either time or size.
        
        Args:
            record: Log record
            
        Returns:
            True if rollover should occur, False otherwise
        """
        # Check if we should rotate based on time
        time_rotate = super().shouldRollover(record)
        
        # Check if we should rotate based on size
        size_rotate = False
        if self.maxBytes > 0:
            if self.stream is None:
                self.stream = self._open()
            if self.stream.tell() + self.computeRollover(record) >= self.maxBytes:
                size_rotate = True
                
        return time_rotate or size_rotate

class JsonLogFileHandler(logging.FileHandler):
    """
    A log handler that writes JSON formatted logs to a file.
    """
    
    def __init__(self, filename, mode='a', encoding=None, delay=False):
        """Initialize with standard FileHandler params."""
        super().__init__(filename, mode, encoding, delay)
    
    def format(self, record):
        """
        Format the record as JSON.
        
        Args:
            record: Log record
            
        Returns:
            JSON formatted log entry
        """
        log_data = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
            "process": record.process,
            "thread": record.thread
        }
        
        # Add exception info if present
        if record.exc_info:
            exception_type = record.exc_info[0].__name__ if record.exc_info[0] else "Unknown"
            exception_msg = str(record.exc_info[1]) if record.exc_info[1] else ""
            log_data["exception"] = {
                "type": exception_type,
                "message": exception_msg,
                "traceback": self.formatException(record.exc_info)
            }
        
        # Add any extra attributes from the record
        for key, value in record.__dict__.items():
            if key not in [
                'args', 'asctime', 'created', 'exc_info', 'exc_text', 'filename',
                'funcName', 'id', 'levelname', 'levelno', 'lineno', 'module',
                'msecs', 'message', 'msg', 'name', 'pathname', 'process',
                'processName', 'relativeCreated', 'stack_info', 'thread', 'threadName'
            ]:
                log_data[key] = value
        
        return json.dumps(log_data)

class LogBufferHandler(logging.Handler):
    """
    A handler that keeps a buffer of recent log records in memory.
    Useful for accessing recent logs programmatically.
    """
    
    def __init__(self, capacity=1000):
        """
        Initialize with buffer capacity.
        
        Args:
            capacity: Maximum number of log records to keep in buffer
        """
        super().__init__()
        self.capacity = capacity
        self.buffer = []
    
    def emit(self, record):
        """
        Add record to buffer.
        
        Args:
            record: Log record
        """
        self.buffer.append({
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage()
        })
        
        # Trim buffer if it exceeds capacity
        if len(self.buffer) > self.capacity:
            self.buffer = self.buffer[-self.capacity:]
    
    def get_logs(self, count=None, level=None, logger=None):
        """
        Get logs from buffer, optionally filtered.
        
        Args:
            count: Maximum number of logs to return
            level: Filter by log level
            logger: Filter by logger name
            
        Returns:
            List of log records
        """
        result = self.buffer
        
        if level:
            result = [r for r in result if r["level"] == level]
        
        if logger:
            result = [r for r in result if r["logger"] == logger]
        
        if count:
            result = result[-count:]
            
        return result
    
    def clear(self):
        """Clear the log buffer."""
        self.buffer = []
</file>

<file path='app/core/logging_config.py'>

# app/core/logging_config.py
import logging
import os
import sys
import uuid
import queue
from logging.handlers import RotatingFileHandler, QueueHandler, QueueListener, TimedRotatingFileHandler
from typing import Optional

# Import from new/refactored modules
from .log_formatters import JsonFormatter
# Context functions are now used implicitly by the formatter or middleware

# Global variable to hold the queue listener instance
_queue_listener: Optional[QueueListener] = None

def get_logger(name: str) -> logging.Logger:
    """Get a logger with the specified name."""
    # This function remains simple, just returns the logger instance.
    # Configuration is handled by setup_logging.
    logger = logging.getLogger(name)
    return logger

def setup_logging(log_level: Optional[int] = None, log_to_file: bool = True, log_dir: str = "/data/logs",
                  log_format: str = "json", async_logging: bool = True,
                  llm_trace_log_file: str = "llm_api_trace.log"):
    """
    Setup application logging using the specified configuration.
    """
    # Determine log level from environment or parameter
    if log_level is None:
        env = os.getenv("ENVIRONMENT", "development").lower()
        log_level_name = os.getenv("LOG_LEVEL", "INFO" if env == "production" else "DEBUG")
        try:
            log_level = getattr(logging, log_level_name.upper())
        except AttributeError:
            print(f"WARNING: Invalid LOG_LEVEL '{log_level_name}'. Defaulting to INFO.")
            log_level = logging.INFO

    # Validate or create log directory
    effective_log_dir = None
    if log_to_file:
        if not log_dir:
            print("ERROR: log_to_file is True, but log_dir is not specified. Disabling file logging.")
            log_to_file = False
        else:
            try:
                abs_log_dir = os.path.abspath(log_dir)
                os.makedirs(abs_log_dir, exist_ok=True)
                # Test write permissions
                test_file_path = os.path.join(abs_log_dir, f"startup_test_{uuid.uuid4()}.log")
                with open(test_file_path, "w") as f:
                    f.write("Write test successful.")
                os.remove(test_file_path)
                effective_log_dir = abs_log_dir
                print(f"Logging directory verified: {effective_log_dir}")
            except OSError as e:
                print(f"ERROR: Could not create or write to logging directory {abs_log_dir}: {e}. Disabling file logging.")
                log_to_file = False
            except Exception as e:
                 print(f"ERROR: Unexpected error verifying logging directory {abs_log_dir}: {e}. Disabling file logging.")
                 log_to_file = False

    # Configure the root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)

    # Clear any existing handlers to prevent duplicates
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
        if hasattr(handler, 'close') and callable(handler.close):
             try: handler.close()
             except Exception as close_err: print(f"Error closing handler: {close_err}")

    # Select formatter based on format
    if log_format.lower() == 'json':
        formatter = JsonFormatter() # Use the imported formatter
    else:
        # Basic formatter as fallback (though JSON is recommended)
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S,%f'
        )

    handlers = []

    # Create console handler (always add this one)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter) # Use selected formatter for console
    handlers.append(console_handler)

    # Create file handler if requested and directory is valid
    if log_to_file and effective_log_dir:
        try:
            main_log_path = os.path.join(effective_log_dir, "vendor_classification.log")
            print(f"Setting up main log file at: {main_log_path}")
            file_handler = RotatingFileHandler(
                main_log_path,
                maxBytes=10 * 1024 * 1024,  # 10 MB
                backupCount=10
            )
            file_handler.setFormatter(formatter)
            handlers.append(file_handler)

            # Create an error log file that only contains ERROR and higher
            error_log_path = os.path.join(effective_log_dir, "errors.log")
            print(f"Setting up error log file at: {error_log_path}")
            error_file_handler = RotatingFileHandler(
                error_log_path,
                maxBytes=10 * 1024 * 1024,  # 10 MB
                backupCount=10
            )
            error_file_handler.setFormatter(formatter)
            error_file_handler.setLevel(logging.ERROR)
            handlers.append(error_file_handler)

        except Exception as file_handler_err:
            print(f"ERROR: Failed to create file handlers in {effective_log_dir}: {file_handler_err}. File logging disabled.")
            handlers = [h for h in handlers if not isinstance(h, RotatingFileHandler)]
            log_to_file = False # Disable flag if handlers failed

    # --- Async Logging Setup ---
    global _queue_listener
    if _queue_listener:
        print("Stopping existing queue listener...")
        try:
            _queue_listener.stop()
            _queue_listener = None
        except Exception as stop_err:
             print(f"Error stopping existing listener: {stop_err}")

    if async_logging:
        log_queue = queue.Queue(-1) # Use an infinite queue size
        queue_handler = QueueHandler(log_queue)
        root_logger.addHandler(queue_handler) # Add ONLY the queue handler to root
        _queue_listener = QueueListener(log_queue, *handlers, respect_handler_level=True)
        _queue_listener.start()
        print("Async logging configured with QueueListener.")
    else:
        for handler in handlers:
            root_logger.addHandler(handler)
        print("Synchronous logging configured.")
    # --- End Async Logging Setup ---

    # --- LLM TRACE LOGGING SETUP ---
    if log_to_file and effective_log_dir and llm_trace_log_file:
        try:
            llm_trace_logger = logging.getLogger("llm_api_trace")
            llm_trace_logger.setLevel(logging.DEBUG) # Log everything for trace
            llm_trace_logger.propagate = False # Don't send to root logger

            for handler in llm_trace_logger.handlers[:]:
                llm_trace_logger.removeHandler(handler)
                if hasattr(handler, 'close') and callable(handler.close):
                    try: handler.close()
                    except Exception as close_err: print(f"Error closing existing trace handler: {close_err}")

            llm_trace_log_path = os.path.join(effective_log_dir, llm_trace_log_file)
            print(f"Setting up LLM trace log at: {llm_trace_log_path}")
            llm_trace_formatter = JsonFormatter() # Use the same JSON formatter
            llm_trace_handler = TimedRotatingFileHandler(
                 llm_trace_log_path, when='midnight', interval=1, backupCount=7
            )
            llm_trace_handler.setFormatter(llm_trace_formatter)
            llm_trace_logger.addHandler(llm_trace_handler)
            root_logger.info(f"LLM API trace logging initialized", extra={"file": llm_trace_log_path})
            llm_trace_logger.info("LLM_TRACE: Initialization successful.", extra={'correlation_id': 'startup'})
        except Exception as e:
            root_logger.error("Failed to initialize LLM API trace logging", exc_info=True)
            print(f"ERROR: Failed to initialize LLM API trace logging: {e}")
    elif log_to_file:
         root_logger.warning("LLM API trace logging disabled because file logging is disabled or trace filename is empty.")
    # --- END LLM TRACE LOGGING SETUP ---

    # Set specific log levels for noisy libraries
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
    logging.getLogger("celery").setLevel(logging.INFO)

    # Log setup completion
    root_logger.info(
        "Logging setup complete",
        extra={
            "log_level": logging.getLevelName(log_level),
            "log_dir": effective_log_dir if log_to_file else None,
            "handlers": [h.__class__.__name__ + (f' ({h.baseFilename})' if hasattr(h, 'baseFilename') else '') for h in handlers],
            "async_mode": async_logging
        }
    )

    return root_logger
</file>

<file path='app/middleware/__init__.py'>
# Import middleware classes to make them available when importing from the package
from .logging_middleware import RequestLoggingMiddleware, RequestBodyLoggingMiddleware, log_request_middleware

__all__ = ['RequestLoggingMiddleware', 'RequestBodyLoggingMiddleware', 'log_request_middleware']
</file>

<file path='app/middleware/db_logging_middleware.py'>
import time
import logging
import sqlalchemy
from sqlalchemy import event
from sqlalchemy.engine import Engine

from core.logging_config import get_logger, set_log_context, LogTimer, get_correlation_id

logger = get_logger("vendor_classification.database")

class SQLQueryLoggingMiddleware:
    """Middleware for logging SQL queries."""
    
    def __init__(self, engine):
        """Initialize the middleware with an SQLAlchemy engine."""
        self.engine = engine
        self.register_events()
        logger.info("SQL query logging middleware initialized")
    
    def register_events(self):
        """Register SQLAlchemy event listeners."""
        event.listen(self.engine, "before_cursor_execute", self.before_cursor_execute)
        event.listen(self.engine, "after_cursor_execute", self.after_cursor_execute)
        event.listen(self.engine, "handle_error", self.handle_error)
        logger.debug("SQLAlchemy event listeners registered")
    
    def before_cursor_execute(self, conn, cursor, statement, parameters, context, executemany):
        """Log query before execution and store start time."""
        conn.info.setdefault('query_start_time', []).append(time.time())
        
        # Truncate long queries for readability
        statement_str = str(statement)
        if len(statement_str) > 1000:
            statement_str = statement_str[:997] + "..."
        
        # Format parameters safely for logging
        params_str = str(parameters)
        if len(params_str) > 500:
            params_str = params_str[:497] + "..."
        
        logger.debug(
            f"Executing SQL query", 
            extra={
                "statement": statement_str,
                "parameters": params_str,
                "executemany": executemany,
                "connection_id": id(conn),
                "correlation_id": get_correlation_id()
            }
        )
    
    def after_cursor_execute(self, conn, cursor, statement, parameters, context, executemany):
        """Log query completion and execution time."""
        if not conn.info.get('query_start_time'):
            return
        
        start_time = conn.info['query_start_time'].pop(-1)
        total_time = time.time() - start_time
        
        # Get result details safely
        row_count = -1
        try:
            row_count = cursor.rowcount
        except:
            pass
        
        # Store query timing in thread-local storage for stats
        statement_type = statement.split(' ')[0].upper() if isinstance(statement, str) else "UNKNOWN"
        
        logger.debug(
            f"SQL query completed", 
            extra={
                "duration": total_time,
                "row_count": row_count,
                "statement_type": statement_type,
                "connection_id": id(conn),
                "correlation_id": get_correlation_id()
            }
        )
        
        # Add to performance stats
        set_log_context({f"db_{statement_type.lower()}_count": 1})
        set_log_context({f"db_{statement_type.lower()}_time": total_time})
    
    def handle_error(self, context):
        """Log database errors."""
        logger.error(
            f"Database error occurred", 
            exc_info=context.original_exception,
            extra={
                "statement": str(context.statement) if context.statement else None,
                "parameters": str(context.parameters) if context.parameters else None,
                "correlation_id": get_correlation_id()
            }
        )

def setup_db_logging(engine=None):
    """Set up database query logging."""
    if engine is None:
        # Get all engines if specific one not provided
        for instance in Engine.__subclasses__():
            for engine in instance.instances:
                SQLQueryLoggingMiddleware(engine)
    else:
        SQLQueryLoggingMiddleware(engine)
    
    logger.info("Database query logging initialized")
</file>

<file path='app/middleware/logging_middleware.py'>

# app/middleware/logging_middleware.py
import time
import json
import uuid
import logging # <<< ADDED IMPORT
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp, Receive, Scope, Send

# Import context functions from the new module
from core.log_context import (
    set_request_id, get_request_id, set_correlation_id, get_correlation_id,
    set_user, set_log_context, clear_all_context
)
# Import logger and log helpers
from core.logging_config import get_logger
from utils.log_utils import log_duration

# Configure logger for this module
logger = get_logger("vendor_classification.middleware")
# --- ADDED: Log confirmation ---
logger.debug("Successfully imported standard logging module.")
# --- END ADDED ---

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """Middleware to log all requests and responses."""

    async def dispatch(self, request: Request, call_next):
        # Generate or extract request ID
        request_id = request.headers.get("X-Request-ID") or str(uuid.uuid4())
        # Set request ID and correlation ID
        set_request_id(request_id)
        correlation_id = request.headers.get("X-Correlation-ID")
        if correlation_id:
            set_correlation_id(correlation_id)
        else:
            set_correlation_id(request_id)  # Use request_id as correlation_id if none provided

        # Add context data
        set_log_context({
            "client_ip": request.client.host if request.client else None,
            "user_agent": request.headers.get("user-agent"),
            "path": request.url.path,
            "method": request.method,
        })

        # Log request
        logger.info(
            f"Request started: {request.method} {request.url.path}",
            extra={
                "request_id": request_id,
                "query_params": dict(request.query_params),
                "path_params": request.path_params,
                "headers": { k.lower(): v for k, v in request.headers.items() }
            }
        )

        # Process the request and measure time
        start_time = time.time()
        try:
            response = await call_next(request)

            # Log response
            process_time = time.time() - start_time
            logger.info(
                f"Request completed: {request.method} {request.url.path}",
                extra={
                    "status_code": response.status_code,
                    "duration": process_time,
                    "response_headers": {
                        k.lower(): v for k, v in response.headers.items()
                    }
                }
            )

            # Add the request ID to response headers
            response.headers["X-Request-ID"] = request_id

            return response

        except Exception as exc:
            # Log any unhandled exceptions
            process_time = time.time() - start_time
            logger.exception(
                f"Request failed: {request.method} {request.url.path}",
                extra={
                    "duration": process_time,
                    "error": str(exc)
                }
            )
            raise
        finally:
            # Clear context data to prevent leaks between requests
            clear_all_context()


class RequestBodyLoggingMiddleware(BaseHTTPMiddleware):
    """
    Middleware to log request and response bodies.
    Note: Use only in development/debugging.
    """

    def __init__(
        self,
        app: ASGIApp,
        exclude_paths: list = None,
        max_body_size: int = 10000
    ):
        super().__init__(app)
        self.exclude_paths = exclude_paths or ["/health", "/metrics", "/static"]
        self.max_body_size = max_body_size

    async def dispatch(self, request: Request, call_next):
        # Skip excluded paths
        if any(request.url.path.startswith(path) for path in self.exclude_paths):
            return await call_next(request)

        # Get request ID
        request_id = get_request_id() or str(uuid.uuid4())

        # Clone the request to access the body
        copied_body = await self._get_request_body(request)

        if copied_body:
            body_str = self._get_body_str(copied_body)
            logger.debug(
                f"Request body: {request.method} {request.url.path}",
                extra={
                    "request_id": request_id,
                    "request_body": body_str
                }
            )

        # Process the request
        response = await call_next(request)

        # Try to get response body for certain content types
        if response.status_code != 204 and hasattr(response, "body"):
            body = response.body
            if body:
                body_str = self._get_body_str(body)
                logger.debug(
                    f"Response body: {request.method} {request.url.path}",
                    extra={
                        "request_id": request_id,
                        "response_body": body_str,
                        "status_code": response.status_code
                    }
                )

        return response

    def _get_body_str(self, body: bytes) -> str:
        """Convert body bytes to string, truncating if needed."""
        if len(body) > self.max_body_size:
            return f"{body[:self.max_body_size].decode('utf-8', errors='replace')}... [truncated]"

        try:
            body_str = body.decode('utf-8')
            # Try to pretty-print JSON
            try:
                json_body = json.loads(body_str)
                body_str = json.dumps(json_body, indent=2)
            except:
                pass
            return body_str
        except:
            return f"[binary data, length: {len(body)}]"

    async def _get_request_body(self, request: Request) -> bytes:
        """Get and restore the request body."""
        body = await request.body()
        # Reset the request body
        async def receive():
            return {"type": "http.request", "body": body}
        request._receive = receive
        return body


async def log_request_middleware(request: Request, call_next) -> Response:
    """
    Alternative logging middleware function that can be used with middleware decorator.
    """
    request_id = request.headers.get("X-Request-ID") or str(uuid.uuid4())
    set_request_id(request_id)
    correlation_id = request.headers.get("X-Correlation-ID")
    if correlation_id:
        set_correlation_id(correlation_id)
    else:
        set_correlation_id(request_id)

    # Add context data
    set_log_context({
        "client_ip": request.client.host if request.client else None,
        "user_agent": request.headers.get("user-agent"),
        "path": request.url.path,
        "method": request.method,
    })

    # Log request
    logger.info(
        f"Request started: {request.method} {request.url.path}",
        extra={
            "request_id": request_id,
            "query_params": dict(request.query_params),
            "path_params": request.path_params,
        }
    )

    try:
        with log_duration(logger, f"Request {request.method} {request.url.path}",
                         level=logging.INFO, include_in_stats=True): # Use INFO level for request duration
            # Process the request
            response = await call_next(request)

        # Log response summary
        logger.info(
            f"Request completed: {request.method} {request.url.path}",
            extra={
                "status_code": response.status_code,
            }
        )

        # Add the request ID to response headers
        response.headers["X-Request-ID"] = request_id

        return response

    except Exception as e:
        logger.exception(f"Request failed: {request.method} {request.url.path}")
        raise
    finally:
        # Clear context data to prevent leaks between requests
        clear_all_context()
</file>

<file path='app/models/classification.py'>
# <file path='app/models/classification.py'>
# --- file path='app/models/classification.py' ---
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any

class VendorClassification(BaseModel):
    """Vendor classification model."""
    vendor_name: str
    category_id: str
    category_name: str
    confidence: float = Field(ge=0.0, le=1.0)
    notes: Optional[str] = None
    classification_not_possible: bool = False
    classification_not_possible_reason: Optional[str] = None
    sources: Optional[List[Dict[str, str]]] = None
    classification_source: Optional[str] = None # e.g., 'Initial', 'Search'

class ClassificationBatchResponse(BaseModel):
    """Response model for classification batch (expected from LLM)."""
    # --- MODIFIED: Allow level up to 5 ---
    level: int = Field(ge=1, le=5)
    # --- END MODIFIED ---
    batch_id: str
    parent_category_id: Optional[str] = None
    classifications: List[VendorClassification] # LLM should return this structure

class ApiUsage(BaseModel):
    """API usage statistics."""
    # Field names match the keys used in the stats dictionary
    openrouter_calls: int = 0
    openrouter_prompt_tokens: int = 0
    openrouter_completion_tokens: int = 0
    openrouter_total_tokens: int = 0
    tavily_search_calls: int = 0
    cost_estimate_usd: float = 0.0

class ProcessingStats(BaseModel):
    """Processing statistics for a job (stored in Job.stats JSON)."""
    job_id: str
    company_name: str
    start_time: Any # Can be datetime or ISO string
    end_time: Optional[Any] = None
    processing_duration_seconds: Optional[float] = None
    total_vendors: int = 0
    unique_vendors: int = 0
    # --- UPDATED/ADDED Fields ---
    successfully_classified_l4: int = 0 # Keep L4 count for reference/comparison
    successfully_classified_l5: int = 0 # NEW: Total vendors reaching L5 (initial or post-search)
    classification_not_possible_initial: int = 0 # Vendors needing search initially
    invalid_category_errors: int = 0 # Count of times LLM returned invalid category ID
    search_attempts: int = 0 # How many vendors triggered the search path
    search_successful_classifications_l1: int = 0 # Vendors getting L1 via search
    search_successful_classifications_l5: int = 0 # NEW: Vendors getting L5 via search path
    # --- END UPDATED/ADDED ---
    api_usage: ApiUsage = Field(default_factory=ApiUsage)

</file>

<file path='app/models/job.py'>

# <file path='app/models/job.py'>
# --- file path='app/models/job.py' ---
from sqlalchemy import Column, String, Float, DateTime, Enum as SQLEnum, JSON, Text, Integer # <<< ADDED Integer
from sqlalchemy.sql import func
from sqlalchemy.orm import Session # <<< ADDED IMPORT FOR TYPE HINTING
from enum import Enum as PyEnum
from datetime import datetime
from typing import Optional, Dict, Any

from core.database import Base

class JobStatus(str, PyEnum):
    """Job status enum."""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingStage(str, PyEnum):
    """Processing stage enum."""
    INGESTION = "ingestion"
    NORMALIZATION = "normalization"
    CLASSIFICATION_L1 = "classification_level_1"
    CLASSIFICATION_L2 = "classification_level_2"
    CLASSIFICATION_L3 = "classification_level_3"
    CLASSIFICATION_L4 = "classification_level_4"
    CLASSIFICATION_L5 = "classification_level_5"
    SEARCH = "search_unknown_vendors" # This stage now covers search AND recursive post-search classification
    RESULT_GENERATION = "result_generation"

class Job(Base):
    """Job model for tracking classification jobs."""

    __tablename__ = "jobs"

    id = Column(String, primary_key=True, index=True)
    company_name = Column(String, nullable=False)
    input_file_name = Column(String, nullable=False)
    output_file_name = Column(String, nullable=True)
    status = Column(String, default=JobStatus.PENDING.value)
    current_stage = Column(String, default=ProcessingStage.INGESTION.value)
    progress = Column(Float, default=0.0)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now(), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    notification_email = Column(String, nullable=True)
    error_message = Column(Text, nullable=True)
    stats = Column(JSON, default={}) # Structure defined by ProcessingStats model
    created_by = Column(String, nullable=False)
    # --- ADDED: Target Level ---
    target_level = Column(Integer, nullable=False, default=5) # Store the desired classification depth (1-5)
    # --- END ADDED ---

    def update_progress(self, progress: float, stage: ProcessingStage, db_session: Optional[Session] = None): # Type hint now valid
        """Update job progress and stage, optionally committing."""
        self.progress = progress
        self.current_stage = stage.value
        self.updated_at = datetime.now()
        # Optionally commit immediately if session provided
        if db_session:
            try:
                db_session.commit()
            except Exception as e:
                from core.logging_config import get_logger # Local import for safety
                logger = get_logger("vendor_classification.job_model")
                logger.error(f"Failed to commit progress update for job {self.id}", exc_info=True)
                db_session.rollback()


    def complete(self, output_file_name: str, stats: Dict[str, Any]):
        """Mark job as completed."""
        self.status = JobStatus.COMPLETED.value
        self.progress = 1.0
        self.current_stage = ProcessingStage.RESULT_GENERATION.value # Ensure stage reflects completion
        self.output_file_name = output_file_name
        self.completed_at = datetime.now()
        self.stats = stats
        self.updated_at = self.completed_at # Align updated_at with completed_at

    def fail(self, error_message: str):
        """Mark job as failed."""
        self.status = JobStatus.FAILED.value
        # Optionally set progress to 1.0 or leave as is upon failure
        # self.progress = 1.0
        self.error_message = error_message
        self.updated_at = datetime.now()
        # Ensure completed_at is Null if it failed
        self.completed_at = None
</file>

<file path='app/models/taxonomy.py'>
# <file path='app/models/taxonomy.py'>
# --- file path='app/models/taxonomy.py' ---
from pydantic import BaseModel, Field
from typing import Dict, List, Optional
import re # Added import

# --- ADDED: Import logger ---
from core.logging_config import get_logger
logger = get_logger("vendor_classification.taxonomy_model")
# --- END ADDED ---


class TaxonomyCategory(BaseModel):
    """Base taxonomy category model."""
    id: str
    name: str
    description: Optional[str] = None

# --- ADDED: Level 5 Model ---
class TaxonomyLevel5(TaxonomyCategory):
    """Level 5 taxonomy category (most specific - typically 6 digits)."""
    pass
# --- END ADDED ---

class TaxonomyLevel4(TaxonomyCategory):
    """Level 4 taxonomy category (typically 5 digits)."""
    # --- MODIFIED: Add children for Level 5 ---
    children: Dict[str, TaxonomyLevel5] = Field(default_factory=dict)
    # --- END MODIFIED ---

class TaxonomyLevel3(TaxonomyCategory):
    """Level 3 taxonomy category."""
    children: Dict[str, TaxonomyLevel4] = Field(default_factory=dict)

class TaxonomyLevel2(TaxonomyCategory):
    """Level 2 taxonomy category."""
    children: Dict[str, TaxonomyLevel3] = Field(default_factory=dict)

class TaxonomyLevel1(TaxonomyCategory):
    """Level 1 taxonomy category (most general)."""
    children: Dict[str, TaxonomyLevel2] = Field(default_factory=dict)

class Taxonomy(BaseModel):
    """Complete taxonomy model."""
    name: str
    version: str
    description: Optional[str] = None
    categories: Dict[str, TaxonomyLevel1] = Field(default_factory=dict)

    def get_level1_categories(self) -> List[TaxonomyCategory]:
        """Get all level 1 categories."""
        logger.debug(f"get_level1_categories: Retrieving {len(self.categories)} L1 categories.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in self.categories.items()
        ]

    def get_level2_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 2 categories for a given parent."""
        logger.debug(f"get_level2_categories: Attempting to get children for L1 parent '{parent_id}'.")
        if parent_id not in self.categories:
            logger.warning(f"get_level2_categories: Parent ID '{parent_id}' not found in L1 categories.")
            return []

        level1_cat = self.categories[parent_id]
        if not hasattr(level1_cat, 'children') or not level1_cat.children:
             logger.warning(f"get_level2_categories: Parent L1 category '{parent_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level1_cat.children)
        logger.debug(f"get_level2_categories: Found {children_count} L2 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level1_cat.children.items()
        ]

    def get_level3_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 3 categories for a given parent ID (expected format: L1.L2 or just L2 ID)."""
        logger.debug(f"get_level3_categories: Attempting to get children for L2 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 2:
                level1_id, level2_id = parts[0], parts[1]
            else:
                 logger.error(f"get_level3_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2' or 'L2'.")
                 return []
        else:
            # Assume it's just the L2 ID - need to find its L1 parent
            level2_id = parent_id
            for l1_key, l1_node in self.categories.items():
                if level2_id in getattr(l1_node, 'children', {}):
                    level1_id = l1_key
                    break
            if not level1_id:
                logger.warning(f"get_level3_categories: Could not find L1 parent for L2 ID '{level2_id}'.")
                return []

        logger.debug(f"get_level3_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}'.")

        if level1_id not in self.categories:
            logger.warning(f"get_level3_categories: L1 parent ID '{level1_id}' not found.")
            return []

        level1_cat = self.categories[level1_id]
        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
             logger.warning(f"get_level3_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
             return []

        level2_cat = level1_cat.children[level2_id]
        if not hasattr(level2_cat, 'children') or not level2_cat.children:
             logger.warning(f"get_level3_categories: Parent L2 category '{level2_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level2_cat.children)
        logger.debug(f"get_level3_categories: Found {children_count} L3 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level2_cat.children.items()
        ]

    def get_level4_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 4 categories for a given parent ID (expected format: L1.L2.L3 or just L3 ID)."""
        logger.debug(f"get_level4_categories: Attempting to get children for L3 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        level3_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 3:
                level1_id, level2_id, level3_id = parts[0], parts[1], parts[2]
            else:
                 logger.error(f"get_level4_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2.L3' or 'L3'.")
                 return []
        else:
            # Assume it's just the L3 ID - need to find its L1/L2 parents
            level3_id = parent_id
            found = False
            for l1_key, l1_node in self.categories.items():
                for l2_key, l2_node in getattr(l1_node, 'children', {}).items():
                    if level3_id in getattr(l2_node, 'children', {}):
                        level1_id = l1_key
                        level2_id = l2_key
                        found = True
                        break
                if found:
                    break
            if not found:
                 logger.warning(f"get_level4_categories: Could not find L1/L2 parents for L3 ID '{level3_id}'.")
                 return []

        logger.debug(f"get_level4_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}', L3='{level3_id}'.")

        if level1_id not in self.categories:
            logger.warning(f"get_level4_categories: L1 parent ID '{level1_id}' not found.")
            return []

        level1_cat = self.categories[level1_id]
        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
            logger.warning(f"get_level4_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
            return []

        level2_cat = level1_cat.children[level2_id]
        if not hasattr(level2_cat, 'children') or level3_id not in level2_cat.children:
            logger.warning(f"get_level4_categories: L3 parent ID '{level3_id}' not found under L2 '{level2_id}'.")
            return []

        level3_cat = level2_cat.children[level3_id]
        if not hasattr(level3_cat, 'children') or not level3_cat.children:
             logger.warning(f"get_level4_categories: Parent L3 category '{level3_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level3_cat.children)
        logger.debug(f"get_level4_categories: Found {children_count} L4 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level3_cat.children.items()
        ]

    # --- ADDED: get_level5_categories ---
    def get_level5_categories(self, parent_id: str) -> List[TaxonomyCategory]:
        """Get level 5 categories for a given parent ID (expected format: L1.L2.L3.L4 or just L4 ID)."""
        logger.debug(f"get_level5_categories: Attempting to get children for L4 parent '{parent_id}'.")
        level1_id = None
        level2_id = None
        level3_id = None
        level4_id = None
        if '.' in parent_id:
            parts = parent_id.split('.')
            if len(parts) == 4:
                level1_id, level2_id, level3_id, level4_id = parts[0], parts[1], parts[2], parts[3]
            else:
                 logger.error(f"get_level5_categories: Invalid parent ID format '{parent_id}'. Expected 'L1.L2.L3.L4' or 'L4'.")
                 return []
        else:
            # Assume it's just the L4 ID - need to find its parents
            level4_id = parent_id
            found = False
            for l1_key, l1_node in self.categories.items():
                for l2_key, l2_node in getattr(l1_node, 'children', {}).items():
                    for l3_key, l3_node in getattr(l2_node, 'children', {}).items():
                        if level4_id in getattr(l3_node, 'children', {}):
                            level1_id = l1_key
                            level2_id = l2_key
                            level3_id = l3_key
                            found = True
                            break
                    if found: break
                if found: break
            if not found:
                 logger.warning(f"get_level5_categories: Could not find L1/L2/L3 parents for L4 ID '{level4_id}'.")
                 return []

        logger.debug(f"get_level5_categories: Parsed parent ID into L1='{level1_id}', L2='{level2_id}', L3='{level3_id}', L4='{level4_id}'.")

        # Traverse the hierarchy
        if level1_id not in self.categories:
            logger.warning(f"get_level5_categories: L1 parent ID '{level1_id}' not found.")
            return []
        level1_cat = self.categories[level1_id]

        if not hasattr(level1_cat, 'children') or level2_id not in level1_cat.children:
            logger.warning(f"get_level5_categories: L2 parent ID '{level2_id}' not found under L1 '{level1_id}'.")
            return []
        level2_cat = level1_cat.children[level2_id]

        if not hasattr(level2_cat, 'children') or level3_id not in level2_cat.children:
            logger.warning(f"get_level5_categories: L3 parent ID '{level3_id}' not found under L2 '{level2_id}'.")
            return []
        level3_cat = level2_cat.children[level3_id]

        if not hasattr(level3_cat, 'children') or level4_id not in level3_cat.children:
            logger.warning(f"get_level5_categories: L4 parent ID '{level4_id}' not found under L3 '{level3_id}'.")
            return []
        level4_cat = level3_cat.children[level4_id]

        # Get L5 children
        if not hasattr(level4_cat, 'children') or not level4_cat.children:
             logger.warning(f"get_level5_categories: Parent L4 category '{level4_id}' has no children dictionary or it is empty.")
             return []

        children_count = len(level4_cat.children)
        logger.debug(f"get_level5_categories: Found {children_count} L5 children for parent '{parent_id}'.")
        return [
            TaxonomyCategory(id=cat_id, name=cat.name, description=cat.description)
            for cat_id, cat in level4_cat.children.items()
        ]
    # --- END ADDED ---
</file>

<file path='app/models/user.py'>
from sqlalchemy import Column, String, Boolean, DateTime
from sqlalchemy.sql import func

from core.database import Base

class User(Base):
    """User model for authentication."""
    
    __tablename__ = "users"
    
    id = Column(String, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    email = Column(String, unique=True, index=True, nullable=False)
    full_name = Column(String, nullable=True)
    hashed_password = Column(String, nullable=False)
    is_active = Column(Boolean, default=True)
    is_superuser = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now(), server_default=func.now())

</file>

<file path='app/schemas/job.py'>

# app/schemas/job.py
from pydantic import BaseModel, Field # <<< ADDED Field
from datetime import datetime
from typing import Optional, Dict, Any

from models.job import JobStatus, ProcessingStage # Import enums from model

class JobResponse(BaseModel):
    """Schema for returning job information."""
    id: str
    company_name: str
    status: JobStatus # Use the enum
    progress: float
    current_stage: ProcessingStage # Use the enum
    created_at: datetime
    updated_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    output_file_name: Optional[str] = None
    input_file_name: str
    created_by: str
    error_message: Optional[str] = None
    # --- ADDED: Target Level ---
    target_level: int = Field(..., ge=1, le=5) # Include target level in response
    # --- END ADDED ---
    # stats: Optional[Dict[str, Any]] = None # Optionally include stats summary

    class Config:
        from_attributes = True # Pydantic v2 way to enable ORM mode
        # orm_mode = True # Pydantic v1 way
</file>

<file path='app/schemas/user.py'>
# app/schemas/user.py
from pydantic import BaseModel, EmailStr, Field
from typing import Optional
from datetime import datetime
import uuid # Import uuid

# --- Base User Info ---
class UserBase(BaseModel):
    email: EmailStr
    full_name: Optional[str] = None
    is_active: Optional[bool] = True
    is_superuser: Optional[bool] = False
    username: str = Field(..., min_length=3, max_length=50)

# --- Properties to receive via API on creation ---
class UserCreate(UserBase):
    password: str = Field(..., min_length=8)

# --- Properties to receive via API on update ---
class UserUpdate(BaseModel):
    email: Optional[EmailStr] = None
    full_name: Optional[str] = None
    password: Optional[str] = Field(None, min_length=8) # Allow password update
    is_active: Optional[bool] = None
    is_superuser: Optional[bool] = None

# --- Properties stored in DB ---
# This is technically represented by the User model itself,
# but can be useful for internal representation if needed.
class UserInDBBase(UserBase):
    id: uuid.UUID # Use UUID for ID
    hashed_password: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True # Pydantic v2 way
        # orm_mode = True # Pydantic v1 way

# --- Properties to return to client ---
class UserResponse(UserBase):
    id: uuid.UUID # Use UUID for ID
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True # Pydantic v2 way
        # orm_mode = True # Pydantic v1 way
</file>

<file path='app/services/file_service.py'>

# app/services/file_service.py
import os
import pandas as pd
from fastapi import UploadFile
import shutil
from typing import List, Dict, Any, Optional, Set
import uuid
import logging
from datetime import datetime

from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import get_logger
from core.log_context import set_log_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call

# Configure logger
logger = get_logger("vendor_classification.file_service")

# --- Define expected column names (case-insensitive matching) ---
VENDOR_NAME_COL = 'vendor_name'
OPTIONAL_EXAMPLE_COL = 'optional_example_good_serviced_purchased'
OPTIONAL_ADDRESS_COL = 'vendor_address'
OPTIONAL_WEBSITE_COL = 'vendor_website'
OPTIONAL_INTERNAL_CAT_COL = 'internal_category'
OPTIONAL_PARENT_CO_COL = 'parent_company'
OPTIONAL_SPEND_CAT_COL = 'spend_category'
# --- End Define expected column names ---

@log_function_call(logger, include_args=False) # Keep args=False for UploadFile
def save_upload_file(file: UploadFile, job_id: str) -> str:
    """
    Save uploaded file to the input directory.
    """
    job_dir = os.path.join(settings.INPUT_DATA_DIR, job_id)
    try:
        os.makedirs(job_dir, exist_ok=True)
        logger.debug(f"Ensured job directory exists", extra={"directory": job_dir})
    except OSError as e:
        logger.error(f"Failed to create job directory", exc_info=True, extra={"directory": job_dir})
        raise IOError(f"Could not create directory for job {job_id}: {e}")

    safe_filename = os.path.basename(file.filename or f"upload_{job_id}.tmp")
    if not safe_filename:
         safe_filename = f"upload_{job_id}.tmp"

    file_path = os.path.join(job_dir, safe_filename)
    logger.info("Attempting to save file", extra={"path": file_path})

    with LogTimer(logger, "File saving"):
        try:
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
        except Exception as e:
            logger.error("Failed to save uploaded file content", exc_info=True, extra={"file_path": file_path})
            if os.path.exists(file_path):
                 try: os.remove(file_path)
                 except OSError: logger.warning("Could not remove partially written file on error.", extra={"file_path": file_path})
            raise IOError(f"Could not save uploaded file content: {e}")
        finally:
            if hasattr(file, 'close') and callable(file.close):
                file.close()

    try:
        file_size = os.path.getsize(file_path)
        logger.info(f"File saved successfully",
                   extra={"path": file_path, "size_bytes": file_size})
    except OSError as e:
        logger.warning(f"Could not get size of saved file", exc_info=False, extra={"file_path": file_path, "error": str(e)})
        file_size = -1

    return file_path


@log_function_call(logger)
def read_vendor_file(file_path: str) -> List[Dict[str, Any]]:
    """
    Read vendor data from Excel file, looking for mandatory 'vendor_name'
    and several optional context columns (case-insensitively).
    """
    logger.info(f"Reading Excel file for vendor data", extra={"file_path": file_path})

    if not os.path.exists(file_path):
         logger.error(f"Input file not found at path", extra={"file_path": file_path})
         raise FileNotFoundError(f"Input file not found at path: {file_path}")

    with LogTimer(logger, "Excel file reading", include_in_stats=True):
        try:
            df = pd.read_excel(file_path, header=0)
            detected_columns = list(df.columns)
            logger.debug(f"Successfully read Excel file. Columns detected: {detected_columns}")
        except Exception as e:
            logger.error(f"Error reading Excel file with pandas", exc_info=True,
                        extra={"file_path": file_path})
            raise ValueError(f"Could not parse the Excel file. Please ensure it is a valid .xlsx or .xls file. Error details: {str(e)}")

    # --- Find columns case-insensitively ---
    column_map: Dict[str, Optional[str]] = {
        'vendor_name': None,
        'example': None,
        'address': None,
        'website': None,
        'internal_cat': None,
        'parent_co': None,
        'spend_cat': None
    }
    normalized_detected_columns = {str(col).strip().lower(): str(col) for col in detected_columns if isinstance(col, str)}

    # Find vendor_name (mandatory)
    if VENDOR_NAME_COL in normalized_detected_columns:
        column_map['vendor_name'] = normalized_detected_columns[VENDOR_NAME_COL]
        logger.info(f"Found mandatory column '{VENDOR_NAME_COL}' as: '{column_map['vendor_name']}'")
    else:
        logger.error(f"Required column '{VENDOR_NAME_COL}' not found in file.",
                    extra={"available_columns": detected_columns})
        raise ValueError(f"Input Excel file must contain a column named '{VENDOR_NAME_COL}' (case-insensitive). Found columns: {', '.join(map(str, detected_columns))}")

    # Find optional columns
    optional_cols = {
        'example': OPTIONAL_EXAMPLE_COL,
        'address': OPTIONAL_ADDRESS_COL,
        'website': OPTIONAL_WEBSITE_COL,
        'internal_cat': OPTIONAL_INTERNAL_CAT_COL,
        'parent_co': OPTIONAL_PARENT_CO_COL,
        'spend_cat': OPTIONAL_SPEND_CAT_COL
    }
    for key, col_name in optional_cols.items():
        if col_name in normalized_detected_columns:
            column_map[key] = normalized_detected_columns[col_name]
            logger.info(f"Found optional column '{col_name}' as: '{column_map[key]}'")
        else:
            logger.info(f"Optional column '{col_name}' not found.")
    # --- End Find columns ---

    # --- Extract data into list of dictionaries ---
    vendors_data: List[Dict[str, Any]] = []
    processed_count = 0
    skipped_count = 0

    try:
        for index, row in df.iterrows():
            vendor_name_raw = row.get(column_map['vendor_name'])
            vendor_name = str(vendor_name_raw).strip() if pd.notna(vendor_name_raw) and str(vendor_name_raw).strip() else None

            if not vendor_name or vendor_name.lower() in ['nan', 'none', 'null']:
                skipped_count += 1
                continue

            vendor_entry: Dict[str, Any] = {'vendor_name': vendor_name}

            # Add optional fields if found
            for key, mapped_col in column_map.items():
                if key != 'vendor_name' and mapped_col: # Check if optional column was found
                    raw_value = row.get(mapped_col)
                    value = str(raw_value).strip() if pd.notna(raw_value) and str(raw_value).strip() else None
                    if value:
                        output_key = key
                        if key == 'example': output_key = 'example'
                        elif key == 'address': output_key = 'vendor_address'
                        elif key == 'website': output_key = 'vendor_website'
                        elif key == 'internal_cat': output_key = 'internal_category'
                        elif key == 'parent_co': output_key = 'parent_company'
                        elif key == 'spend_cat': output_key = 'spend_category'
                        vendor_entry[output_key] = value

            vendors_data.append(vendor_entry)
            processed_count += 1

        logger.info(f"Extracted data for {processed_count} vendors. Skipped {skipped_count} rows due to missing/invalid vendor name.")
        if not vendors_data:
             logger.warning(f"No valid vendor data found in the file after processing rows.")

    except KeyError as e:
        logger.error(f"Internal Error: KeyError accessing column '{e}' after it was seemingly mapped.",
                     extra={"column_map": column_map, "available_columns": detected_columns})
        raise ValueError(f"Internal error accessing column '{e}'.")
    except Exception as e:
        logger.error(f"Error extracting or processing data from file rows", exc_info=True)
        raise ValueError(f"Could not extract vendor data. Please check data format. Error: {e}")

    return vendors_data
    # --- End Extract data ---


@log_function_call(logger)
def normalize_vendor_data(vendors_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Normalize vendor names within the list of dictionaries by converting
    to title case and stripping whitespace. Filters out entries with
    empty names after normalization. Preserves other fields.
    """
    start_count = len(vendors_data)
    logger.info(f"Normalizing vendor names for {start_count} entries...")

    normalized_vendors_data = []
    empty_removed_count = 0

    with LogTimer(logger, "Vendor name normalization", include_in_stats=True):
        for entry in vendors_data:
            original_name = entry.get('vendor_name')
            if isinstance(original_name, str):
                normalized_name = original_name.strip().title()
                if normalized_name:
                    normalized_entry = entry.copy()
                    normalized_entry['vendor_name'] = normalized_name
                    normalized_vendors_data.append(normalized_entry)
                else:
                    empty_removed_count += 1
                    logger.warning("Skipping vendor entry due to empty name after normalization", extra={"original_name": original_name})
            else:
                logger.warning("Skipping vendor entry due to missing or non-string name during normalization", extra={"entry": entry})
                empty_removed_count += 1

    final_count = len(normalized_vendors_data)
    logger.info(f"Vendor names normalized.",
               extra={
                   "original_count": start_count,
                   "normalized_count": final_count,
                   "empty_or_skipped": empty_removed_count
               })

    return normalized_vendors_data


@log_function_call(logger)
def generate_output_file(
    original_vendor_data: List[Dict[str, Any]],
    classification_results: Dict[str, Dict],
    job_id: str
) -> str:
    """
    Generate output Excel file with classification results (up to Level 5), mapping back to
    original vendor data including optional fields read from the input.
    """
    logger.info(f"Generating output file for {len(original_vendor_data)} original vendor entries",
               extra={"job_id": job_id})

    output_data = []

    with LogTimer(logger, "Mapping results to original vendors"):
        for original_entry in original_vendor_data:
            original_vendor_name = original_entry.get('vendor_name', '')
            result = classification_results.get(original_vendor_name, {})

            final_level1_id = ""; final_level1_name = ""
            final_level2_id = ""; final_level2_name = ""
            final_level3_id = ""; final_level3_name = ""
            final_level4_id = ""; final_level4_name = ""
            final_level5_id = ""; final_level5_name = ""
            final_confidence = 0.0
            classification_not_possible_flag = True
            final_notes = ""
            reason = "Classification not possible"
            classification_source = "Initial"

            highest_successful_level = 0
            for level in range(5, 0, -1):
                level_key = f"level{level}"
                level_res = result.get(level_key)
                if level_res and isinstance(level_res, dict) and not level_res.get("classification_not_possible", True):
                    highest_successful_level = level
                    break

            if highest_successful_level > 0:
                classification_not_possible_flag = False
                reason = None
                final_confidence = result[f"level{highest_successful_level}"].get("confidence", 0.0)
                final_notes = result[f"level{highest_successful_level}"].get("notes", "")

                for level in range(1, highest_successful_level + 1):
                    level_res = result.get(f"level{level}", {})
                    if level == 1: final_level1_id = level_res.get("category_id", ""); final_level1_name = level_res.get("category_name", "")
                    elif level == 2: final_level2_id = level_res.get("category_id", ""); final_level2_name = level_res.get("category_name", "")
                    elif level == 3: final_level3_id = level_res.get("category_id", ""); final_level3_name = level_res.get("category_name", "")
                    elif level == 4: final_level4_id = level_res.get("category_id", ""); final_level4_name = level_res.get("category_name", "")
                    elif level == 5: final_level5_id = level_res.get("category_id", ""); final_level5_name = level_res.get("category_name", "")

                if result.get("classified_via_search"):
                    classification_source = "Search"
                    final_notes = f"Classified via search: {final_notes}"

            else: # No level was successfully classified
                classification_not_possible_flag = True
                final_confidence = 0.0
                failure_reason_found = False
                for level in range(5, 0, -1):
                     level_res = result.get(f"level{level}")
                     if level_res and isinstance(level_res, dict) and level_res.get("classification_not_possible", False):
                          reason = level_res.get("classification_not_possible_reason", f"Classification failed at Level {level}")
                          final_notes = level_res.get("notes", "")
                          failure_reason_found = True
                          break
                if not failure_reason_found:
                     if result.get("search_attempted"):
                          search_l1_result = result.get("search_results", {}).get("classification_l1", {})
                          if search_l1_result and search_l1_result.get("classification_not_possible", False):
                               reason = search_l1_result.get("classification_not_possible_reason", "Search did not yield classification")
                               final_notes = search_l1_result.get("notes", "")
                          elif result.get("search_results", {}).get("error"):
                               reason = f"Search error: {result['search_results']['error']}"

            original_address = original_entry.get('vendor_address')
            original_website = original_entry.get('vendor_website')
            original_internal_cat = original_entry.get('internal_category')
            original_parent_co = original_entry.get('parent_company')
            original_spend_cat = original_entry.get('spend_category')
            original_example = original_entry.get('example')

            search_sources_urls = ""
            search_data = result.get("search_results", {})
            if search_data and isinstance(search_data.get("sources"), list):
                 search_sources_urls = ", ".join(
                     source.get("url", "") for source in search_data["sources"] if isinstance(source, dict) and source.get("url")
                 )

            row = {
                "vendor_name": original_vendor_name,
                "vendor_address": original_address or "",
                "vendor_website": original_website or "",
                "internal_category": original_internal_cat or "",
                "parent_company": original_parent_co or "",
                "spend_category": original_spend_cat or "",
                "Optional_example_good_serviced_purchased": original_example or "",
                "level1_category_id": final_level1_id, "level1_category_name": final_level1_name,
                "level2_category_id": final_level2_id, "level2_category_name": final_level2_name,
                "level3_category_id": final_level3_id, "level3_category_name": final_level3_name,
                "level4_category_id": final_level4_id, "level4_category_name": final_level4_name,
                "level5_category_id": final_level5_id, "level5_category_name": final_level5_name,
                "final_confidence": final_confidence,
                "classification_not_possible": classification_not_possible_flag,
                "classification_notes_or_reason": reason or final_notes or "",
                "classification_source": classification_source,
                "sources": search_sources_urls
            }
            output_data.append(row)

    output_columns = [
        "vendor_name", "vendor_address", "vendor_website", "internal_category", "parent_company", "spend_category",
        "Optional_example_good_serviced_purchased",
        "level1_category_id", "level1_category_name", "level2_category_id", "level2_category_name",
        "level3_category_id", "level3_category_name", "level4_category_id", "level4_category_name",
        "level5_category_id", "level5_category_name",
        "final_confidence", "classification_not_possible", "classification_notes_or_reason",
        "classification_source", "sources"
    ]
    if not output_data:
        logger.warning("No data rows generated for the output file.")
        df = pd.DataFrame(columns=output_columns)
    else:
        with LogTimer(logger, "Creating DataFrame for output"):
            df = pd.DataFrame(output_data, columns=output_columns)

    output_dir = os.path.join(settings.OUTPUT_DATA_DIR, job_id)
    try:
        os.makedirs(output_dir, exist_ok=True)
        logger.debug(f"Ensured output directory exists", extra={"directory": output_dir})
    except OSError as e:
        logger.error(f"Failed to create output directory", exc_info=True, extra={"directory": output_dir})
        raise IOError(f"Could not create output directory for job {job_id}: {e}")

    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file_name = f"classification_results_{job_id[:8]}_{timestamp_str}.xlsx"
    output_path = os.path.join(output_dir, output_file_name)

    logger.info("Attempting to write final results to Excel file.", extra={"output_path": output_path})
    with LogTimer(logger, "Writing Excel file"):
        try:
            df.to_excel(output_path, index=False, engine='xlsxwriter')
        except Exception as e:
            logger.error("Failed to write output Excel file", exc_info=True, extra={"output_path": output_path})
            raise IOError(f"Could not write output file: {e}")

    try:
        file_size = os.path.getsize(output_path)
        logger.info(f"Output file generated successfully",
                   extra={"file_name": output_file_name, "path": output_path, "size_bytes": file_size})
    except OSError as e:
         logger.warning(f"Could not get size of generated output file", exc_info=False, extra={"output_path": output_path, "error": str(e)})

    return output_file_name
</file>

<file path='app/services/llm_service.py'>
# app/services/llm_service.py
import httpx
import json
import re
from typing import List, Dict, Any, Optional, Set
import logging
import time
import uuid
from tenacity import retry, stop_after_attempt, wait_exponential

from core.config import settings
from models.taxonomy import Taxonomy
from core.logging_config import get_logger
# Import context functions if needed directly
from core.log_context import set_log_context, get_correlation_id
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call

# --- MODIFIED IMPORT ---
# Use absolute import path from the root of PYTHONPATH (/app)
from tasks.classification_prompts import generate_batch_prompt, generate_search_prompt
# --- END MODIFIED IMPORT ---

# Configure logger
logger = get_logger("vendor_classification.llm_service")
llm_trace_logger = logging.getLogger("llm_api_trace") # ENSURE NAME CONSISTENT

# --- ADDED: Log confirmation ---
logger.debug("Successfully imported generate_batch_prompt and generate_search_prompt from tasks.classification_prompts.")
# --- END ADDED ---

# --- Helper function for JSON parsing (Remains here) ---
def _extract_json_from_response(response_content: str) -> Optional[Dict[str, Any]]:
    """
    Attempts to extract a JSON object from a string, handling common LLM response issues.
    """
    if not response_content:
        logger.warning("Attempted to parse empty response content.")
        return None

    content = response_content.strip()

    # 1. Check for markdown code fences (json ...  or  ... )
    match = re.search(r"(?:json)?\s*(\{.*\})\s*", content, re.DOTALL | re.IGNORECASE)
    if match:
        content = match.group(1).strip()
        logger.debug("Extracted JSON content from within markdown code fences.")
    else:
        # 2. If no fences, try finding the first '{' and last '}'
        start_index = content.find('{')
        end_index = content.rfind('}')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            potential_json = content[start_index:end_index+1]
            # Basic brace count check
            if potential_json.count('{') == potential_json.count('}'):
                 content = potential_json
                 logger.debug("Extracted potential JSON content based on first '{' and last '}'.")
            else:
                 logger.debug("Found '{' and '}', but braces don't match. Proceeding with original stripped content.")
        else:
            logger.debug("No markdown fences found, and couldn't reliably find JSON object boundaries. Proceeding with original stripped content.")

    # 3. Attempt to parse the cleaned/extracted content
    try:
        parsed_json = json.loads(content)
        logger.debug("Successfully parsed JSON after cleaning/extraction.")
        return parsed_json
    except json.JSONDecodeError as e:
        logger.error("JSONDecodeError after cleaning/extraction attempt.",
                     exc_info=False, # Less noise in main log, trace log has full info
                     extra={"error": str(e), "cleaned_content_preview": content[:500]})
        return None
    except Exception as e:
        logger.error("Unexpected error during JSON parsing after cleaning/extraction.",
                     exc_info=True,
                     extra={"cleaned_content_preview": content[:500]})
        return None
# --- END HELPER ---


class LLMService:
    """Service for interacting with OpenRouter API."""

    def __init__(self):
        """Initialize the LLM service."""
        logger.info("Initializing LLM service with OpenRouter")
        self.api_key = settings.OPENROUTER_API_KEY
        self.api_base = settings.OPENROUTER_API_BASE
        self.model = settings.OPENROUTER_MODEL
        logger.debug("LLM service initialized",
                    extra={"api_base": settings.OPENROUTER_API_BASE,
                          "model": settings.OPENROUTER_MODEL})
        if not self.api_key:
             logger.error("OpenRouter API key is missing!")

    @retry(stop=stop_after_attempt(settings.MAX_RETRIES), wait=wait_exponential(multiplier=1, min=settings.RETRY_DELAY, max=10))
    @log_function_call(logger, include_args=False)
    async def classify_batch(
        self,
        batch_data: List[Dict[str, Any]],
        level: int,
        taxonomy: Taxonomy,
        parent_category_id: Optional[str] = None,
        search_context: Optional[Dict[str, Any]] = None # ADDED: Optional search context
    ) -> Dict[str, Any]:
        """
        Send a batch of vendors to LLM for classification.
        Can optionally include search context for post-search classification attempts.
        """
        batch_names = [vd.get('vendor_name', f'Unknown_{i}') for i, vd in enumerate(batch_data)]
        context_type = "Search Context" if search_context else "Initial Data"
        logger.info(f"Classifying vendor batch using {context_type}",
                   extra={
                       "batch_size": len(batch_data),
                       "level": level,
                       "parent_category_id": parent_category_id,
                       "has_search_context": bool(search_context)
                   })
        set_log_context({"vendor_count": len(batch_data), "taxonomy_level": level, "context_type": context_type})
        batch_id = str(uuid.uuid4())
        logger.debug(f"Generated batch ID", extra={"batch_id": batch_id})
        correlation_id = get_correlation_id() # Get correlation ID for tracing
        llm_trace_logger.debug(f"LLM_TRACE: Starting classify_batch (Batch ID: {batch_id}, Level: {level}, Parent: {parent_category_id}, Context: {context_type})", extra={'correlation_id': correlation_id})

        if not self.api_key:
            logger.error("Cannot classify batch: OpenRouter API key is missing.")
            llm_trace_logger.error(f"LLM_TRACE: LLM API Error (Batch ID: {batch_id}): API key missing.", extra={'correlation_id': correlation_id})
            return {
                "result": {
                    "level": level, "batch_id": batch_id, "parent_category_id": parent_category_id,
                    "classifications": [
                        {
                            "vendor_name": vd.get('vendor_name', f'Unknown_{i}'), "category_id": "ERROR", "category_name": "ERROR",
                            "confidence": 0.0, "classification_not_possible": True,
                            "classification_not_possible_reason": "API key missing", "notes": "Failed due to missing API key configuration."
                        } for i, vd in enumerate(batch_data)
                    ]
                },
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }

        logger.debug(f"Creating classification prompt with {context_type}")
        with LogTimer(logger, "Prompt creation", include_in_stats=True):
            # --- Use imported prompt function ---
            prompt = generate_batch_prompt(
                batch_data, level, taxonomy, parent_category_id, batch_id, search_context
            )
            # --- End Use imported prompt function ---
            prompt_length = len(prompt)
            logger.debug(f"Classification prompt created", extra={"prompt_length": prompt_length})
            llm_trace_logger.debug(f"LLM_TRACE: Generated Prompt (Batch ID: {batch_id}):\n-------\n{prompt}\n-------", extra={'correlation_id': correlation_id})

        headers = {
            "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json",
            "HTTP-Referer": "naicsvendorclassification.com", "X-Title": "NAICS Vendor Classification"
        }
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1, "max_tokens": 2048, "top_p": 0.9,
            "frequency_penalty": 0, "presence_penalty": 0,
            "response_format": {"type": "json_object"} # Request JSON output mode
        }

        # Log Request Details (Trace Log)
        try:
            log_headers = {k: v for k, v in headers.items() if k.lower() != 'authorization'}
            log_headers['Authorization'] = 'Bearer [REDACTED]'
            llm_trace_logger.debug(f"LLM_TRACE: LLM Request Headers (Batch ID: {batch_id}):\n{json.dumps(log_headers, indent=2)}", extra={'correlation_id': correlation_id})
            llm_trace_logger.debug(f"LLM_TRACE: LLM Request Payload (Batch ID: {batch_id}):\n{json.dumps(payload, indent=2)}", extra={'correlation_id': correlation_id})
        except Exception as log_err:
            llm_trace_logger.warning(f"LLM_TRACE: Failed to log LLM request details (Batch ID: {batch_id}): {log_err}", extra={'correlation_id': correlation_id})

        response_data = None; raw_content = None; response = None; status_code = None; api_duration = 0.0
        try:
            logger.debug(f"Sending request to OpenRouter API")
            start_time = time.time()
            async with httpx.AsyncClient() as client:
                response = await client.post(f"{self.api_base}/chat/completions", json=payload, headers=headers, timeout=90.0) # Increased timeout
                raw_content = response.text # Get raw text immediately
                status_code = response.status_code
                api_duration = time.time() - start_time
                llm_trace_logger.debug(f"LLM_TRACE: LLM Raw Response (Batch ID: {batch_id}, Status: {status_code}, Duration: {api_duration:.3f}s):\n-------\n{raw_content or '[No Content Received]'}\n-------", extra={'correlation_id': correlation_id})
                response.raise_for_status() # Check for HTTP errors AFTER logging raw response
                response_data = response.json() # Parse JSON only if status is OK

            # Extract raw content field AFTER potential JSON parse
            if response_data and response_data.get("choices") and isinstance(response_data["choices"], list) and len(response_data["choices"]) > 0:
                 message = response_data["choices"][0].get("message")
                 if message and isinstance(message, dict):
                     content_field = message.get("content")
                     if content_field:
                         raw_content = content_field # Overwrite with content field if possible
                         logger.debug("Extracted 'content' field from LLM JSON response.")
                     else:
                         logger.warning("LLM response message object missing 'content' field.", extra={"message_obj": message})
                 else:
                     logger.warning("LLM response choice missing 'message' object or it's not a dict.", extra={"choice_obj": response_data["choices"][0]})

            usage = response_data.get("usage", {}) if response_data else {}
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)

            logger.info(f"OpenRouter API response received",
                       extra={
                           "duration": api_duration, "batch_id": batch_id, "level": level,
                           "status_code": status_code,
                           "openrouter_prompt_tokens": prompt_tokens,
                           "openrouter_completion_tokens": completion_tokens,
                           "openrouter_total_tokens": total_tokens
                       })

            logger.debug("Raw LLM response content received (after potential extraction)", extra={"content_preview": str(raw_content)[:500]})
            with LogTimer(logger, "JSON parsing and extraction", include_in_stats=True):
                result = _extract_json_from_response(raw_content) # Parse the potentially extracted content

            if result is None:
                llm_trace_logger.error(f"LLM_TRACE: LLM JSON Parse Error (Batch ID: {batch_id}). Raw content logged above.", extra={'correlation_id': correlation_id})
                raise ValueError(f"LLM response was not valid JSON or could not be extracted. Preview: {str(raw_content)[:200]}")

            try:
                 llm_trace_logger.debug(f"LLM_TRACE: LLM Parsed Response (Batch ID: {batch_id}):\n{json.dumps(result, indent=2)}", extra={'correlation_id': correlation_id})
            except Exception as log_err:
                 llm_trace_logger.warning(f"LLM_TRACE: Failed to log LLM parsed response (Batch ID: {batch_id}): {log_err}", extra={'correlation_id': correlation_id})

            response_batch_id = result.get("batch_id")
            if response_batch_id != batch_id:
                logger.warning(f"LLM response batch_id mismatch!",
                               extra={"expected_batch_id": batch_id, "received_batch_id": response_batch_id})
                result["batch_id_mismatch"] = True # Add flag but continue processing

            usage_data = { "prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens, "total_tokens": total_tokens }
            set_log_context({
                "openrouter_prompt_tokens": prompt_tokens, "openrouter_completion_tokens": completion_tokens, "openrouter_total_tokens": total_tokens
            })

            classification_count = len(result.get("classifications", []))
            successful_count = sum( 1 for c in result.get("classifications", []) if isinstance(c, dict) and not c.get("classification_not_possible", False) )
            logger.info(f"LLM Classification attempt completed for batch",
                       extra={
                           "level": level, "batch_id": batch_id, "total_in_batch": len(batch_data),
                           "classifications_in_response": classification_count, "successful_classifications": successful_count,
                           "failed_or_not_possible": classification_count - successful_count
                       })

            return { "result": result, "usage": usage_data }

        except httpx.HTTPStatusError as e:
             response_text = raw_content or (e.response.text[:500] if hasattr(e.response, 'text') else "[No Response Body]")
             status_code = e.response.status_code
             logger.error(f"HTTP error during LLM batch classification", exc_info=False,
                         extra={ "status_code": status_code, "response_text": response_text, "batch_id": batch_id, "level": level })
             llm_trace_logger.error(f"LLM_TRACE: LLM API HTTP Error (Batch ID: {batch_id}): Status={status_code}, Response='{response_text}'", exc_info=True, extra={'correlation_id': correlation_id})
             raise # Re-raise for tenacity
        except httpx.RequestError as e:
             logger.error(f"Network error during LLM batch classification", exc_info=False,
                         extra={ "error_details": str(e), "batch_id": batch_id, "level": level })
             llm_trace_logger.error(f"LLM_TRACE: LLM API Network Error (Batch ID: {batch_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
             raise # Re-raise for tenacity
        except ValueError as ve: # Catch the specific error raised on JSON parse failure
             logger.error(f"LLM response parsing error during batch classification", exc_info=False,
                          extra={"error": str(ve), "batch_id": batch_id, "level": level})
             raise # Re-raise for tenacity
        except Exception as e:
            error_context = { "batch_size": len(batch_data), "level": level, "parent_category_id": parent_category_id, "error": str(e), "model": self.model, "batch_id": batch_id }
            logger.error(f"Unexpected error during LLM batch classification", exc_info=True, extra=error_context)
            llm_trace_logger.error(f"LLM_TRACE: LLM Unexpected Error (Batch ID: {batch_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
            raise # Re-raise for tenacity

    @retry(stop=stop_after_attempt(settings.MAX_RETRIES), wait=wait_exponential(multiplier=1, min=settings.RETRY_DELAY, max=10))
    @log_function_call(logger, include_args=False)
    async def process_search_results(
        self,
        vendor_data: Dict[str, Any],
        search_results: Dict[str, Any],
        taxonomy: Taxonomy
    ) -> Dict[str, Any]:
        """
        Process search results to determine **Level 1** classification only.
        """
        vendor_name = vendor_data.get('vendor_name', 'UnknownVendor')
        logger.info(f"Processing search results for initial L1 classification",
                   extra={ "vendor": vendor_name, "source_count": len(search_results.get("sources", [])) })
        set_log_context({"vendor": vendor_name})
        attempt_id = str(uuid.uuid4())
        logger.debug(f"Generated search processing attempt ID", extra={"attempt_id": attempt_id})
        correlation_id = get_correlation_id()
        llm_trace_logger.debug(f"LLM_TRACE: Starting process_search_results (Attempt ID: {attempt_id}, Vendor: {vendor_name})", extra={'correlation_id': correlation_id})

        if not self.api_key:
            logger.error("Cannot process search results: OpenRouter API key is missing.")
            llm_trace_logger.error(f"LLM_TRACE: LLM API Error (Attempt ID: {attempt_id}): API key missing.", extra={'correlation_id': correlation_id})
            return {
                "result": { "vendor_name": vendor_name, "category_id": "ERROR", "category_name": "ERROR", "confidence": 0.0, "classification_not_possible": True, "classification_not_possible_reason": "API key missing", "notes": "" },
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }

        with LogTimer(logger, "Search prompt creation", include_in_stats=True):
            # --- Use imported prompt function ---
            prompt = generate_search_prompt(vendor_data, search_results, taxonomy, attempt_id)
            # --- End Use imported prompt function ---
            prompt_length = len(prompt)
            logger.debug(f"Search results prompt created", extra={"prompt_length": prompt_length})
            llm_trace_logger.debug(f"LLM_TRACE: Generated Search Prompt (Attempt ID: {attempt_id}):\n-------\n{prompt}\n-------", extra={'correlation_id': correlation_id})

        headers = {
            "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json",
            "HTTP-Referer": "naicsvendorclassification.com", "X-Title": "NAICS Vendor Classification"
        }
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1, "max_tokens": 1024, "top_p": 0.9,
            "frequency_penalty": 0, "presence_penalty": 0,
            "response_format": {"type": "json_object"} # Request JSON output mode
        }

        # Log Request Details (Trace Log)
        try:
            log_headers = {k: v for k, v in headers.items() if k.lower() != 'authorization'}
            log_headers['Authorization'] = 'Bearer [REDACTED]'
            llm_trace_logger.debug(f"LLM_TRACE: LLM Request Headers (Attempt ID: {attempt_id}):\n{json.dumps(log_headers, indent=2)}", extra={'correlation_id': correlation_id})
            llm_trace_logger.debug(f"LLM_TRACE: LLM Request Payload (Attempt ID: {attempt_id}):\n{json.dumps(payload, indent=2)}", extra={'correlation_id': correlation_id})
        except Exception as log_err:
            llm_trace_logger.warning(f"LLM_TRACE: Failed to log LLM request details (Attempt ID: {attempt_id}): {log_err}", extra={'correlation_id': correlation_id})

        response_data = None; raw_content = None; response = None; status_code = None; api_duration = 0.0
        try:
            logger.debug(f"Sending search results to OpenRouter API")
            start_time = time.time()
            async with httpx.AsyncClient() as client:
                response = await client.post(f"{self.api_base}/chat/completions", json=payload, headers=headers, timeout=60.0)
                raw_content = response.text # Get raw text immediately
                status_code = response.status_code
                api_duration = time.time() - start_time
                llm_trace_logger.debug(f"LLM_TRACE: LLM Raw Response (Attempt ID: {attempt_id}, Status: {status_code}, Duration: {api_duration:.3f}s):\n-------\n{raw_content or '[No Content Received]'}\n-------", extra={'correlation_id': correlation_id})
                response.raise_for_status() # Check for HTTP errors AFTER logging raw response
                response_data = response.json() # Parse JSON only if status is OK

            # Extract raw content field AFTER potential JSON parse
            if response_data and response_data.get("choices") and isinstance(response_data["choices"], list) and len(response_data["choices"]) > 0:
                 message = response_data["choices"][0].get("message")
                 if message and isinstance(message, dict):
                     content_field = message.get("content")
                     if content_field:
                         raw_content = content_field # Overwrite with content field if possible
                         logger.debug("Extracted 'content' field from LLM JSON response (search).")
                     else:
                         logger.warning("LLM response message object missing 'content' field (search).", extra={"message_obj": message})
                 else:
                     logger.warning("LLM response choice missing 'message' object or it's not a dict (search).", extra={"choice_obj": response_data["choices"][0]})

            usage = response_data.get("usage", {}) if response_data else {}
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)

            logger.info(f"OpenRouter API response received for search results",
                       extra={ "duration": api_duration, "vendor": vendor_name, "status_code": status_code, "openrouter_prompt_tokens": prompt_tokens, "openrouter_completion_tokens": completion_tokens, "openrouter_total_tokens": total_tokens, "attempt_id": attempt_id })

            logger.debug("Raw LLM response content received (search, after potential extraction)", extra={"content_preview": str(raw_content)[:500]})
            with LogTimer(logger, "JSON parsing and extraction (search)", include_in_stats=True):
                result = _extract_json_from_response(raw_content) # Parse the potentially extracted content

            if result is None:
                llm_trace_logger.error(f"LLM_TRACE: LLM JSON Parse Error (Attempt ID: {attempt_id}). Raw content logged above.", extra={'correlation_id': correlation_id})
                raise ValueError(f"LLM response after search was not valid JSON or could not be extracted. Preview: {str(raw_content)[:200]}")

            try:
                 llm_trace_logger.debug(f"LLM_TRACE: LLM Parsed Response (Attempt ID: {attempt_id}):\n{json.dumps(result, indent=2)}", extra={'correlation_id': correlation_id})
            except Exception as log_err:
                 llm_trace_logger.warning(f"LLM_TRACE: Failed to log LLM parsed response (Attempt ID: {attempt_id}): {log_err}", extra={'correlation_id': correlation_id})

            response_vendor = result.get("vendor_name")
            if response_vendor != vendor_name:
                logger.warning(f"LLM search response vendor name mismatch!",
                               extra={"expected_vendor": vendor_name, "received_vendor": response_vendor})
                result["vendor_name"] = vendor_name # Ensure the correct vendor name is in the result

            usage_data = { "prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens, "total_tokens": total_tokens }
            set_log_context({ "openrouter_prompt_tokens": prompt_tokens, "openrouter_completion_tokens": completion_tokens, "openrouter_total_tokens": total_tokens })

            # This function returns the direct result from the LLM for the L1 search attempt
            return { "result": result, "usage": usage_data }

        except httpx.HTTPStatusError as e:
             response_text = raw_content or (e.response.text[:500] if hasattr(e.response, 'text') else "[No Response Body]")
             status_code = e.response.status_code
             logger.error(f"HTTP error during search result processing", exc_info=False,
                         extra={ "status_code": status_code, "response_text": response_text, "vendor": vendor_name, "attempt_id": attempt_id })
             llm_trace_logger.error(f"LLM_TRACE: LLM API HTTP Error (Attempt ID: {attempt_id}): Status={status_code}, Response='{response_text}'", exc_info=True, extra={'correlation_id': correlation_id})
             raise # Re-raise for tenacity
        except httpx.RequestError as e:
             logger.error(f"Network error during search result processing", exc_info=False,
                         extra={ "error_details": str(e), "vendor": vendor_name, "attempt_id": attempt_id })
             llm_trace_logger.error(f"LLM_TRACE: LLM API Network Error (Attempt ID: {attempt_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
             raise # Re-raise for tenacity
        except ValueError as ve: # Catch the specific error raised on JSON parse failure
             logger.error(f"LLM response parsing error during search result processing", exc_info=False,
                          extra={"error": str(ve), "vendor": vendor_name, "attempt_id": attempt_id})
             raise # Re-raise for tenacity
        except Exception as e:
            error_context = { "vendor": vendor_name, "error": str(e), "model": self.model, "attempt_id": attempt_id }
            logger.error(f"Unexpected error during search result processing", exc_info=True, extra=error_context)
            llm_trace_logger.error(f"LLM_TRACE: LLM Unexpected Error (Attempt ID: {attempt_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
            raise # Re-raise for tenacity
</file>

<file path='app/services/search_service.py'>

# app/services/search_service.py
import httpx
import logging
from typing import Dict, Any, List
from tenacity import retry, stop_after_attempt, wait_exponential
import time
import uuid
import json

from core.config import settings
# Import logger and context functions from refactored modules
from core.logging_config import get_logger
from core.log_context import set_log_context, get_correlation_id
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call

llm_trace_logger = logging.getLogger("llm_api_trace") # Get the trace logger
logger = get_logger("vendor_classification.search_service")

class SearchService:
    """Service for interacting with Tavily Search API."""

    def __init__(self):
        """Initialize the search service."""
        logger.info("Initializing Tavily Search service")
        self.api_key = settings.TAVILY_API_KEY
        self.base_url = "https://api.tavily.com"
        if not self.api_key:
            logger.error("Tavily API key is missing!")
        logger.debug("Search service initialized",
                   extra={"api_endpoint": self.base_url})

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
    @log_function_call(logger)
    async def search_vendor(self, vendor_name: str) -> Dict[str, Any]:
        """
        Search for information about a vendor.
        """
        logger.info(f"Searching for vendor information",
                   extra={"vendor": vendor_name})
        set_log_context({"vendor": vendor_name})
        search_query = f"{vendor_name} company business type industry"
        logger.debug(f"Generated search query",
                   extra={"search_query": search_query})

        if not self.api_key:
             logger.error("Cannot perform Tavily search: API key is missing.")
             return {
                "vendor": vendor_name,
                "error": "Tavily API key not configured",
                "search_query": search_query,
                "sources": []
             }

        correlation_id = get_correlation_id() # Get correlation ID
        search_attempt_id = str(uuid.uuid4()) # Unique ID for this specific search attempt
        llm_trace_logger.debug(f"LLM_TRACE: Starting Tavily Search (Attempt ID: {search_attempt_id}, Vendor: {vendor_name})", extra={'correlation_id': correlation_id})

        response = None; raw_content = None; response_data = None; status_code = None
        try:
            logger.debug(f"Sending request to Tavily API")

            payload = {
                "api_key": "[REDACTED]", # Redact for logging payload
                "query": search_query,
                "search_depth": "advanced",
                "include_answer": True,
                "max_results": 5
            }
            headers = {"Content-Type": "application/json"}

            # Trace Logging for Request
            try:
                llm_trace_logger.debug(f"LLM_TRACE: Tavily Request Headers (Attempt ID: {search_attempt_id}):\n{json.dumps(headers, indent=2)}", extra={'correlation_id': correlation_id})
                llm_trace_logger.debug(f"LLM_TRACE: Tavily Request Payload (Attempt ID: {search_attempt_id}):\n{json.dumps(payload, indent=2)}", extra={'correlation_id': correlation_id})
            except Exception as log_err:
                 llm_trace_logger.warning(f"LLM_TRACE: Failed to log Tavily request details (Attempt ID: {search_attempt_id}): {log_err}", extra={'correlation_id': correlation_id})

            actual_payload = payload.copy()
            actual_payload["api_key"] = self.api_key

            async with httpx.AsyncClient() as client:
                with LogTimer(logger, "Tavily API request", include_in_stats=True):
                    start_time = time.time()
                    response = await client.post(
                        f"{self.base_url}/search",
                        json=actual_payload,
                        headers=headers,
                        timeout=30.0
                    )
                    api_duration = time.time() - start_time
                    raw_content = response.text
                    status_code = response.status_code
                    # Trace Logging for Response
                    llm_trace_logger.debug(f"LLM_TRACE: Tavily Raw Response (Attempt ID: {search_attempt_id}, Status: {status_code}, Duration: {api_duration:.3f}s):\n-------\n{raw_content or '[No Content Received]'}\n-------", extra={'correlation_id': correlation_id})
                    response.raise_for_status()
                    response_data = response.json()

            processed_results = {
                "vendor": vendor_name,
                "search_query": search_query,
                "sources": [
                    {
                        "title": result.get("title", ""),
                        "url": result.get("url", ""),
                        "content": result.get("content", "")
                    }
                    for result in response_data.get("results", []) if result.get("url")
                ],
                "summary": response_data.get("answer", ""),
                "error": None
            }
            logger.info(f"Tavily search successful", extra={"vendor": vendor_name, "results_found": len(processed_results["sources"])})
            return processed_results

        except httpx.HTTPStatusError as e:
             response_text = raw_content or (e.response.text[:500] if hasattr(e.response, 'text') else "[No Response Body]")
             status_code = e.response.status_code
             logger.error(f"Tavily API HTTP error for vendor '{vendor_name}'", exc_info=False,
                         extra={"status_code": status_code, "response": response_text})
             llm_trace_logger.error(f"LLM_TRACE: Tavily API HTTP Error (Attempt ID: {search_attempt_id}): Status={status_code}, Response='{response_text}'", exc_info=True, extra={'correlation_id': correlation_id})
             return { "vendor": vendor_name, "error": f"Tavily API returned status {status_code}: {response_text}", "search_query": search_query, "sources": [] }
        except httpx.RequestError as e:
             logger.error(f"Tavily API request error for vendor '{vendor_name}'", exc_info=False, extra={"error_details": str(e)})
             llm_trace_logger.error(f"LLM_TRACE: Tavily API Network Error (Attempt ID: {search_attempt_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
             return { "vendor": vendor_name, "error": f"Tavily API request failed: {str(e)}", "search_query": search_query, "sources": [] }
        except Exception as e:
            logger.error(f"Unexpected error during Tavily search for vendor '{vendor_name}'", exc_info=True)
            llm_trace_logger.error(f"LLM_TRACE: Tavily Unexpected Error (Attempt ID: {search_attempt_id}): {e}", exc_info=True, extra={'correlation_id': correlation_id})
            return { "vendor": vendor_name, "error": f"Unexpected error during search: {str(e)}", "search_query": search_query, "sources": [] }
</file>

<file path='app/services/user_service.py'>
# app/services/user_service.py
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from fastapi import HTTPException, status
from typing import List, Optional
import uuid

from models.user import User
from schemas.user import UserCreate, UserUpdate
from api.auth import get_password_hash # Assuming auth.py is in api directory
from core.logging_config import get_logger

logger = get_logger("vendor_classification.user_service")

def create_user(db: Session, user_in: UserCreate) -> User:
    """
    Creates a new user in the database.
    Handles password hashing and potential integrity errors.
    """
    logger.info(f"Attempting to create user", extra={"username": user_in.username, "email": user_in.email})
    hashed_password = get_password_hash(user_in.password)
    db_user = User(
        id=str(uuid.uuid4()), # Generate UUID string for ID
        username=user_in.username,
        email=user_in.email,
        full_name=user_in.full_name,
        hashed_password=hashed_password,
        is_active=user_in.is_active if user_in.is_active is not None else True,
        is_superuser=user_in.is_superuser if user_in.is_superuser is not None else False,
    )
    db.add(db_user)
    try:
        db.commit()
        db.refresh(db_user)
        logger.info(f"User created successfully", extra={"user_id": db_user.id, "username": db_user.username})
        return db_user
    except IntegrityError as e:
        db.rollback()
        logger.warning(f"Failed to create user due to integrity error (duplicate username/email?)",
                        extra={"username": user_in.username, "email": user_in.email, "error": str(e)})
        # Determine if it's username or email conflict (crude check)
        detail = "Username or email already registered."
        if "users_username_key" in str(e).lower():
            detail = f"Username '{user_in.username}' is already taken."
        elif "users_email_key" in str(e).lower():
            detail = f"Email '{user_in.email}' is already registered."
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail)
    except Exception as e:
        db.rollback()
        logger.error(f"Unexpected error creating user", exc_info=True, extra={"username": user_in.username})
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create user.")


def get_user(db: Session, user_id: str) -> Optional[User]:
    """Retrieves a user by their ID."""
    logger.debug(f"Fetching user by ID", extra={"user_id": user_id})
    user = db.query(User).filter(User.id == user_id).first()
    if user:
        logger.debug(f"User found", extra={"user_id": user_id, "username": user.username})
    else:
        logger.debug(f"User not found", extra={"user_id": user_id})
    return user

def get_user_by_username(db: Session, username: str) -> Optional[User]:
    """Retrieves a user by their username."""
    logger.debug(f"Fetching user by username", extra={"username": username})
    user = db.query(User).filter(User.username == username).first()
    if user:
        logger.debug(f"User found", extra={"user_id": user.id, "username": username})
    else:
        logger.debug(f"User not found", extra={"username": username})
    return user

def get_user_by_email(db: Session, email: str) -> Optional[User]:
    """Retrieves a user by their email."""
    logger.debug(f"Fetching user by email", extra={"email": email})
    user = db.query(User).filter(User.email == email).first()
    if user:
        logger.debug(f"User found", extra={"user_id": user.id, "username": user.username, "email": email})
    else:
        logger.debug(f"User not found", extra={"email": email})
    return user


def get_users(db: Session, skip: int = 0, limit: int = 100) -> List[User]:
    """Retrieves a list of users with pagination."""
    logger.info(f"Fetching list of users", extra={"skip": skip, "limit": limit})
    users = db.query(User).offset(skip).limit(limit).all()
    logger.info(f"Retrieved {len(users)} users.")
    return users


def update_user(db: Session, user_id: str, user_in: UserUpdate) -> Optional[User]:
    """Updates an existing user."""
    logger.info(f"Attempting to update user", extra={"user_id": user_id})
    db_user = get_user(db, user_id)
    if not db_user:
        logger.warning(f"User not found for update", extra={"user_id": user_id})
        return None

    update_data = user_in.model_dump(exclude_unset=True) # Pydantic v2
    # update_data = user_in.dict(exclude_unset=True) # Pydantic v1

    if "password" in update_data and update_data["password"]:
        logger.info(f"Updating password for user", extra={"user_id": user_id})
        hashed_password = get_password_hash(update_data["password"])
        db_user.hashed_password = hashed_password
        del update_data["password"] # Remove plain password from update dict

    for field, value in update_data.items():
        setattr(db_user, field, value)

    try:
        db.commit()
        db.refresh(db_user)
        logger.info(f"User updated successfully", extra={"user_id": user_id})
        return db_user
    except IntegrityError as e:
        db.rollback()
        logger.warning(f"Failed to update user due to integrity error (duplicate username/email?)",
                        extra={"user_id": user_id, "error": str(e)})
        detail = "Username or email already registered by another user."
        if "users_username_key" in str(e).lower():
            detail = f"Username '{update_data.get('username', db_user.username)}' is already taken."
        elif "users_email_key" in str(e).lower():
            detail = f"Email '{update_data.get('email', db_user.email)}' is already registered."
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=detail)
    except Exception as e:
        db.rollback()
        logger.error(f"Unexpected error updating user", exc_info=True, extra={"user_id": user_id})
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not update user.")


def delete_user(db: Session, user_id: str) -> bool:
    """Deletes a user."""
    logger.info(f"Attempting to delete user", extra={"user_id": user_id})
    db_user = get_user(db, user_id)
    if not db_user:
        logger.warning(f"User not found for deletion", extra={"user_id": user_id})
        return False

    # Prevent deleting the default admin? (Optional safeguard)
    # if db_user.username == 'admin':
    #     logger.error("Attempted to delete the default admin user.", extra={"user_id": user_id})
    #     raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Cannot delete the primary admin user.")

    try:
        db.delete(db_user)
        db.commit()
        logger.info(f"User deleted successfully", extra={"user_id": user_id})
        return True
    except Exception as e:
        db.rollback()
        logger.error(f"Error deleting user", exc_info=True, extra={"user_id": user_id})
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not delete user.")

</file>

<file path='app/tasks/celery_app.py'>

# app/tasks/celery_app.py
from celery import Celery
import logging
import sys
import os

# Attempt initial logging setup
try:
    from core.logging_config import setup_logging
    log_dir_worker = "/data/logs" if os.path.exists("/data") else "./logs_worker"
    os.makedirs(log_dir_worker, exist_ok=True)
    print(f"WORKER: Attempting initial logging setup to {log_dir_worker}")
    setup_logging(log_to_file=True, log_dir=log_dir_worker, async_logging=False, llm_trace_log_file="llm_api_trace_worker.log")
    print("WORKER: Initial logging setup attempted.")
except Exception as setup_err:
    print(f"WORKER: CRITICAL ERROR during initial logging setup: {setup_err}")
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Import logger *after* setup attempt
from core.logging_config import get_logger
# Import context functions from the new module
from core.log_context import set_correlation_id, set_job_id, clear_all_context

# Use direct signal imports from celery.signals
from celery.signals import task_prerun, task_postrun, task_failure

logger = get_logger("vendor_classification.celery")

# Log diagnostic information
logger.info(
    f"Initializing Celery app",
    extra={
        "python_executable": sys.executable,
        "python_version": sys.version,
        "python_path": sys.path,
        "cwd": os.getcwd()
    }
)

try:
    from core.config import settings
    logger.info("Successfully imported settings")
except Exception as e:
    logger.error("Error importing settings", exc_info=True)
    raise

# Create Celery app
logger.info("Creating Celery app")
try:
    celery_app = Celery(
        "vendor_classification",
        broker=settings.REDIS_URL,
        backend=settings.REDIS_URL
    )
    logger.info("Celery app created", extra={"broker": settings.REDIS_URL})

    # Configure Celery
    celery_app.conf.update(
        task_serializer="json",
        accept_content=["json"],
        result_serializer="json",
        timezone="UTC",
        enable_utc=True,
        task_track_started=True,
        task_send_sent_event=True,
    )
    logger.info("Celery configuration updated")

    # Signal Handlers
    logger.info("Connecting Celery signal handlers...")

    @task_prerun.connect
    def handle_task_prerun(task_id, task, args, kwargs, **extra_options):
        """Signal handler before task runs."""
        clear_all_context() # Clear any lingering context
        job_id = kwargs.get('job_id') or (args[0] if args else None) or task_id
        set_correlation_id(job_id) # Use job_id as correlation_id
        set_job_id(job_id)
        logger.info(
            "Task about to run",
            extra={
                "signal": "task_prerun",
                "task_id": task_id,
                "task_name": task.name,
                "args": args,
                "kwargs": kwargs
            }
        )

    @task_postrun.connect
    def handle_task_postrun(task_id, task, args, kwargs, retval, state, **extra_options):
        """Signal handler after task completes."""
        logger.info(
            "Task finished running",
            extra={
                "signal": "task_postrun",
                "task_id": task_id,
                "task_name": task.name,
                "retval": repr(retval)[:200],
                "final_state": state
            }
        )
        clear_all_context() # Clean up context

    @task_failure.connect
    def handle_task_failure(task_id, exception, args, kwargs, traceback, einfo, **extra_options):
        """Signal handler if task fails."""
        task_name = getattr(kwargs.get('task'), 'name', None) or getattr(einfo, 'task', {}).get('name', 'UnknownTask')
        logger.error(
            "Task failed",
            exc_info=(type(exception), exception, traceback),
            extra={
                "signal": "task_failure",
                "task_id": task_id,
                "task_name": task_name,
                "args": args,
                "kwargs": kwargs,
                "exception_type": type(exception).__name__,
                "error": str(exception),
                "einfo": str(einfo)[:1000] if einfo else None
            }
        )
        clear_all_context() # Clean up context

    logger.info("Celery signal handlers connected.")

except Exception as e:
    logger.error("Error creating or configuring Celery app", exc_info=True)
    raise

# Import tasks to register them
logger.info("Attempting to import tasks for registration...")
try:
    from tasks.classification_tasks import process_vendor_file
    logger.info("Successfully imported 'tasks.classification_tasks.process_vendor_file'")
except ImportError as e:
    logger.error("ImportError when importing tasks", exc_info=True)
    logger.error(f"sys.path during task import: {sys.path}")
    raise
except Exception as e:
    logger.error("Unexpected error importing tasks", exc_info=True)
    raise

# Log discovered tasks
logger.info(f"Tasks registered in Celery app: {list(celery_app.tasks.keys())}")

logger.info("Celery app initialization finished.")

if __name__ == "__main__":
    logger.warning("celery_app.py run directly (likely for testing/debugging)")
</file>

<file path='app/tasks/classification_logic.py'>

# app/tasks/classification_logic.py
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError

from core.config import settings
from core.logging_config import get_logger
# Import context functions if needed directly (though often used via logger)
from core.log_context import set_log_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_function_call, log_duration

from models.job import Job, JobStatus, ProcessingStage
from models.taxonomy import Taxonomy
from services.llm_service import LLMService
from services.search_service import SearchService

logger = get_logger("vendor_classification.classification_logic")

# --- Constants ---
MAX_CONCURRENT_SEARCHES = 10 # Limit concurrent search/LLM processing for unknown vendors

# --- Helper Functions (Moved from classification_tasks.py) ---

def create_batches(items: List[Any], batch_size: int) -> List[List[Any]]:
    """Create batches from a list of items."""
    if not items: return []
    if not isinstance(items, list):
        logger.warning(f"create_batches expected a list, got {type(items)}. Returning empty list.")
        return []
    if batch_size <= 0:
        logger.warning(f"Invalid batch_size {batch_size}, using default from settings.")
        batch_size = settings.BATCH_SIZE
    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]

def group_by_parent_category(
    results: Dict[str, Dict],
    parent_level: int,
    vendors_to_group_names: List[str]
) -> Dict[Optional[str], List[str]]:
    """
    Group a specific list of vendor names based on their classification result at the parent_level.
    Only includes vendors that were successfully classified with a valid ID at the parent level.
    Returns a dictionary mapping parent category ID to a list of vendor *names*.
    """
    grouped: Dict[Optional[str], List[str]] = {}
    parent_key = f"level{parent_level}"
    logger.debug(f"group_by_parent_category: Grouping {len(vendors_to_group_names)} vendors based on results from '{parent_key}'.")

    grouped_count = 0
    excluded_count = 0

    for vendor_name in vendors_to_group_names:
        vendor_results = results.get(vendor_name)
        level_result = None
        if vendor_results is not None:
            level_result = vendor_results.get(parent_key)
        else:
            logger.warning(f"group_by_parent_category: Vendor '{vendor_name}' not found in results dictionary.")
            excluded_count += 1
            continue

        if level_result and isinstance(level_result, dict) and not level_result.get("classification_not_possible", True):
            category_id = level_result.get("category_id")
            if category_id and category_id not in ["N/A", "ERROR"]:
                if category_id not in grouped:
                    grouped[category_id] = []
                grouped[category_id].append(vendor_name)
                grouped_count += 1
                logger.debug(f"  Grouping vendor '{vendor_name}' under parent '{category_id}'.")
            else:
                logger.debug(f"  Excluding vendor '{vendor_name}': classified at '{parent_key}' but has invalid category_id '{category_id}'.")
                excluded_count += 1
        else:
            reason = "Not processed"
            if level_result and isinstance(level_result, dict):
                reason = level_result.get('classification_not_possible_reason', 'Marked not possible')
            elif not level_result:
                    reason = f"No result found for {parent_key}"
            logger.info(f"  Excluding vendor '{vendor_name}' from Level {parent_level + 1}: not successfully classified at '{parent_key}'. Reason: {reason}.")
            excluded_count += 1

    logger.info(f"group_by_parent_category: Finished grouping for Level {parent_level + 1}. Created {len(grouped)} groups, included {grouped_count} vendors, excluded {excluded_count} vendors.")
    return grouped

# --- Core Processing Logic (Moved from classification_tasks.py) ---

@log_function_call(logger, include_args=False) # Keep args=False
async def process_batch(
    batch_data: List[Dict[str, Any]], # Pass list of dicts including optional fields
    level: int,
    parent_category_id: Optional[str],
    taxonomy: Taxonomy,
    llm_service: LLMService,
    stats: Dict[str, Any],
    search_context: Optional[Dict[str, Any]] = None # ADDED: Optional search context
) -> Dict[str, Dict]:
    """
    Process a batch of vendors for a specific classification level (1-5), including taxonomy validation.
    Optionally uses search context for post-search classification attempts.
    Updates stats dictionary in place. Passes full vendor data and context to LLM.
    Returns results for the batch.
    """
    results = {}
    if not batch_data:
        logger.warning(f"process_batch called with empty batch_data for Level {level}, Parent '{parent_category_id}'.")
        return results

    batch_names = [vd.get('vendor_name', f'Unknown_{i}') for i, vd in enumerate(batch_data)] # For logging
    context_type = "Search Context" if search_context else "Initial Data"

    logger.info(f"process_batch: Starting Level {level} batch using {context_type}.",
                extra={"batch_size": len(batch_data), "parent_category_id": parent_category_id, "first_vendor": batch_names[0] if batch_names else 'N/A'})

    # --- Get valid category IDs for this level/parent (Updated for L5) ---
    valid_category_ids: Set[str] = set()
    category_id_lookup_error = False
    try:
        logger.debug(f"process_batch: Retrieving valid category IDs for Level {level}, Parent '{parent_category_id}'.")
        categories = []
        if level == 1:
            categories = taxonomy.get_level1_categories()
        elif parent_category_id:
            if level == 2:
                categories = taxonomy.get_level2_categories(parent_category_id)
            elif level == 3:
                categories = taxonomy.get_level3_categories(parent_category_id)
            elif level == 4:
                categories = taxonomy.get_level4_categories(parent_category_id)
            elif level == 5:
                categories = taxonomy.get_level5_categories(parent_category_id)
            else:
                    logger.error(f"process_batch: Invalid level {level} requested.")
                    categories = [] # Should not happen
        else: # level > 1 and no parent_category_id
                logger.error(f"process_batch: Parent category ID is required for Level {level} but was not provided.")
                categories = []

        valid_category_ids = {cat.id for cat in categories}

        if not valid_category_ids:
                if level > 1 and parent_category_id:
                    logger.warning(f"process_batch: No valid child categories found or retrieved for Level {level}, Parent '{parent_category_id}'. LLM cannot classify.")
                elif level == 1:
                    logger.error("process_batch: No Level 1 categories found in taxonomy!")
                    category_id_lookup_error = True
        else:
                logger.debug(f"process_batch: Found {len(valid_category_ids)} valid IDs for Level {level}, Parent '{parent_category_id}'. Example: {list(valid_category_ids)[:5]}")

    except Exception as tax_err:
        logger.error(f"process_batch: Error getting valid categories from taxonomy", exc_info=True,
                        extra={"level": level, "parent_category_id": parent_category_id})
        valid_category_ids = set()
        category_id_lookup_error = True

    # --- Call LLM ---
    llm_response_data = None
    try:
        logger.info(f"process_batch: Calling LLM for Level {level}, Parent '{parent_category_id or 'None'}', Context: {context_type}")
        with LogTimer(logger, f"LLM classification - Level {level}, Parent '{parent_category_id or 'None'}' ({context_type})", include_in_stats=True):
            llm_response_data = await llm_service.classify_batch(
                batch_data=batch_data,
                level=level,
                taxonomy=taxonomy,
                parent_category_id=parent_category_id,
                search_context=search_context
            )
        logger.info(f"process_batch: LLM call completed for Level {level}, Parent '{parent_category_id or 'None'}'.")

        if llm_response_data and isinstance(llm_response_data.get("usage"), dict):
            usage = llm_response_data["usage"]
            stats["api_usage"]["openrouter_calls"] += 1
            stats["api_usage"]["openrouter_prompt_tokens"] += usage.get("prompt_tokens", 0)
            stats["api_usage"]["openrouter_completion_tokens"] += usage.get("completion_tokens", 0)
            stats["api_usage"]["openrouter_total_tokens"] += usage.get("total_tokens", 0)
            logger.debug(f"process_batch: LLM API usage updated", extra=usage)
        else:
            logger.warning("process_batch: LLM response missing or has invalid usage data.")

        if llm_response_data is None:
                logger.error("process_batch: Received None response from llm_service.classify_batch. Cannot process results.")
                raise ValueError("LLM service returned None, indicating a failure in the call.")

        llm_result = llm_response_data.get("result", {})
        classifications = llm_result.get("classifications", [])
        if not isinstance(classifications, list):
                logger.warning("LLM response 'classifications' is not a list.", extra={"response_preview": str(llm_result)[:500]})
                classifications = []

        logger.debug(f"process_batch: Received {len(classifications)} classifications from LLM for batch size {len(batch_data)} at Level {level}.")
        if llm_result.get("batch_id_mismatch"):
                logger.warning(f"process_batch: Processing batch despite batch_id mismatch warning from LLM service.")

        # --- Validate and process results ---
        processed_vendors_in_response = set()
        for classification in classifications:
            if not isinstance(classification, dict):
                logger.warning("Invalid classification item format received from LLM (not a dict)", extra={"item": classification})
                continue

            vendor_name = classification.get("vendor_name")
            if not vendor_name:
                logger.warning("Classification received without vendor_name", extra={"classification": classification})
                continue

            target_vendor_name = vendor_name
            processed_vendors_in_response.add(target_vendor_name)

            category_id = classification.get("category_id", "N/A")
            category_name = classification.get("category_name", "N/A")
            confidence = classification.get("confidence", 0.0)
            classification_not_possible = classification.get("classification_not_possible", False)
            reason = classification.get("classification_not_possible_reason")
            notes = classification.get("notes")
            is_valid_category = True

            # --- TAXONOMY VALIDATION ---
            if not classification_not_possible and not category_id_lookup_error and valid_category_ids:
                if category_id not in valid_category_ids:
                    is_valid_category = False
                    logger.warning(f"Invalid category ID '{category_id}' returned by LLM for vendor '{target_vendor_name}' at level {level}, parent '{parent_category_id}'.",
                                    extra={"valid_ids": list(valid_category_ids)[:10]})
                    classification_not_possible = True
                    reason = f"Invalid category ID '{category_id}' returned by LLM (Valid examples: {list(valid_category_ids)[:3]})"
                    confidence = 0.0
                    category_id = "N/A"
                    category_name = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1
                else:
                        logger.debug(f"Category ID '{category_id}' for '{target_vendor_name}' is valid for Level {level}, Parent '{parent_category_id}'.")
            elif not classification_not_possible and category_id_lookup_error:
                    logger.warning(f"Cannot validate category ID '{category_id}' for '{target_vendor_name}' due to earlier taxonomy lookup error.")
            elif not classification_not_possible and not valid_category_ids and level > 1:
                    logger.warning(f"Cannot validate category ID '{category_id}' for '{target_vendor_name}' because no valid child categories were found for parent '{parent_category_id}'.")
                    is_valid_category = False
                    classification_not_possible = True
                    reason = f"LLM returned category '{category_id}' but no valid children found for parent '{parent_category_id}'."
                    confidence = 0.0
                    category_id = "N/A"; category_name = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1
            # --- End TAXONOMY VALIDATION ---

            # --- Consistency Checks ---
            if classification_not_possible and confidence > 0.0:
                logger.warning("Correcting confidence to 0.0 for classification_not_possible=true", extra={"vendor": target_vendor_name})
                confidence = 0.0
            if not classification_not_possible and is_valid_category and (category_id == "N/A" or not category_id):
                    logger.warning("Classification marked possible by LLM but category ID is 'N/A' or empty", extra={"vendor": target_vendor_name, "classification": classification})
                    classification_not_possible = True
                    reason = reason or "Missing category ID despite LLM success claim"
                    confidence = 0.0
                    category_id = "N/A"
                    category_name = "N/A"
            # --- End Consistency Checks ---

            results[target_vendor_name] = {
                "category_id": category_id,
                "category_name": category_name,
                "confidence": confidence,
                "classification_not_possible": classification_not_possible,
                "classification_not_possible_reason": reason,
                "notes": notes,
                "vendor_name": target_vendor_name
            }
            logger.debug(f"process_batch: Processed result for '{target_vendor_name}' at Level {level}. Possible: {not classification_not_possible}, ID: {category_id}")

        # Handle missing vendors from batch
        missing_vendors = set(batch_names) - processed_vendors_in_response
        if missing_vendors:
            logger.warning(f"LLM response did not include results for all vendors in the batch.", extra={"missing_vendors": list(missing_vendors), "level": level})
            for vendor_name in missing_vendors:
                results[vendor_name] = {
                    "category_id": "N/A", "category_name": "N/A", "confidence": 0.0,
                    "classification_not_possible": True,
                    "classification_not_possible_reason": "Vendor missing from LLM response batch",
                    "notes": None,
                    "vendor_name": vendor_name
                }

    except Exception as e:
        logger.error(f"Failed to process batch at Level {level} ({context_type})", exc_info=True,
                        extra={"batch_names": batch_names, "error": str(e)})
        for vendor_name in batch_names:
            results[vendor_name] = {
                "category_id": "ERROR", "category_name": "ERROR", "confidence": 0.0,
                "classification_not_possible": True,
                "classification_not_possible_reason": f"Batch processing error: {str(e)[:100]}",
                "notes": None,
                "vendor_name": vendor_name
            }
    logger.info(f"process_batch: Finished Level {level} batch for parent '{parent_category_id or 'None'}'. Returning {len(results)} results.")
    return results


@log_function_call(logger, include_args=False)
async def search_and_classify_recursively(
    vendor_data: Dict[str, Any],
    taxonomy: Taxonomy,
    llm_service: LLMService,
    search_service: SearchService,
    stats: Dict[str, Any],
    semaphore: asyncio.Semaphore,
    target_level: int # <<< ADDED target_level
) -> Dict[str, Any]:
    """
    Performs Tavily search, attempts L1 classification, and then recursively
    attempts L2 up to target_level classification using the search context.
    Controlled by semaphore.
    Returns the search_result_data dictionary, potentially augmented with
    classification results (keyed as classification_l1, classification_l2, etc.).
    """
    vendor_name = vendor_data.get('vendor_name', 'UnknownVendor')
    logger.debug(f"search_and_classify_recursively: Waiting to acquire semaphore for vendor '{vendor_name}'.")
    async with semaphore: # Limit concurrency
        logger.info(f"search_and_classify_recursively: Acquired semaphore. Starting for vendor '{vendor_name}' up to Level {target_level}.")
        search_result_data = {
            "vendor": vendor_name,
            "search_query": f"{vendor_name} company business type industry",
            "sources": [],
            "summary": None,
            "error": None,
            # Keys for storing classification results obtained via search
            "classification_l1": None,
            "classification_l2": None,
            "classification_l3": None,
            "classification_l4": None,
            "classification_l5": None
            }

        # --- 1. Perform Tavily Search ---
        try:
            logger.debug(f"search_and_classify_recursively: Calling search_service.search_vendor for '{vendor_name}'.")
            with LogTimer(logger, f"Tavily search for '{vendor_name}'", include_in_stats=True):
                tavily_response = await search_service.search_vendor(vendor_name)
            logger.debug(f"search_and_classify_recursively: search_service.search_vendor returned for '{vendor_name}'.")

            stats["api_usage"]["tavily_search_calls"] += 1
            search_result_data.update(tavily_response) # Update with actual search results or error

            source_count = len(search_result_data.get("sources", []))
            if search_result_data.get("error"):
                logger.warning(f"search_and_classify_recursively: Search failed", extra={"vendor": vendor_name, "error": search_result_data["error"]})
                search_result_data["classification_l1"] = {
                        "classification_not_possible": True,
                        "classification_not_possible_reason": f"Search error: {str(search_result_data['error'])[:100]}",
                        "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed"
                }
                logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to search error for '{vendor_name}'.")
                return search_result_data # Stop if search failed
            else:
                logger.info(f"search_and_classify_recursively: Search completed", extra={"vendor": vendor_name, "source_count": source_count, "summary_present": bool(search_result_data.get('summary'))})

        except Exception as search_exc:
            logger.error(f"search_and_classify_recursively: Unexpected error during Tavily search for {vendor_name}", exc_info=True)
            search_result_data["error"] = f"Unexpected search error: {str(search_exc)}"
            search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": f"Search task error: {str(search_exc)[:100]}",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed"
                }
            logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to search exception for '{vendor_name}'.")
            return search_result_data # Stop if search failed

        # --- 2. Attempt L1 Classification using Search Results ---
        search_content_available = search_result_data.get("sources") or search_result_data.get("summary")
        if not search_content_available:
            logger.warning(f"search_and_classify_recursively: No usable search results found for vendor, cannot classify", extra={"vendor": vendor_name})
            search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": "No search results content found",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "No Search Content"
            }
            logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to no search content for '{vendor_name}'.")
            return search_result_data # Stop if no content

        valid_l1_category_ids: Set[str] = set(taxonomy.categories.keys())
        llm_response_l1 = None
        try:
            logger.debug(f"search_and_classify_recursively: Calling llm_service.process_search_results (L1) for '{vendor_name}'.")
            with LogTimer(logger, f"LLM L1 classification from search for '{vendor_name}'", include_in_stats=True):
                llm_response_l1 = await llm_service.process_search_results(vendor_data, search_result_data, taxonomy)
            logger.debug(f"search_and_classify_recursively: llm_service.process_search_results (L1) returned for '{vendor_name}'.")

            if llm_response_l1 is None:
                    logger.error("search_and_classify_recursively: Received None response from llm_service.process_search_results. Cannot process L1.")
                    raise ValueError("LLM service (process_search_results) returned None.")

            if isinstance(llm_response_l1.get("usage"), dict):
                usage = llm_response_l1["usage"]
                stats["api_usage"]["openrouter_calls"] += 1
                stats["api_usage"]["openrouter_prompt_tokens"] += usage.get("prompt_tokens", 0)
                stats["api_usage"]["openrouter_completion_tokens"] += usage.get("completion_tokens", 0)
                stats["api_usage"]["openrouter_total_tokens"] += usage.get("total_tokens", 0)

            l1_classification = llm_response_l1.get("result", {})
            if "vendor_name" not in l1_classification: l1_classification["vendor_name"] = vendor_name

            # Validate L1 result
            classification_not_possible_l1 = l1_classification.get("classification_not_possible", True)
            category_id_l1 = l1_classification.get("category_id", "N/A")
            is_valid_l1 = True

            if not classification_not_possible_l1 and valid_l1_category_ids:
                if category_id_l1 not in valid_l1_category_ids:
                    is_valid_l1 = False
                    logger.warning(f"Invalid L1 category ID '{category_id_l1}' from search LLM for '{vendor_name}'.", extra={"valid_ids": list(valid_l1_category_ids)[:10]})
                    l1_classification["classification_not_possible"] = True
                    l1_classification["classification_not_possible_reason"] = f"Invalid L1 category ID '{category_id_l1}' from search."
                    l1_classification["confidence"] = 0.0
                    l1_classification["category_id"] = "N/A"
                    l1_classification["category_name"] = "N/A"
                    stats["invalid_category_errors"] = stats.get("invalid_category_errors", 0) + 1

            if l1_classification.get("classification_not_possible") and l1_classification.get("confidence", 0.0) > 0.0: l1_classification["confidence"] = 0.0
            if not l1_classification.get("classification_not_possible") and not l1_classification.get("category_id", "N/A"):
                    l1_classification["classification_not_possible"] = True
                    l1_classification["classification_not_possible_reason"] = "Missing L1 category ID despite LLM success claim"
                    l1_classification["confidence"] = 0.0
                    l1_classification["category_id"] = "N/A"; l1_classification["category_name"] = "N/A"

            search_result_data["classification_l1"] = l1_classification # Store validated L1 result

        except Exception as llm_err:
                logger.error(f"search_and_classify_recursively: Error during LLM L1 processing for {vendor_name}", exc_info=True)
                search_result_data["error"] = search_result_data.get("error") or f"LLM L1 processing error: {str(llm_err)}"
                search_result_data["classification_l1"] = {
                    "classification_not_possible": True,
                    "classification_not_possible_reason": f"LLM L1 processing error: {str(llm_err)[:100]}",
                    "confidence": 0.0, "vendor_name": vendor_name, "notes": "LLM L1 Error"
                }
                logger.debug(f"search_and_classify_recursively: Releasing semaphore early due to L1 LLM exception for '{vendor_name}'.")
                return search_result_data # Stop if L1 classification failed

        # --- 3. Recursive Classification L2 up to target_level using Search Context ---
        current_parent_id = search_result_data["classification_l1"].get("category_id")
        classification_possible = not search_result_data["classification_l1"].get("classification_not_possible", True)

        # --- UPDATED: Loop up to target_level ---
        if classification_possible and current_parent_id and current_parent_id != "N/A" and target_level > 1:
            logger.info(f"search_and_classify_recursively: L1 successful ({current_parent_id}), proceeding to L2-{target_level} for {vendor_name} using search context.")
            for level in range(2, target_level + 1):
        # --- END UPDATED ---
                logger.debug(f"Attempting post-search Level {level} for {vendor_name}, parent {current_parent_id}")
                try:
                    logger.debug(f"search_and_classify_recursively: Calling process_batch (Level {level}) for '{vendor_name}' with search context.")
                    batch_result_dict = await process_batch(
                        batch_data=[vendor_data], # Batch of one
                        level=level,
                        parent_category_id=current_parent_id,
                        taxonomy=taxonomy,
                        llm_service=llm_service,
                        stats=stats,
                        search_context=search_result_data # Pass the full search results as context
                    )
                    logger.debug(f"search_and_classify_recursively: process_batch (Level {level}) returned for '{vendor_name}'.")

                    level_result = batch_result_dict.get(vendor_name)
                    if level_result:
                        search_result_data[f"classification_l{level}"] = level_result # Store result
                        if level_result.get("classification_not_possible", True):
                            logger.info(f"Post-search classification stopped at Level {level} for {vendor_name}. Reason: {level_result.get('classification_not_possible_reason')}")
                            break # Stop recursion if classification fails
                        else:
                            current_parent_id = level_result.get("category_id") # Update parent for next level
                            if not current_parent_id or current_parent_id == "N/A":
                                logger.warning(f"Post-search Level {level} successful but returned invalid parent_id '{current_parent_id}' for {vendor_name}. Stopping recursion.")
                                break
                    else:
                        logger.error(f"Post-search Level {level} batch processing did not return result for {vendor_name}. Stopping recursion.")
                        search_result_data[f"classification_l{level}"] = {
                                "classification_not_possible": True,
                                "classification_not_possible_reason": f"Missing result from L{level} post-search batch",
                                "confidence": 0.0, "vendor_name": vendor_name, "notes": f"L{level} Error"
                        }
                        break

                except Exception as recursive_err:
                    logger.error(f"search_and_classify_recursively: Error during post-search Level {level} for {vendor_name}", exc_info=True)
                    search_result_data[f"classification_l{level}"] = {
                            "classification_not_possible": True,
                            "classification_not_possible_reason": f"L{level} processing error: {str(recursive_err)[:100]}",
                            "confidence": 0.0, "vendor_name": vendor_name, "notes": f"L{level} Error"
                    }
                    break # Stop recursion on error
        else:
            logger.info(f"search_and_classify_recursively: L1 classification failed or not possible for {vendor_name}, or target level is 1. Skipping L2-{target_level}.")

        logger.info(f"search_and_classify_recursively: Finished for vendor", extra={"vendor": vendor_name})
        logger.debug(f"search_and_classify_recursively: Releasing semaphore for vendor '{vendor_name}'.")
        return search_result_data


@log_function_call(logger, include_args=False) # Keep args=False
async def process_vendors(
    unique_vendors_map: Dict[str, Dict[str, Any]], # Pass map containing full vendor data
    taxonomy: Taxonomy,
    results: Dict[str, Dict],
    stats: Dict[str, Any],
    job: Job,
    db: Session,
    llm_service: LLMService,
    search_service: SearchService,
    target_level: int # <<< ADDED target_level
):
    """
    Main orchestration function for processing vendors through the classification workflow (up to target_level),
    including recursive search for unknowns (up to target_level). Updates results and stats dictionaries in place.
    """
    unique_vendor_names = list(unique_vendors_map.keys()) # Get names from map
    total_unique_vendors = len(unique_vendor_names)
    processed_count = 0 # Count unique vendors processed in batches

    logger.info(f"Starting classification loop for {total_unique_vendors} unique vendors up to target Level {target_level}.")

    # --- Initial Hierarchical Classification (Levels 1 to target_level) ---
    vendors_to_process_next_level_names = set(unique_vendor_names) # Start with all unique vendor names for Level 1
    initial_l4_success_count = 0 # Track L4 for stats
    initial_l5_success_count = 0 # Track L5 for stats

    # --- UPDATED: Loop up to target_level ---
    for level in range(1, target_level + 1):
    # --- END UPDATED ---
        if not vendors_to_process_next_level_names:
            logger.info(f"No vendors remaining to process for Level {level}. Skipping.")
            continue # Skip level if no vendors need processing

        current_vendors_for_this_level = list(vendors_to_process_next_level_names) # Copy names for processing this level
        vendors_successfully_classified_in_level_names = set() # Track vendors that pass this level

        stage_enum_name = f"CLASSIFICATION_L{level}"
        if hasattr(ProcessingStage, stage_enum_name):
                job.current_stage = getattr(ProcessingStage, stage_enum_name).value
        else:
                logger.error(f"ProcessingStage enum does not have member '{stage_enum_name}'. Using default.")
                job.current_stage = ProcessingStage.PROCESSING.value # Fallback

        # Adjust progress calculation based on target_level (distribute 0.7 across target_level steps)
        progress_per_level = 0.7 / target_level if target_level > 0 else 0.7
        job.progress = min(0.8, 0.1 + ((level - 1) * progress_per_level))
        logger.info(f"[process_vendors] Committing status update before Level {level}: {job.status}, {job.current_stage}, {job.progress:.3f}")
        db.commit()
        logger.info(f"===== Starting Initial Level {level} Classification =====",
                    extra={ "vendors_to_process": len(current_vendors_for_this_level), "progress": job.progress })

        if level == 1:
            grouped_vendors_names = { None: current_vendors_for_this_level }
            logger.info(f"Level 1: Processing all {len(current_vendors_for_this_level)} vendors.")
        else:
            logger.info(f"Level {level}: Grouping {len(current_vendors_for_this_level)} vendors based on Level {level-1} results.")
            grouped_vendors_names = group_by_parent_category(results, level - 1, current_vendors_for_this_level)
            logger.info(f"Level {level}: Created {len(grouped_vendors_names)} groups for processing.")
            for parent_id, names in grouped_vendors_names.items():
                    logger.debug(f"  Group Parent ID '{parent_id}': {len(names)} vendors")

        processed_in_level_count = 0
        batch_counter_for_level = 0
        total_batches_for_level = sum( (len(names) + settings.BATCH_SIZE - 1) // settings.BATCH_SIZE for names in grouped_vendors_names.values() )
        logger.info(f"Level {level}: Total batches to process: {total_batches_for_level}")

        for parent_category_id, group_vendor_names in grouped_vendors_names.items():
            if not group_vendor_names:
                logger.debug(f"Skipping empty group for parent '{parent_category_id}' at Level {level}.")
                continue

            logger.info(f"Processing Level {level} group",
                        extra={"parent_category_id": parent_category_id, "vendor_count": len(group_vendor_names)})

            group_vendor_data = [unique_vendors_map[name] for name in group_vendor_names if name in unique_vendors_map]
            level_batches_data = create_batches(group_vendor_data, batch_size=settings.BATCH_SIZE)
            logger.debug(f"Created {len(level_batches_data)} batches for group '{parent_category_id}' at Level {level}.")

            for i, batch_data in enumerate(level_batches_data):
                batch_counter_for_level += 1
                batch_names = [vd['vendor_name'] for vd in batch_data] # Get names for logging
                logger.info(f"Processing Level {level} batch {i+1}/{len(level_batches_data)} for parent '{parent_category_id or 'None'}'",
                            extra={"batch_size": len(batch_data), "first_vendor": batch_names[0] if batch_names else 'N/A', "batch_num": batch_counter_for_level, "total_batches": total_batches_for_level})
                try:
                    # Process batch WITHOUT search context initially
                    batch_results = await process_batch(batch_data, level, parent_category_id, taxonomy, llm_service, stats, search_context=None)
                    logger.debug(f"Level {level} batch {i+1} results received. Count: {len(batch_results)}.")

                    for vendor_name, classification in batch_results.items():
                        if vendor_name in results:
                            results[vendor_name][f"level{level}"] = classification
                            processed_in_level_count += 1

                            if not classification.get("classification_not_possible", True):
                                vendors_successfully_classified_in_level_names.add(vendor_name)
                                logger.debug(f"Vendor '{vendor_name}' successfully classified at Level {level} (ID: {classification.get('category_id')}). Added for L{level+1}.")
                                # Update stats based on the actual level completed
                                if level == 4:
                                    initial_l4_success_count += 1
                                    logger.debug(f"Incremented initial_l4_success_count for {vendor_name}. Current L4 count: {initial_l4_success_count}")
                                if level == 5:
                                    initial_l5_success_count += 1
                                    logger.debug(f"Incremented initial_l5_success_count for {vendor_name}. Current L5 count: {initial_l5_success_count}")
                            else:
                                logger.debug(f"Vendor '{vendor_name}' not successfully classified at Level {level}. Reason: {classification.get('classification_not_possible_reason', 'Unknown')}. Will not proceed.")
                        else:
                                logger.warning(f"Vendor '{vendor_name}' from batch result not found in main results dictionary.", extra={"level": level})

                except Exception as batch_error:
                    logger.error(f"Error during initial batch processing logic (Level {level}, parent '{parent_category_id or 'None'}')", exc_info=True,
                                    extra={"batch_vendors": batch_names, "error": str(batch_error)})
                    for vendor_name in batch_names:
                            if vendor_name in results:
                                if f"level{level}" not in results[vendor_name]:
                                    results[vendor_name][f"level{level}"] = {
                                        "category_id": "ERROR", "category_name": "ERROR", "confidence": 0.0,
                                        "classification_not_possible": True,
                                        "classification_not_possible_reason": f"Batch processing logic error: {str(batch_error)[:100]}",
                                        "vendor_name": vendor_name
                                    }
                                    processed_in_level_count += 1
                                else: 
                                    logger.warning(f"Vendor '{vendor_name}' from failed batch not found in main results dictionary.", extra={"level": level})

                # Update progress within the level (based on batches completed)
                level_progress_fraction = batch_counter_for_level / total_batches_for_level if total_batches_for_level > 0 else 1
                job.progress = min(0.8, 0.1 + ((level - 1) * progress_per_level) + (progress_per_level * level_progress_fraction))
                try:
                    logger.info(f"[process_vendors] Committing progress update after batch {batch_counter_for_level}/{total_batches_for_level} (Level {level}): {job.progress:.3f}")
                    db.commit()
                except Exception as db_err:
                        logger.error("Failed to commit progress update during batch processing", exc_info=True)
                        db.rollback()

        logger.info(f"===== Initial Level {level} Classification Completed =====")
        logger.info(f"  Processed {processed_in_level_count} vendor results at Level {level}.")
        logger.info(f"  {len(vendors_successfully_classified_in_level_names)} vendors successfully classified and validated at Level {level}, proceeding to L{level+1}.")
        logger.debug(f"Vendors proceeding to Level {level+1}: {list(vendors_successfully_classified_in_level_names)[:10]}...") # Log first 10
        vendors_to_process_next_level_names = vendors_successfully_classified_in_level_names

    # --- End of Initial Level Loop ---
    logger.info(f"===== Finished Initial Hierarchical Classification Loop (Up to Level {target_level}) =====")

    # --- Identify vendors needing search (those not successfully classified at target_level) ---
    unknown_vendors_data_to_search = []
    for vendor_name in unique_vendor_names:
        is_classified_target = False
        if vendor_name in results:
            target_level_result = results[vendor_name].get(f"level{target_level}")
            if target_level_result and not target_level_result.get("classification_not_possible", False):
                    is_classified_target = True

        if not is_classified_target:
            logger.debug(f"Vendor '{vendor_name}' did not initially reach/pass target Level {target_level} classification. Adding to search list.")
            if vendor_name in unique_vendors_map:
                unknown_vendors_data_to_search.append(unique_vendors_map[vendor_name])
            else:
                logger.warning(f"Vendor '{vendor_name}' marked for search but not found in unique_vendors_map.")
                unknown_vendors_data_to_search.append({'vendor_name': vendor_name})

    stats["classification_not_possible_initial"] = len(unknown_vendors_data_to_search)
    stats["successfully_classified_l4"] = initial_l4_success_count # Store initial L4 count
    stats["successfully_classified_l5"] = initial_l5_success_count # Store initial L5 count

    logger.info(f"Initial Classification Summary (Target L{target_level}): {total_unique_vendors - stats['classification_not_possible_initial']} reached target, {stats['classification_not_possible_initial']} did not.")
    if target_level >= 4: logger.info(f"  Ref: {initial_l4_success_count} reached L4 initially.")
    if target_level >= 5: logger.info(f"  Ref: {initial_l5_success_count} reached L5 initially.")

    # --- Search and Recursive Classification for Unknown Vendors (up to target_level) ---
    if unknown_vendors_data_to_search:
        job.current_stage = ProcessingStage.SEARCH.value
        job.progress = 0.8 # Progress after initial classification attempts
        logger.info(f"[process_vendors] Committing status update before Search stage: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"===== Starting Search and Recursive Classification for {stats['classification_not_possible_initial']} Unclassified Vendors (Up to Level {target_level}) =====")

        stats["search_attempts"] = len(unknown_vendors_data_to_search)

        search_tasks = []
        if MAX_CONCURRENT_SEARCHES <= 0:
            logger.error(f"MAX_CONCURRENT_SEARCHES is {MAX_CONCURRENT_SEARCHES}. Cannot proceed with search tasks.")
            raise ValueError("MAX_CONCURRENT_SEARCHES must be positive.")
        search_semaphore = asyncio.Semaphore(MAX_CONCURRENT_SEARCHES)
        logger.info(f"Created search semaphore with concurrency limit: {MAX_CONCURRENT_SEARCHES}")

        for vendor_data in unknown_vendors_data_to_search:
            task = asyncio.create_task(
                search_and_classify_recursively(
                    vendor_data, taxonomy, llm_service, search_service, stats, search_semaphore, target_level # Pass target_level
                )
            )
            search_tasks.append(task)

        logger.info(f"Gathering results for {len(search_tasks)} search & recursive classification tasks...")
        logger.debug(f"Starting asyncio.gather for {len(search_tasks)} tasks.")
        gather_start_time = time.monotonic()
        search_and_recursive_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        gather_duration = time.monotonic() - gather_start_time
        logger.info(f"Search & recursive classification tasks completed (asyncio.gather finished). Duration: {gather_duration:.3f}s")

        job.progress = 0.95 # Indicate search phase is done, before result processing/generation
        logger.info(f"[process_vendors] Committing progress update after search gather: {job.progress:.3f}")
        db.commit()

        successful_l1_searches = 0
        successful_l5_searches = 0 # Track L5 success via search
        processed_search_count = 0

        logger.info(f"Processing {len(search_and_recursive_results)} results from search/recursive tasks.")
        for i, result_or_exc in enumerate(search_and_recursive_results):
            processed_search_count += 1
            if i >= len(unknown_vendors_data_to_search):
                    logger.error(f"Search result index {i} out of bounds for unknown_vendors list.")
                    continue
            vendor_data = unknown_vendors_data_to_search[i]
            vendor_name = vendor_data.get('vendor_name', f'UnknownVendor_{i}')

            if vendor_name not in results:
                    logger.warning(f"Vendor '{vendor_name}' from search task not found in main results dict. Initializing.")
                    results[vendor_name] = {}

            results[vendor_name]["search_attempted"] = True # Add flag

            if isinstance(result_or_exc, Exception):
                logger.error(f"Error during search_and_classify_recursively for vendor {vendor_name}", exc_info=result_or_exc)
                results[vendor_name]["search_results"] = {"error": f"Search/Recursive task error: {str(result_or_exc)}"}
                if "level1" not in results[vendor_name] or results[vendor_name]["level1"].get("classification_not_possible", True):
                    results[vendor_name]["level1"] = {
                        "classification_not_possible": True,
                        "classification_not_possible_reason": f"Search task error: {str(result_or_exc)[:100]}",
                        "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed"
                    }
            elif isinstance(result_or_exc, dict):
                search_data = result_or_exc
                results[vendor_name]["search_results"] = search_data # Store raw search info

                l1_classification = search_data.get("classification_l1")
                if l1_classification and not l1_classification.get("classification_not_possible", True):
                    successful_l1_searches += 1
                    logger.info(f"Vendor '{vendor_name}' classified via search (Level 1: {l1_classification.get('category_id')}).")
                    results[vendor_name]["classified_via_search"] = True # Add flag

                    if not results[vendor_name].get("level1") or results[vendor_name]["level1"].get("classification_not_possible", True):
                            results[vendor_name]["level1"] = l1_classification
                            if "notes" not in results[vendor_name]["level1"]: results[vendor_name]["level1"]["notes"] = ""
                            results[vendor_name]["level1"]["notes"] = f"Classified via search: {results[vendor_name]['level1']['notes']}"

                    # Store L2-L5 results obtained recursively (up to target_level)
                    # --- UPDATED: Loop up to target_level ---
                    for lvl in range(2, target_level + 1):
                    # --- END UPDATED ---
                        lvl_key = f"classification_l{lvl}"
                        if lvl_key in search_data:
                            results[vendor_name][f"level{lvl}"] = search_data[lvl_key]
                            # Check if L5 was successfully reached post-search (only if target was >= 5)
                            if lvl == 5 and target_level >= 5 and not search_data[lvl_key].get("classification_not_possible", True):
                                successful_l5_searches += 1
                                logger.info(f"Vendor '{vendor_name}' reached L5 classification via search.")
                        else:
                            # Explicitly remove higher levels if recursion stopped early or target was lower
                            results[vendor_name].pop(f"level{lvl}", None)
                            if f"classification_l{lvl-1}" in search_data:
                                logger.debug(f"Recursion for {vendor_name} stopped before L{lvl}, likely due to L{lvl-1} failure or no children.")

                else: # L1 classification via search failed or wasn't possible
                    reason = "Search did not yield L1 classification"
                    if l1_classification: reason = l1_classification.get("classification_not_possible_reason", reason)
                    logger.info(f"Vendor '{vendor_name}' could not be classified via search at L1. Reason: {reason}")
                    if not results[vendor_name].get("level1") or results[vendor_name]["level1"].get("classification_not_possible", True):
                        results[vendor_name]["level1"] = {
                            "classification_not_possible": True,
                            "classification_not_possible_reason": reason,
                            "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Failed L1"
                        }
                    # --- UPDATED: Loop up to target_level ---
                    for lvl in range(2, target_level + 1): results[vendor_name].pop(f"level{lvl}", None)
                    # --- END UPDATED ---

            else: # Handle unexpected return type
                logger.error(f"Unexpected result type for vendor {vendor_name} search task: {type(result_or_exc)}")
                results[vendor_name]["search_results"] = {"error": f"Unexpected search result type: {type(result_or_exc)}"}
                if "level1" not in results[vendor_name] or results[vendor_name]["level1"].get("classification_not_possible", True):
                        results[vendor_name]["level1"] = { "classification_not_possible": True, "classification_not_possible_reason": "Internal search error", "confidence": 0.0, "vendor_name": vendor_name, "notes": "Search Error" }
                # --- UPDATED: Loop up to target_level ---
                for lvl in range(2, target_level + 1): results[vendor_name].pop(f"level{lvl}", None)
                # --- END UPDATED ---

        stats["search_successful_classifications_l1"] = successful_l1_searches
        stats["search_successful_classifications_l5"] = successful_l5_searches # Updated stat
        # Update total L5 success count (if target was >= 5)
        if target_level >= 5:
            stats["successfully_classified_l5"] = initial_l5_success_count + successful_l5_searches

        logger.info(f"===== Unknown Vendor Search & Recursive Classification Completed =====")
        logger.info(f"  Attempted search for {stats['search_attempts']} vendors.")
        logger.info(f"  Successfully classified {successful_l1_searches} at L1 via search.")
        if target_level >= 5:
            logger.info(f"  Successfully classified {successful_l5_searches} at L5 via search.")
            logger.info(f"  Total vendors successfully classified at L5: {stats['successfully_classified_l5']}")
    else:
        logger.info("No unknown vendors required search.")
        job.progress = 0.95 # Set progress high if search wasn't needed
        logger.info(f"[process_vendors] Committing status update as search was skipped: {job.progress:.3f}")
        db.commit()
    logger.info("process_vendors function is returning.")
</file>

<file path='app/tasks/classification_prompts.py'>

# app/prompts/classification_prompts.py
import json
import logging
from typing import List, Dict, Any, Optional

from models.taxonomy import Taxonomy, TaxonomyCategory

# Configure logger for this module
logger = logging.getLogger("vendor_classification.prompts")

def generate_batch_prompt(
    vendors_data: List[Dict[str, Any]],
    level: int,
    taxonomy: Taxonomy,
    parent_category_id: Optional[str] = None,
    batch_id: str = "unknown-batch",
    search_context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Create an appropriate prompt for the current classification level (1-5),
    optionally including search context for post-search classification.
    """
    context_type = "Search Context" if search_context else "Initial Data"
    logger.debug(f"generate_batch_prompt: Generating prompt for Level {level} using {context_type}",
                extra={ "vendor_count": len(vendors_data), "parent_category_id": parent_category_id, "batch_id": batch_id, "has_search_context": bool(search_context) })

    # --- Build Vendor Data Section ---
    vendor_data_xml = "<vendor_data>\n"
    for i, vendor_entry in enumerate(vendors_data):
        vendor_name = vendor_entry.get('vendor_name', f'UnknownVendor_{i}')
        example = vendor_entry.get('example')
        address = vendor_entry.get('vendor_address')
        website = vendor_entry.get('vendor_website')
        internal_cat = vendor_entry.get('internal_category')
        parent_co = vendor_entry.get('parent_company')
        spend_cat = vendor_entry.get('spend_category')

        vendor_data_xml += f"  <vendor index=\"{i+1}\">\n"
        vendor_data_xml += f"    <name>{vendor_name}</name>\n"
        if example: vendor_data_xml += f"    <example_goods_services>{str(example)[:200]}</example_goods_services>\n"
        if address: vendor_data_xml += f"    <address>{str(address)[:200]}</address>\n"
        if website: vendor_data_xml += f"    <website>{str(website)[:100]}</website>\n"
        if internal_cat: vendor_data_xml += f"    <internal_category>{str(internal_cat)[:100]}</internal_category>\n"
        if parent_co: vendor_data_xml += f"    <parent_company>{str(parent_co)[:100]}</parent_company>\n"
        if spend_cat: vendor_data_xml += f"    <spend_category>{str(spend_cat)[:100]}</spend_category>\n"
        vendor_data_xml += f"  </vendor>\n"
    vendor_data_xml += "</vendor_data>"

    # --- Build Search Context Section ---
    search_context_xml = ""
    if search_context and level > 1:
        logger.debug(f"Including search context in prompt for Level {level}", extra={"batch_id": batch_id})
        search_context_xml += "<search_context>\n"
        summary = search_context.get("summary")
        sources = search_context.get("sources")
        if summary:
            search_context_xml += f"  <summary>{str(summary)[:1000]}</summary>\n" # Limit length
        if sources and isinstance(sources, list):
            search_context_xml += "  <sources>\n"
            for j, source in enumerate(sources[:3]): # Limit to top 3 sources for brevity
                title = source.get('title', 'N/A')
                url = source.get('url', 'N/A')
                content_preview = str(source.get('content', ''))[:500] # Limit length
                search_context_xml += f"    <source index=\"{j+1}\">\n"
                search_context_xml += f"      <title>{title}</title>\n"
                search_context_xml += f"      <url>{url}</url>\n"
                search_context_xml += f"      <content_snippet>{content_preview}...</content_snippet>\n"
                search_context_xml += f"    </source>\n"
            search_context_xml += "  </sources>\n"
        else:
             search_context_xml += "  <message>No relevant search results sources were provided.</message>\n"
        search_context_xml += "</search_context>\n"

    # --- Get Category Options ---
    categories: List[TaxonomyCategory] = []
    parent_category_name = "N/A"
    category_lookup_successful = True
    try:
        logger.debug(f"generate_batch_prompt: Retrieving categories via taxonomy methods for Level {level}, Parent: {parent_category_id}")
        parent_obj = None
        if level == 1:
            categories = taxonomy.get_level1_categories()
        elif parent_category_id:
            if level == 2:
                categories = taxonomy.get_level2_categories(parent_category_id)
                parent_obj = taxonomy.categories.get(parent_category_id)
            elif level == 3:
                categories = taxonomy.get_level3_categories(parent_category_id)
                l1_id, l2_id = parent_category_id.split('.') if '.' in parent_category_id else (None, parent_category_id)
                if not l1_id:
                    for l1_key, l1_node in taxonomy.categories.items():
                        if l2_id in getattr(l1_node, 'children', {}): l1_id = l1_key; break
                if l1_id: parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id)
            elif level == 4:
                categories = taxonomy.get_level4_categories(parent_category_id)
                l1_id, l2_id, l3_id = parent_category_id.split('.') if parent_category_id.count('.') == 2 else (None, None, parent_category_id)
                if not l1_id:
                     found = False
                     for l1k, l1n in taxonomy.categories.items():
                         for l2k, l2n in getattr(l1n, 'children', {}).items():
                             if l3_id in getattr(l2n, 'children', {}): l1_id = l1k; l2_id = l2k; found = True; break
                         if found: break
                if l1_id and l2_id: parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id, {}).children.get(l3_id)
            elif level == 5:
                categories = taxonomy.get_level5_categories(parent_category_id)
                l1_id, l2_id, l3_id, l4_id = parent_category_id.split('.') if parent_category_id.count('.') == 3 else (None, None, None, parent_category_id)
                if not l1_id:
                    found = False
                    for l1k, l1n in taxonomy.categories.items():
                        for l2k, l2n in getattr(l1n, 'children', {}).items():
                            for l3k, l3n in getattr(l2n, 'children', {}).items():
                                if l4_id in getattr(l3n, 'children', {}): l1_id = l1k; l2_id = l2k; l3_id = l3k; found = True; break
                            if found: break
                        if found: break
                if l1_id and l2_id and l3_id:
                    parent_obj = taxonomy.categories.get(l1_id, {}).children.get(l2_id, {}).children.get(l3_id, {}).children.get(l4_id)

            if parent_obj: parent_category_name = parent_obj.name
        else: # level > 1 and no parent_category_id
            logger.error(f"Parent category ID is required for level {level} prompt generation but was not provided.")
            category_lookup_successful = False

        if not categories and level > 1 and parent_category_id:
             logger.warning(f"No subcategories found for Level {level}, Parent '{parent_category_id}'.")
             if level == 1: category_lookup_successful = False
        elif not categories and level == 1:
             logger.error(f"No Level 1 categories found in taxonomy!")
             category_lookup_successful = False

        logger.debug(f"generate_batch_prompt: Retrieved {len(categories)} categories for Level {level}, Parent '{parent_category_id}' ('{parent_category_name}').")

    except Exception as e:
        logger.error(f"Error retrieving categories for prompt (Level {level}, Parent: {parent_category_id})", exc_info=True)
        category_lookup_successful = False

    # --- Build Category Options Section ---
    category_options_xml = "<category_options>\n"
    if category_lookup_successful:
        category_options_xml += f"  <level>{level}</level>\n"
        if level > 1 and parent_category_id:
            category_options_xml += f"  <parent_id>{parent_category_id}</parent_id>\n"
            category_options_xml += f"  <parent_name>{parent_category_name}</parent_name>\n"
        category_options_xml += "  <categories>\n"
        if categories: # Check if categories list is not empty
            for cat in categories:
                category_options_xml += f"    <category id=\"{cat.id}\" name=\"{cat.name}\"/>\n"
        else:
             category_options_xml += f"    <message>No subcategories available for this level and parent.</message>\n"
        category_options_xml += "  </categories>\n"
    else:
        category_options_xml += f"  <error>Could not retrieve valid categories for Level {level}, Parent '{parent_category_id}'. Classification is not possible.</error>\n"
    category_options_xml += "</category_options>"

    # --- Define Output Format Section ---
    output_format_xml = f"""<output_format>
Respond *only* with a valid JSON object matching this exact schema. Do not include any text before or after the JSON object.

json
{{
  "level": {level},
  "batch_id": "{batch_id}",
  "parent_category_id": {json.dumps(parent_category_id)},
  "classifications": [
    {{
      "vendor_name": "string", // Exact vendor name from input <vendor_data>
      "category_id": "string", // ID from <category_options> or "N/A" if not possible
      "category_name": "string", // Name corresponding to category_id or "N/A"
      "confidence": "float", // 0.0 to 1.0. MUST be 0.0 if classification_not_possible is true.
      "classification_not_possible": "boolean", // true if classification cannot be confidently made from options, false otherwise.
      "classification_not_possible_reason": "string | null", // Brief reason if true (e.g., "Ambiguous", "Insufficient info"), null if false.
      "notes": "string | null" // Optional brief justification or reasoning, especially if confidence is low or not possible.
    }}
    // ... one entry for EACH vendor in <vendor_data>
  ]
}}

</output_format>"""

    # --- Assemble Final Prompt ---
    prompt_base = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>

<task>Classify each vendor provided in `<vendor_data>` into **ONE** appropriate NAICS category from the `<category_options>` for Level {level}. {f"Consider that these vendors belong to the parent category '{parent_category_id}: {parent_category_name}'. " if level > 1 and parent_category_id else ""}</task>"""

    if search_context_xml:
        prompt_base += f"""
<search_context_instruction>You have been provided with additional context from a web search in `<search_context>`. Use this information, along with the original `<vendor_data>`, to make the most accurate classification decision for Level {level}.</search_context_instruction>"""

    prompt_base += f"""
<instructions>
1.  Analyze each vendor's details in `<vendor_data>` {f"and the supplementary information in `<search_context>`" if search_context_xml else ""}.
2.  Compare the vendor's likely primary business activity against the available categories in `<category_options>`.
3.  Assign the **single most specific and appropriate** category ID and name from the list.
4.  Provide a confidence score (0.0 to 1.0).
5.  **CRITICAL:** If the vendor's primary activity is genuinely ambiguous, cannot be determined from the provided information, or does not fit well into *any* of the specific categories listed in `<category_options>`, **DO NOT GUESS**. Instead: Set `classification_not_possible` to `true`, `confidence` to `0.0`, provide a brief `classification_not_possible_reason`, and set `category_id`/`category_name` to "N/A".
6.  If classification *is* possible (`classification_not_possible: false`), ensure `confidence` > 0.0 and `category_id`/`category_name` are populated correctly from `<category_options>`.
7.  Provide brief optional `notes` for reasoning, especially if confidence is low or classification was not possible.
8.  Ensure the `batch_id` in the final JSON output matches the `batch_id` specified in `<output_format>`.
9.  Ensure the output contains an entry for **every** vendor listed in `<vendor_data>`.
10. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>

{vendor_data_xml}
{search_context_xml if search_context_xml else ""}
{category_options_xml}
{output_format_xml}
"""
    prompt = prompt_base

    if not category_lookup_successful:
         prompt = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>
<task>Acknowledge that classification is not possible for the vendors in `<vendor_data>` at Level {level} because the necessary subcategories could not be provided.</task>
<instructions>
1. For **every** vendor listed in `<vendor_data>`, create a classification entry in the final JSON output.
2. In each entry, set `classification_not_possible` to `true`.
3. Set `confidence` to `0.0`.
4. Set `category_id` and `category_name` to "N/A".
5. Set `classification_not_possible_reason` to "No subcategories defined or retrievable for parent {parent_category_id} at Level {level}".
6. Ensure the `batch_id` in the final JSON output matches the `batch_id` specified in `<output_format>`.
7. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>
{vendor_data_xml}
{category_options_xml}
{output_format_xml}
"""

    return prompt


def generate_search_prompt(
    vendor_data: Dict[str, Any],
    search_results: Dict[str, Any],
    taxonomy: Taxonomy,
    attempt_id: str = "unknown-attempt"
) -> str:
    """
    Create a prompt for processing search results, aiming for Level 1 classification.
    """
    logger.debug(f"Entering generate_search_prompt for vendor: {vendor_data.get('vendor_name', 'Unknown')}")
    vendor_name = vendor_data.get('vendor_name', 'UnknownVendor')
    example = vendor_data.get('example')
    address = vendor_data.get('vendor_address')
    website = vendor_data.get('vendor_website')
    internal_cat = vendor_data.get('internal_category')
    parent_co = vendor_data.get('parent_company')
    spend_cat = vendor_data.get('spend_category')

    logger.debug(f"Creating search results prompt for vendor",
                extra={ "vendor": vendor_name, "source_count": len(search_results.get("sources", [])), "attempt_id": attempt_id })

    # --- Build Vendor Data Section ---
    vendor_data_xml = "<vendor_data>\n"
    vendor_data_xml += f"  <name>{vendor_name}</name>\n"
    if example: vendor_data_xml += f"  <example_goods_services>{str(example)[:300]}</example_goods_services>\n"
    if address: vendor_data_xml += f"  <address>{str(address)[:200]}</address>\n"
    if website: vendor_data_xml += f"  <website>{str(website)[:100]}</website>\n"
    if internal_cat: vendor_data_xml += f"  <internal_category>{str(internal_cat)[:100]}</internal_category>\n"
    if parent_co: vendor_data_xml += f"  <parent_company>{str(parent_co)[:100]}</parent_company>\n"
    if spend_cat: vendor_data_xml += f"  <spend_category>{str(spend_cat)[:100]}</spend_category>\n"
    vendor_data_xml += "</vendor_data>"

    # --- Build Search Results Section ---
    search_results_xml = "<search_results>\n"
    sources = search_results.get("sources")
    if sources and isinstance(sources, list):
        search_results_xml += "  <sources>\n"
        for i, source in enumerate(sources):
            content_preview = str(source.get('content', ''))[:1500] # Limit length
            search_results_xml += f"    <source index=\"{i+1}\">\n"
            search_results_xml += f"      <title>{source.get('title', 'N/A')}</title>\n"
            search_results_xml += f"      <url>{source.get('url', 'N/A')}</url>\n"
            search_results_xml += f"      <content_snippet>{content_preview}...</content_snippet>\n"
            search_results_xml += f"    </source>\n"
        search_results_xml += "  </sources>\n"
    else:
        search_results_xml += "  <message>No relevant search results sources were found.</message>\n"

    summary_str = search_results.get("summary", "")
    if summary_str:
        search_results_xml += f"  <summary>{summary_str}</summary>\n"
    search_results_xml += "</search_results>"

    # --- Get Level 1 Category Options ---
    categories = taxonomy.get_level1_categories()
    category_options_xml = "<category_options>\n"
    category_options_xml += "  <level>1</level>\n" # Explicitly state Level 1
    category_options_xml += "  <categories>\n"
    for cat in categories:
        category_options_xml += f"    <category id=\"{cat.id}\" name=\"{cat.name}\"/>\n" # Omit description
    category_options_xml += "  </categories>\n"
    category_options_xml += "</category_options>"

    # --- Define Output Format Section ---
    output_format_xml = f"""<output_format>
Respond *only* with a valid JSON object matching this exact schema. Do not include any text before or after the JSON object.

json
{{
  "attempt_id": "{attempt_id}", // ID for this specific attempt
  "vendor_name": "{vendor_name}", // Exact vendor name from input <vendor_data>
  "category_id": "string", // Level 1 ID from <category_options> or "N/A" if not possible
  "category_name": "string", // Name corresponding to category_id or "N/A"
  "confidence": "float", // 0.0 to 1.0. MUST be 0.0 if classification_not_possible is true.
  "classification_not_possible": "boolean", // true if classification cannot be confidently made from options based *only* on provided info, false otherwise.
  "classification_not_possible_reason": "string | null", // Brief reason if true (e.g., "Insufficient info", "Conflicting sources"), null if false.
  "notes": "string | null" // Brief explanation of decision based *only* on the provided context and search results. Reference specific sources if helpful.
}}

</output_format>"""

    # --- Assemble Final Prompt ---
    prompt = f"""
<role>You are a precise vendor classification expert using the NAICS taxonomy.</role>

<task>Analyze the vendor details in `<vendor_data>` and the web search information in `<search_results>` to classify the vendor into **ONE** appropriate **Level 1** NAICS category from `<category_options>`. Base your decision *only* on the provided information.</task>

<instructions>
1.  Carefully review the vendor details in `<vendor_data>` (name, examples, address, website, internal category, parent company, spend category).
2.  Carefully review the search results in `<search_results>` (sources and summary).
3.  Synthesize all provided information to understand the vendor's **primary business activity**. Focus on what the company *does*, not just what it might resell.
4.  Compare this primary activity against the **Level 1** categories listed in `<category_options>`.
5.  Assign the **single most appropriate** Level 1 category ID and name.
6.  Provide a confidence score (0.0 to 1.0) based on the clarity, consistency, and relevance of the provided information.
7.  **CRITICAL:** If the provided information (vendor data + search results) is insufficient, contradictory, irrelevant, focuses only on products sold rather than the business activity, or does not allow for confident determination of the primary business activity *from the listed L1 categories*, **DO NOT GUESS**. Instead: Set `classification_not_possible` to `true`, `confidence` to `0.0`, provide a brief `classification_not_possible_reason`, and set `category_id`/`category_name` to "N/A".
8.  If classification *is* possible (`classification_not_possible: false`), ensure `confidence` > 0.0 and `category_id`/`category_name` are populated correctly from `<category_options>`.
9.  Provide brief optional `notes` explaining your reasoning, referencing specific details from `<vendor_data>` or `<search_results>`.
10. Ensure the `vendor_name` in the final JSON output matches the name in `<vendor_data>`.
11. Respond *only* with the valid JSON object as specified in `<output_format>`.
</instructions>

{vendor_data_xml}

{search_results_xml}

{category_options_xml}

{output_format_xml}
"""
    return prompt
</file>

<file path='app/tasks/classification_tasks.py'>

# app/tasks/classification_tasks.py
import os
import asyncio
import logging
from datetime import datetime
from celery import shared_task
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from typing import Dict, Any # <<< ADDED IMPORTS

from core.database import SessionLocal
from core.config import settings
from core.logging_config import get_logger
# Import context functions from the new module
from core.log_context import set_correlation_id, set_job_id, set_log_context, clear_all_context
# Import log helpers from utils
from utils.log_utils import LogTimer, log_duration

from models.job import Job, JobStatus, ProcessingStage
from services.file_service import read_vendor_file, normalize_vendor_data, generate_output_file
from services.llm_service import LLMService
from services.search_service import SearchService
from utils.taxonomy_loader import load_taxonomy

# Import the refactored logic
from .classification_logic import process_vendors

# Configure logger
logger = get_logger("vendor_classification.tasks")
# --- ADDED: Log confirmation ---
logger.debug("Successfully imported Dict and Any from typing for classification tasks.")
# --- END ADDED ---

@shared_task(bind=True)
# --- UPDATED: Added target_level parameter ---
def process_vendor_file(self, job_id: str, file_path: str, target_level: int):
# --- END UPDATED ---
    """
    Celery task entry point for processing a vendor file.
    Orchestrates the overall process by calling the main async helper.

    Args:
        job_id: Job ID
        file_path: Path to vendor file
        target_level: The desired maximum classification level (1-5)
    """
    task_id = self.request.id if self.request and self.request.id else "UnknownTaskID"
    logger.info(f"***** process_vendor_file TASK RECEIVED *****",
                extra={
                    "celery_task_id": task_id,
                    "job_id_arg": job_id,
                    "file_path_arg": file_path,
                    "target_level_arg": target_level # Log received target level
                })

    set_correlation_id(job_id) # Set correlation ID early
    set_job_id(job_id)
    set_log_context({"target_level": target_level}) # Add target level to context
    logger.info(f"Starting vendor file processing task (inside function)",
                extra={"job_id": job_id, "file_path": file_path, "target_level": target_level})

    # Validate target_level
    if not 1 <= target_level <= 5:
        logger.error(f"Invalid target_level received: {target_level}. Must be between 1 and 5.")
        # Fail the job immediately if level is invalid
        db_fail = SessionLocal()
        try:
            job_fail = db_fail.query(Job).filter(Job.id == job_id).first()
            if job_fail:
                job_fail.fail(f"Invalid target level specified: {target_level}. Must be 1-5.")
                db_fail.commit()
        except Exception as db_err:
            logger.error("Failed to mark job as failed due to invalid target level", exc_info=db_err)
            db_fail.rollback()
        finally:
            db_fail.close()
        return # Stop task execution

    # Initialize loop within the task context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    logger.debug(f"Created and set new asyncio event loop for job {job_id}")

    db = SessionLocal()
    job = None # Initialize job to None

    try:
        job = db.query(Job).filter(Job.id == job_id).first()
        if job:
            # Verify the target level matches the job record (optional sanity check)
            if job.target_level != target_level:
                logger.warning(f"Task received target_level {target_level} but job record has {job.target_level}. Using task value: {target_level}.")
                # Optionally update job record here if desired, or just proceed with task value

            set_log_context({
                "company_name": job.company_name,
                "creator": job.created_by,
                "file_name": job.input_file_name
                # target_level already set above
            })
            logger.info(f"Processing file for company",
                        extra={"company": job.company_name})
        else:
            logger.error("Job not found in database at start of task!", extra={"job_id": job_id})
            loop.close() # Close loop if job not found
            db.close() # Close db session
            return # Exit task if job doesn't exist

        logger.info(f"About to run async processing for job {job_id}")
        with LogTimer(logger, "Complete file processing", level=logging.INFO, include_in_stats=True):
            # Run the async function within the loop created for this task
            # --- UPDATED: Pass target_level to async helper ---
            loop.run_until_complete(_process_vendor_file_async(job_id, file_path, db, target_level))
            # --- END UPDATED ---

        logger.info(f"Vendor file processing completed successfully (async part finished)")

    except Exception as e:
        logger.error(f"Error processing vendor file task (in main try block)", exc_info=True, extra={"job_id": job_id})
        try:
            # Re-query the job within this exception handler if it wasn't fetched initially or became None
            db_error_session = SessionLocal()
            try:
                job_in_error = db_error_session.query(Job).filter(Job.id == job_id).first()
                if job_in_error:
                    if job_in_error.status != JobStatus.COMPLETED.value:
                        err_msg = f"Task failed: {type(e).__name__}: {str(e)}"
                        job_in_error.fail(err_msg[:2000]) # Limit error message length
                        db_error_session.commit()
                        logger.info(f"Job status updated to failed due to task error",
                                    extra={"error": str(e)})
                    else:
                        logger.warning(f"Task error occurred after job was marked completed, status not changed.",
                                        extra={"error": str(e)})
                else:
                    logger.error("Job not found when trying to mark as failed.", extra={"job_id": job_id})
            except Exception as db_error:
                logger.error(f"Error updating job status during task failure handling", exc_info=True,
                            extra={"original_error": str(e), "db_error": str(db_error)})
                db_error_session.rollback()
            finally:
                    db_error_session.close()
        except Exception as final_db_error:
                logger.critical(f"CRITICAL: Failed even to handle database update in task error handler.", exc_info=final_db_error)

    finally:
        if db: # Close the main session used by the async function
            db.close()
            logger.debug(f"Main database session closed for task.")
        if loop and not loop.is_closed():
            loop.close()
            logger.debug(f"Event loop closed for task.")
        clear_all_context()
        logger.info(f"***** process_vendor_file TASK FINISHED *****", extra={"job_id": job_id})


# --- UPDATED: Added target_level parameter ---
async def _process_vendor_file_async(job_id: str, file_path: str, db: Session, target_level: int):
# --- END UPDATED ---
    """
    Asynchronous part of the vendor file processing.
    Sets up services, initializes stats, calls the core processing logic,
    and handles final result generation and job status updates.
    """
    logger.info(f"[_process_vendor_file_async] Starting async processing for job {job_id} to target level {target_level}")

    llm_service = LLMService()
    search_service = SearchService()

    job = db.query(Job).filter(Job.id == job_id).first()

    if not job:
        logger.error(f"[_process_vendor_file_async] Job not found in database", extra={"job_id": job_id})
        return

    # --- Initialize stats (Updated for L5) ---
    start_time = datetime.now()
    # --- MODIFIED: Type hints added ---
    stats: Dict[str, Any] = {
        "job_id": job.id,
        "company_name": job.company_name,
        "target_level": target_level, # Store target level in stats
        "start_time": start_time.isoformat(),
        "end_time": None,
        "processing_duration_seconds": None,
        "total_vendors": 0,
        "unique_vendors": 0,
        "successfully_classified_l4": 0, # Keep L4 count for reference
        "successfully_classified_l5": 0, # Count successful classifications reaching L5 (if target >= 5)
        "classification_not_possible_initial": 0, # Count initially unclassifiable before search
        "invalid_category_errors": 0, # Track validation errors
        "search_attempts": 0, # Count how many vendors needed search
        "search_successful_classifications_l1": 0, # Count successful L1 classifications *after* search
        "search_successful_classifications_l5": 0, # Count successful L5 classifications *after* search (if target >= 5)
        "api_usage": {
            "openrouter_calls": 0,
            "openrouter_prompt_tokens": 0,
            "openrouter_completion_tokens": 0,
            "openrouter_total_tokens": 0,
            "tavily_search_calls": 0,
            "cost_estimate_usd": 0.0
        }
    }
    # --- END MODIFIED ---
    # --- End Initialize stats ---

    try:
        job.status = JobStatus.PROCESSING.value
        job.current_stage = ProcessingStage.INGESTION.value
        job.progress = 0.05
        logger.info(f"[_process_vendor_file_async] Committing initial status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"status": job.status, "stage": job.current_stage, "progress": job.progress})

        logger.info(f"Reading vendor file")
        with log_duration(logger, "Reading vendor file"):
            vendors_data = read_vendor_file(file_path)
        logger.info(f"Vendor file read successfully",
                    extra={"vendor_count": len(vendors_data)})

        job.current_stage = ProcessingStage.NORMALIZATION.value
        job.progress = 0.1
        logger.info(f"[_process_vendor_file_async] Committing status update: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        logger.info(f"Normalizing vendor data")
        with log_duration(logger, "Normalizing vendor data"):
            normalized_vendors_data = normalize_vendor_data(vendors_data)
        logger.info(f"Vendor data normalized",
                    extra={"normalized_count": len(normalized_vendors_data)})

        logger.info(f"Identifying unique vendors")
        # --- MODIFIED: Type hints added ---
        unique_vendors_map: Dict[str, Dict[str, Any]] = {}
        # --- END MODIFIED ---
        for entry in normalized_vendors_data:
            name = entry.get('vendor_name')
            if name and name not in unique_vendors_map:
                unique_vendors_map[name] = entry
        logger.info(f"Unique vendors identified",
                    extra={"unique_count": len(unique_vendors_map)})

        stats["total_vendors"] = len(normalized_vendors_data)
        stats["unique_vendors"] = len(unique_vendors_map)

        logger.info(f"Loading taxonomy")
        with log_duration(logger, "Loading taxonomy"):
            taxonomy = load_taxonomy() # Can raise exceptions
        logger.info(f"Taxonomy loaded",
                    extra={"taxonomy_version": taxonomy.version})

        # --- MODIFIED: Type hints added ---
        results: Dict[str, Dict] = {vendor_name: {} for vendor_name in unique_vendors_map.keys()}
        # --- END MODIFIED ---

        logger.info(f"Starting vendor classification process by calling classification_logic.process_vendors up to Level {target_level}")
        # --- Call the refactored logic, passing target_level ---
        await process_vendors(
            unique_vendors_map=unique_vendors_map,
            taxonomy=taxonomy,
            results=results,
            stats=stats,
            job=job,
            db=db,
            llm_service=llm_service,
            search_service=search_service,
            target_level=target_level # Pass the target level
        )
        # --- End call to refactored logic ---
        logger.info(f"Vendor classification process completed (returned from classification_logic.process_vendors)")

        logger.info("Starting result generation phase.")

        job.current_stage = ProcessingStage.RESULT_GENERATION.value
        job.progress = 0.98 # Progress after all classification/search
        logger.info(f"[_process_vendor_file_async] Committing status update before result generation: {job.status}, {job.current_stage}, {job.progress}")
        db.commit()
        logger.info(f"Job status updated",
                    extra={"stage": job.current_stage, "progress": job.progress})

        output_file_name = None # Initialize
        try:
                logger.info(f"Generating output file")
                with log_duration(logger, "Generating output file"):
                    output_file_name = generate_output_file(normalized_vendors_data, results, job_id) # Can raise IOError
                logger.info(f"Output file generated", extra={"output_file": output_file_name})
        except Exception as gen_err:
                logger.error("Failed during output file generation", exc_info=True)
                job.fail(f"Failed to generate output file: {str(gen_err)}")
                db.commit()
                return # Stop processing

        # --- Finalize stats ---
        end_time = datetime.now()
        processing_duration = (end_time - datetime.fromisoformat(stats["start_time"])).total_seconds()
        stats["end_time"] = end_time.isoformat()
        stats["processing_duration_seconds"] = round(processing_duration, 2)
        # Cost calculation remains the same
        cost_input_per_1k = 0.0005
        cost_output_per_1k = 0.0015
        estimated_cost = (stats["api_usage"]["openrouter_prompt_tokens"] / 1000) * cost_input_per_1k + \
                            (stats["api_usage"]["openrouter_completion_tokens"] / 1000) * cost_output_per_1k
        estimated_cost += (stats["api_usage"]["tavily_search_calls"] / 1000) * 4.0
        stats["api_usage"]["cost_estimate_usd"] = round(estimated_cost, 4)
        # --- End Finalize stats ---

        # --- Final Commit Block ---
        try:
            logger.info("Attempting final job completion update in database.")
            job.complete(output_file_name, stats)
            job.progress = 1.0 # Ensure progress is 1.0 on completion
            logger.info(f"[_process_vendor_file_async] Committing final job completion status.")
            db.commit()
            logger.info(f"Job completed successfully",
                        extra={
                            "processing_duration": processing_duration,
                            "output_file": output_file_name,
                            "target_level": target_level,
                            "openrouter_calls": stats["api_usage"]["openrouter_calls"],
                            "tokens_used": stats["api_usage"]["openrouter_total_tokens"],
                            "tavily_calls": stats["api_usage"]["tavily_search_calls"],
                            "estimated_cost": stats["api_usage"]["cost_estimate_usd"],
                            "invalid_category_errors": stats.get("invalid_category_errors", 0),
                            "successfully_classified_l5_total": stats.get("successfully_classified_l5", 0)
                        })
        except Exception as final_commit_err:
            logger.error("CRITICAL: Failed to commit final job completion status!", exc_info=True)
            db.rollback()
            try:
                err_msg = f"Failed during final commit: {type(final_commit_err).__name__}: {str(final_commit_err)}"
                job.fail(err_msg[:2000])
                db.commit()
            except Exception as fail_err:
                logger.error("CRITICAL: Also failed to mark job as failed after final commit error.", exc_info=fail_err)
                db.rollback()
        # --- End Final Commit Block ---

    except (ValueError, FileNotFoundError, IOError) as file_err:
        logger.error(f"[_process_vendor_file_async] File reading or writing error", exc_info=True,
                    extra={"error": str(file_err)})
        if job:
            err_msg = f"File processing error: {type(file_err).__name__}: {str(file_err)}"
            job.fail(err_msg[:2000])
            db.commit()
        else:
            logger.error("Job object was None during file error handling.")
    except SQLAlchemyError as db_err:
        logger.error(f"[_process_vendor_file_async] Database error during processing", exc_info=True,
                    extra={"error": str(db_err)})
        if job:
            if job.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                    err_msg = f"Database error: {type(db_err).__name__}: {str(db_err)}"
                    job.fail(err_msg[:2000])
                    db.commit()
            else:
                    logger.warning(f"Database error occurred but job status was already {job.status}. Error: {db_err}")
                    db.rollback()
        else:
            logger.error("Job object was None during database error handling.")
    except Exception as async_err:
        logger.error(f"[_process_vendor_file_async] Unexpected error during async processing", exc_info=True,
                    extra={"error": str(async_err)})
        if job:
            if job.status not in [JobStatus.FAILED.value, JobStatus.COMPLETED.value]:
                err_msg = f"Unexpected error: {type(async_err).__name__}: {str(async_err)}"
                job.fail(err_msg[:2000])
                db.commit()
            else:
                logger.warning(f"Unexpected error occurred but job status was already {job.status}. Error: {async_err}")
                db.rollback()
        else:
            logger.error("Job object was None during unexpected error handling.")
    finally:
        logger.info(f"[_process_vendor_file_async] Finished async processing for job {job_id}")
</file>

<file path='app/utils/log_utils.py'>

# app/utils/log_utils.py
import time
import functools
import uuid
import json
import traceback
import logging # Import standard logging
import inspect # Import inspect
import asyncio # Import asyncio
from typing import Dict, Any, Optional, Callable
from contextlib import contextmanager

# Import context functions from the new module
from core.log_context import (
    local_storage, # Need local_storage for LogTimer stats
    get_correlation_id,
    set_correlation_id, # <<< ADDED IMPORT
    set_log_context,
    get_performance_stats,
    clear_performance_stats
)

# Configure logger for this utility module itself
logger = logging.getLogger("vendor_classification.log_utils")
# --- ADDED: Log confirmation ---
logger.debug("Successfully imported set_correlation_id from core.log_context.")
# --- END ADDED ---

# Timer utility for performance logging
class LogTimer:
    def __init__(self, logger_instance, description="Operation", level=logging.DEBUG,
                 include_in_stats=False, stats_name=None):
        self.logger = logger_instance
        self.description = description
        self.level = level
        self.start_time = None
        self.include_in_stats = include_in_stats
        self.stats_name = stats_name or description.replace(" ", "_").lower()

    def __enter__(self):
        self.start_time = time.monotonic()
        self.logger.log(self.level, f"Starting: {self.description}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time is None: # Avoid error if __enter__ failed
            return
        elapsed = time.monotonic() - self.start_time

        if self.include_in_stats:
            # Use the imported local_storage
            if not hasattr(local_storage, 'performance_stats'):
                local_storage.performance_stats = {}
            if self.stats_name not in local_storage.performance_stats:
                local_storage.performance_stats[self.stats_name] = { 'count': 0, 'total_time': 0.0, 'min_time': float('inf'), 'max_time': 0.0 }
            stats = local_storage.performance_stats[self.stats_name]
            stats['count'] += 1
            stats['total_time'] += elapsed
            stats['min_time'] = min(stats['min_time'], elapsed)
            stats['max_time'] = max(stats['max_time'], elapsed)

        log_extra = {"duration_seconds": round(elapsed, 3)}
        if exc_type:
            log_extra["error"] = str(exc_val) if exc_val else exc_type.__name__
            self.logger.error(
                f"Failed: {self.description} after {elapsed:.3f}s",
                exc_info=(exc_type, exc_val, exc_tb) if exc_tb else False,
                extra=log_extra
            )
        else:
            self.logger.log(
                self.level,
                f"Completed: {self.description} in {elapsed:.3f}s",
                extra=log_extra
            )

@contextmanager
def log_duration(logger_instance, description="Operation", level=logging.DEBUG, include_in_stats=False):
    """Context manager for logging the duration of a code block using LogTimer."""
    timer = LogTimer(logger_instance, description, level, include_in_stats)
    with timer:
        yield

def log_function_call(logger_instance, level=logging.DEBUG, include_args=True,
                      include_result=False, arg_char_limit=100, result_char_limit=100,
                      include_in_stats=False):
    """
    Decorator to log function calls with parameters and return value.
    Handles both sync and async functions.
    """
    def decorator(func):
        logger_instance.debug(f"Applying @log_function_call decorator to function: {func.__module__}.{func.__name__}")

        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            func_name = func.__name__
            module_name = func.__module__
            full_func_name = f"{module_name}.{func_name}"
            timer_name = full_func_name

            args_repr = ""
            if include_args:
                try:
                    sig = inspect.signature(func)
                    bound_args = sig.bind_partial(*args, **kwargs)
                    bound_args.apply_defaults()
                    arg_items = []
                    for name, value in bound_args.arguments.items():
                        try: value_repr = repr(value)
                        except Exception: value_repr = "[repr error]"
                        if len(value_repr) > arg_char_limit:
                             type_name = type(value).__name__
                             value_repr = f"<{type_name} object>" if type_name != 'object' else value_repr[:arg_char_limit-3] + "..."
                        arg_items.append(f"{name}={value_repr}")
                    args_repr = ", ".join(arg_items)
                except Exception as sig_err:
                    logger_instance.warning(f"Could not format arguments for {full_func_name}: {sig_err}")
                    args_repr = "[Arg formatting error]"

            log_extra_start = {"function_args": args_repr} if include_args else {}
            logger_instance.log(level, f"Calling: {full_func_name}", extra=log_extra_start)

            start_time = time.monotonic()
            try:
                result = func(*args, **kwargs)
                elapsed = time.monotonic() - start_time

                if include_in_stats:
                    if not hasattr(local_storage, 'performance_stats'): local_storage.performance_stats = {}
                    if timer_name not in local_storage.performance_stats: local_storage.performance_stats[timer_name] = {'count': 0, 'total_time': 0.0, 'min_time': float('inf'), 'max_time': 0.0}
                    stats = local_storage.performance_stats[timer_name]
                    stats['count'] += 1; stats['total_time'] += elapsed; stats['min_time'] = min(stats['min_time'], elapsed); stats['max_time'] = max(stats['max_time'], elapsed)

                log_extra_end = {"duration_seconds": round(elapsed, 3)}
                result_repr = ""
                if include_result:
                    try:
                        result_repr = repr(result)
                        if len(result_repr) > result_char_limit:
                             type_name = type(result).__name__
                             result_repr = f"<{type_name} object>" if type_name != 'object' else result_repr[:result_char_limit-3] + "..."
                    except Exception: result_repr = "[repr error]"
                    log_extra_end["result"] = result_repr

                logger_instance.log(
                    level,
                    f"Completed: {full_func_name} in {elapsed:.3f}s",
                    extra=log_extra_end
                )
                return result

            except Exception as e:
                elapsed = time.monotonic() - start_time
                logger_instance.error(
                    f"Failed: {full_func_name} after {elapsed:.3f}s",
                    exc_info=True,
                    extra={"duration_seconds": round(elapsed, 3), "error": str(e)}
                )
                raise

        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            func_name = func.__name__
            module_name = func.__module__
            full_func_name = f"{module_name}.{func_name}"
            timer_name = full_func_name

            args_repr = ""
            if include_args:
                try:
                    sig = inspect.signature(func)
                    bound_args = sig.bind_partial(*args, **kwargs)
                    bound_args.apply_defaults()
                    arg_items = []
                    for name, value in bound_args.arguments.items():
                        try: value_repr = repr(value)
                        except Exception: value_repr = "[repr error]"
                        if len(value_repr) > arg_char_limit:
                            type_name = type(value).__name__
                            value_repr = f"<{type_name} object>" if type_name != 'object' else value_repr[:arg_char_limit-3] + "..."
                        arg_items.append(f"{name}={value_repr}")
                    args_repr = ", ".join(arg_items)
                except Exception as sig_err:
                    logger_instance.warning(f"Could not format arguments for async {full_func_name}: {sig_err}")
                    args_repr = "[Arg formatting error]"

            log_extra_start = {"function_args": args_repr} if include_args else {}
            logger_instance.log(level, f"Calling async: {full_func_name}", extra=log_extra_start)

            start_time = time.monotonic()
            try:
                result = await func(*args, **kwargs) # Await the async function
                elapsed = time.monotonic() - start_time

                if include_in_stats:
                    if not hasattr(local_storage, 'performance_stats'): local_storage.performance_stats = {}
                    if timer_name not in local_storage.performance_stats: local_storage.performance_stats[timer_name] = {'count': 0, 'total_time': 0.0, 'min_time': float('inf'), 'max_time': 0.0}
                    stats = local_storage.performance_stats[timer_name]
                    stats['count'] += 1; stats['total_time'] += elapsed; stats['min_time'] = min(stats['min_time'], elapsed); stats['max_time'] = max(stats['max_time'], elapsed)

                log_extra_end = {"duration_seconds": round(elapsed, 3)}
                result_repr = ""
                if include_result:
                    try:
                        result_repr = repr(result)
                        if len(result_repr) > result_char_limit:
                            type_name = type(result).__name__
                            result_repr = f"<{type_name} object>" if type_name != 'object' else result_repr[:result_char_limit-3] + "..."
                    except Exception: result_repr = "[repr error]"
                    log_extra_end["result"] = result_repr

                logger_instance.log(
                    level,
                    f"Completed async: {full_func_name} in {elapsed:.3f}s",
                    extra=log_extra_end
                )
                return result

            except Exception as e:
                elapsed = time.monotonic() - start_time
                logger_instance.error(
                    f"Failed async: {full_func_name} after {elapsed:.3f}s",
                    exc_info=True,
                    extra={"duration_seconds": round(elapsed, 3), "error": str(e)}
                )
                raise

        if asyncio.iscoroutinefunction(func):
            logger_instance.debug(f"Function {func.__module__}.{func.__name__} is async, using async_wrapper.")
            return async_wrapper
        else:
            logger_instance.debug(f"Function {func.__module__}.{func.__name__} is sync, using sync_wrapper.")
            return sync_wrapper
    return decorator

# --- Other utility functions from the original log_utils.py ---

def log_api_request(service_name: str):
    """
    Decorator to log API requests with detailed metrics.
    (Implementation remains the same as provided in the original log_utils.py)
    """
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            request_id = str(uuid.uuid4())
            correlation_id = get_correlation_id() or request_id
            url = kwargs.get("url") or (args[0] if len(args) > 0 and isinstance(args[0], str) else None)
            method = func.__name__
            request_data = kwargs.get("json", {})
            if isinstance(request_data, dict):
                if "api_key" in request_data: request_data["api_key"] = "[REDACTED]"
                if "password" in request_data: request_data["password"] = "[REDACTED]"

            start_time = time.time()
            logger.info(
                f"API request to {service_name} started",
                extra={ "service": service_name, "url": url, "method": method, "request_id": request_id, "correlation_id": correlation_id, "request_data": json.dumps(request_data)[:500] if request_data else None }
            )
            try:
                response = await func(*args, **kwargs)
                duration = time.time() - start_time
                # Update metrics (simplified example)
                set_log_context({f"{service_name}_api_call_count": 1, f"{service_name}_api_call_time": duration})

                status_code = getattr(response, 'status_code', None)
                response_data_str = "[Response data omitted]" # Simplified logging
                logger.info(
                    f"API request to {service_name} completed",
                    extra={ "service": service_name, "url": url, "method": method, "request_id": request_id, "correlation_id": correlation_id, "duration": duration, "status_code": status_code, "response_snippet": response_data_str }
                )
                return response
            except Exception as e:
                duration = time.time() - start_time
                set_log_context({f"{service_name}_api_error_count": 1})
                logger.error(
                    f"API request to {service_name} failed", exc_info=True,
                    extra={ "service": service_name, "url": url, "method": method, "request_id": request_id, "correlation_id": correlation_id, "duration": duration, "error": str(e) }
                )
                raise
        return wrapper
    return decorator

def log_method(level=logging.DEBUG, include_args=True, include_result=False):
    """
    Enhanced method logging decorator for class methods.
    (Implementation remains the same as provided in the original log_utils.py)
    """
    def decorator(method):
        @functools.wraps(method)
        def wrapper(self, *args, **kwargs):
            class_name = self.__class__.__name__
            method_name = method.__name__
            method_logger = getattr(self, 'logger', logger)
            log_level = level or getattr(method_logger, 'level', logging.DEBUG)
            operation = f"{class_name}.{method_name}"
            args_str = ""
            if include_args:
                arg_items = []
                for i, arg in enumerate(args):
                    arg_str = repr(arg); arg_str = arg_str[:97] + "..." if len(arg_str) > 100 else arg_str
                    arg_items.append(f"arg{i+1}={arg_str}")
                for name, value in kwargs.items():
                    value_str = repr(value); value_str = value_str[:97] + "..." if len(value_str) > 100 else value_str
                    arg_items.append(f"{name}={value_str}")
                args_str = ", ".join(arg_items)

            log_msg = f"Calling {operation}" + (f"({args_str})" if args_str else "")
            method_logger.log(log_level, log_msg)

            with LogTimer(method_logger, operation, level=log_level, include_in_stats=True):
                try:
                    result = method(self, *args, **kwargs)
                    result_str = ""
                    if include_result:
                        result_str = repr(result); result_str = result_str[:97] + "..." if len(result_str) > 100 else result_str
                        method_logger.log(log_level, f"{operation} returned: {result_str}")
                    else:
                        method_logger.log(log_level, f"{operation} completed")
                    return result
                except Exception as e:
                    method_logger.error(f"{operation} failed: {str(e)}", exc_info=True, extra={"error_type": type(e).__name__})
                    raise
        return wrapper
    return decorator

def setup_request_context(request=None):
    """
    Set up a new request context with correlation ID and other metadata.
    (Implementation remains the same as provided in the original log_utils.py)
    """
    correlation_id = str(uuid.uuid4())
    set_correlation_id(correlation_id) # Use the imported function
    if request:
        client_ip = request.client.host if hasattr(request, 'client') and request.client else None
        user_agent = request.headers.get('user-agent') if hasattr(request, 'headers') else None
        set_log_context({ 'client_ip': client_ip, 'user_agent': user_agent, 'request_path': getattr(request, 'url', None) })
    return correlation_id

def log_critical_operation(operation_name: str, include_in_stats=True):
    """
    Context manager for logging critical operations with mandatory timing.
    (Implementation remains the same as provided in the original log_utils.py)
    """
    return LogTimer(logger, operation_name, level=logging.INFO, include_in_stats=include_in_stats)
</file>

<file path='app/utils/taxonomy_loader.py'>
# <file path='app/utils/taxonomy_loader.py'>
# --- file path='app/utils/taxonomy_loader.py' ---
import os
import json
import pandas as pd
import re # <<< Added import
from typing import Dict, Any

# --- ADDED: Import logging for the main script part ---
import logging
# --- END ADDED ---

from core.config import settings
# --- MODIFIED IMPORT: Import all level classes including L5 ---
from models.taxonomy import Taxonomy, TaxonomyLevel1, TaxonomyLevel2, TaxonomyLevel3, TaxonomyLevel4, TaxonomyLevel5
# --- END MODIFIED IMPORT ---
from core.logging_config import get_logger

# Configure logger
logger = get_logger("vendor_classification.taxonomy_loader")

# --- Global cache for taxonomy ---
_taxonomy_cache: Taxonomy | None = None

def load_taxonomy(force_reload: bool = False) -> Taxonomy:
    """
    Load taxonomy data, using cache if available unless forced.
    Tries JSON first, then Excel as fallback.

    Args:
        force_reload: If True, bypass cache and reload from file.

    Returns:
        Taxonomy object

    Raises:
        FileNotFoundError: If neither JSON nor Excel file can be found.
        ValueError: If both JSON and Excel loading fail or result in empty taxonomy.
    """
    global _taxonomy_cache
    if _taxonomy_cache is not None and not force_reload:
        logger.info("Returning cached taxonomy.")
        return _taxonomy_cache

    excel_path = os.path.join(settings.TAXONOMY_DATA_DIR, "2022_NAICS_Codes.xlsx")
    json_path = os.path.join(settings.TAXONOMY_DATA_DIR, "naics_taxonomy.json")

    taxonomy = None

    # --- Try JSON first ---
    if os.path.exists(json_path):
        try:
            logger.info(f"Attempting to load taxonomy from JSON: {json_path}")
            with open(json_path, "r") as f:
                taxonomy_data = json.load(f)

            # Validate structure before creating Taxonomy object
            if not taxonomy_data.get("categories"):
                raise ValueError("JSON data is missing the 'categories' key.")

            taxonomy = Taxonomy(**taxonomy_data)
            logger.info(f"Taxonomy loaded successfully from JSON with {len(taxonomy.categories)} top-level categories.")
            _taxonomy_cache = taxonomy # Update cache
            return taxonomy
        except json.JSONDecodeError as json_err:
            logger.error(f"Failed to decode JSON from {json_path}: {json_err}", exc_info=False)
            logger.warning(f"JSON parsing failed. Will attempt fallback to Excel if available.")
        except Exception as e:
            logger.error(f"Error loading taxonomy from JSON: {e}", exc_info=True)
            logger.warning(f"Unexpected error loading JSON. Will attempt fallback to Excel if available.")
            taxonomy = None # Ensure taxonomy is None if JSON loading fails

    # --- Fallback to Excel if JSON failed or didn't exist ---
    if taxonomy is None and os.path.exists(excel_path):
        try:
            logger.warning(f"JSON load failed or file missing, attempting to load taxonomy from Excel: {excel_path}")
            taxonomy = load_taxonomy_from_excel(excel_path)

            # Save as JSON for future use IF successful
            logger.info(f"Saving newly loaded taxonomy to JSON: {json_path}")
            os.makedirs(settings.TAXONOMY_DATA_DIR, exist_ok=True)
            with open(json_path, "w") as f:
                # Use model_dump for Pydantic v2
                json.dump(taxonomy.model_dump(exclude_none=True, mode='json'), f, indent=2) # Added mode='json'

            logger.info(f"Taxonomy loaded successfully from Excel with {len(taxonomy.categories)} top-level categories and saved to JSON.")
            _taxonomy_cache = taxonomy # Update cache
            return taxonomy
        except Exception as e:
            logger.error(f"Error loading taxonomy from Excel after JSON failure: {e}", exc_info=True)
            taxonomy = None # Ensure taxonomy is None if Excel loading also fails

    # --- If both failed, raise error ---
    if taxonomy is None:
        error_msg = f"Failed to load taxonomy. Neither JSON ({json_path}) nor Excel ({excel_path}) file could be loaded or found."
        logger.critical(error_msg)
        raise FileNotFoundError(error_msg)

    # This part should theoretically not be reached if errors are raised correctly
    logger.critical("Reached unexpected end of load_taxonomy function.")
    raise RuntimeError("Taxonomy loading finished in an unexpected state.")


def load_taxonomy_from_excel(file_path: str) -> Taxonomy:
    """
    Load taxonomy from Excel file, building the hierarchical structure up to 5 levels.
    Correctly handles sector range titles.

    Args:
        file_path: Path to Excel file

    Returns:
        Taxonomy object

    Raises:
        FileNotFoundError: If the file_path does not exist.
        ValueError: If the file cannot be parsed or required columns are missing.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Taxonomy Excel file not found at {file_path}")

    try:
        # Read Excel file
        logger.debug(f"Reading Excel file: {file_path}")

        try:
            # Try reading as Excel first
            df = pd.read_excel(file_path, dtype=str) # Read all as string initially
            logger.debug(f"Successfully read Excel file with {len(df)} rows")
        except Exception as excel_error:
            # Fallback to CSV with pipe delimiter if Excel read fails
            logger.warning(f"Failed to read as Excel ({str(excel_error)}), trying as pipe-delimited text...")
            try:
                # Assuming the Excel file provided might actually be pipe-delimited
                df = pd.read_csv(file_path, delimiter='|', quotechar='"', dtype=str, skipinitialspace=True)
                logger.debug(f"Successfully read pipe-delimited file with {len(df)} rows")
            except Exception as csv_error:
                logger.error(f"Failed to read as pipe-delimited text: {str(csv_error)}")
                raise ValueError(f"Could not parse taxonomy file '{file_path}'. Tried Excel and Pipe-Delimited CSV.") from excel_error

        # --- Column Identification ---
        code_col_options = ['naics code', '2022 naics us code', 'code']
        title_col_options = ['naics title', '2022 naics us title', 'title']
        desc_col_options = ['description', 'desc']

        df_cols_lower = {col.lower().strip(): col for col in df.columns}

        code_column = next((df_cols_lower[opt] for opt in code_col_options if opt in df_cols_lower), None)
        title_column = next((df_cols_lower[opt] for opt in title_col_options if opt in df_cols_lower), None)
        desc_column = next((df_cols_lower[opt] for opt in desc_col_options if opt in df_cols_lower), None)

        if not code_column or not title_column:
            error_msg = f"Could not identify required 'code' and 'title' columns in the taxonomy file. Found columns: {list(df.columns)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        logger.info(f"Identified taxonomy columns - Code: '{code_column}', Title: '{title_column}', Desc: '{desc_column or 'None'}'")

        # --- Data Cleaning ---
        df = df.dropna(subset=[code_column])
        df[code_column] = df[code_column].astype(str).str.strip()
        df[title_column] = df[title_column].astype(str).str.strip().str.replace(r'\s*T$', '', regex=True) # Remove trailing 'T'

        if desc_column:
            df[desc_column] = df[desc_column].fillna('').astype(str).str.strip()
        else:
            desc_column = 'Description' # Assign a name
            df[desc_column] = ''
            logger.warning("No description column found, descriptions will be empty.")

        # --- Filter out rows with non-standard codes BEFORE processing ranges ---
        valid_code_pattern = r'^(\d{2}-\d{2}|\d+)$'
        original_row_count = len(df)
        df = df[df[code_column].str.match(valid_code_pattern)]
        filtered_row_count = len(df)
        if original_row_count != filtered_row_count:
            logger.warning(f"Filtered out {original_row_count - filtered_row_count} rows with invalid code formats (neither ##-## nor numeric).")

        # --- Build taxonomy structure ---
        categories_level1: Dict[str, TaxonomyLevel1] = {}
        logger.info("Building taxonomy hierarchy...")

        rows_processed = 0
        skipped_rows = 0
        for index, row in df.iterrows():
            code_raw = row[code_column].strip()
            title = row[title_column].strip()
            description = row[desc_column].strip()

            code = code_raw
            original_code = code_raw
            is_range = '-' in code

            # --- Range Handling ---
            if is_range:
                range_match = re.match(r'^(\d{2})-\d{2}$', code)
                if range_match:
                    start_code = range_match.group(1)
                    range_title = title
                    logger.debug(f"Row {index}: Processing range code '{original_code}' ('{range_title}') - Associating title with start code '{start_code}'")
                    if start_code not in categories_level1:
                        categories_level1[start_code] = TaxonomyLevel1(id=start_code, name=range_title, description=description, children={})
                        logger.info(f"Row {index}: Added L1 '{start_code}' using range title '{range_title}'.")
                    else:
                        existing_name = categories_level1[start_code].name
                        if existing_name != range_title:
                            logger.warning(f"Row {index}: L1 '{start_code}' already exists with name '{existing_name}'. Overwriting with range title '{range_title}'.")
                            categories_level1[start_code].name = range_title
                        if not categories_level1[start_code].description and description:
                             categories_level1[start_code].description = description
                             logger.debug(f"Row {index}: Updated empty L1 '{start_code}' description.")
                    rows_processed += 1
                    continue # Skip rest of hierarchy logic for this row
                else:
                    logger.warning(f"Row {index}: Skipping row with unhandled range format code: '{original_code}' ('{title}')")
                    skipped_rows += 1
                    continue
            elif not code.isdigit():
                logger.warning(f"Row {index}: Skipping row with non-numeric/non-range code: '{original_code}' ('{title}')")
                skipped_rows += 1
                continue
            # --- End Range Handling ---

            # --- Hierarchy Building for Numeric Codes ---
            code_length = len(code)
            try:
                if code_length == 2:  # Level 1 (Specific code)
                    if code not in categories_level1:
                        categories_level1[code] = TaxonomyLevel1(id=code, name=title, description=description, children={})
                        logger.debug(f"Row {index}: Added L1: {code} - {title}")
                    elif not categories_level1[code].description and description:
                        categories_level1[code].description = description
                        logger.debug(f"Row {index}: Updated L1 '{code}' description (already existed).")

                elif code_length == 3: # Level 2
                    l1_code = code[:2]
                    if l1_code in categories_level1:
                        l1_cat = categories_level1[l1_code]
                        if code not in l1_cat.children:
                            l1_cat.children[code] = TaxonomyLevel2(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:   Added L2: {code} under {l1_code}")
                    else:
                        logger.warning(f"Row {index}: L1 parent '{l1_code}' not found for L2 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 4: # Level 3
                    l1_code = code[:2]; l2_code = code[:3]
                    if l1_code in categories_level1 and l2_code in categories_level1[l1_code].children:
                        l2_cat = categories_level1[l1_code].children[l2_code]
                        if code not in l2_cat.children:
                            l2_cat.children[code] = TaxonomyLevel3(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:     Added L3: {code} under {l2_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2 parent '{l1_code}/{l2_code}' not found for L3 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 5: # Level 4
                    l1_code = code[:2]; l2_code = code[:3]; l3_code = code[:4]
                    if l1_code in categories_level1 and \
                       l2_code in categories_level1[l1_code].children and \
                       l3_code in categories_level1[l1_code].children[l2_code].children:
                        l3_cat = categories_level1[l1_code].children[l2_code].children[l3_code]
                        if code not in l3_cat.children:
                            # Create L4 with empty children dict for potential L5
                            l3_cat.children[code] = TaxonomyLevel4(id=code, name=title, description=description, children={})
                            logger.debug(f"Row {index}:       Added L4: {code} under {l3_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2/L3 parent '{l1_code}/{l2_code}/{l3_code}' not found for L4 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue

                elif code_length == 6: # Level 5
                    l1_code = code[:2]; l2_code = code[:3]; l3_code = code[:4]; l4_code = code[:5]
                    if l1_code in categories_level1 and \
                       l2_code in categories_level1[l1_code].children and \
                       l3_code in categories_level1[l1_code].children[l2_code].children and \
                       l4_code in categories_level1[l1_code].children[l2_code].children[l3_code].children:
                        l4_cat = categories_level1[l1_code].children[l2_code].children[l3_code].children[l4_code]
                        if code not in l4_cat.children:
                            l4_cat.children[code] = TaxonomyLevel5(id=code, name=title, description=description)
                            logger.debug(f"Row {index}:         Added L5: {code} under {l4_code}")
                    else:
                        logger.warning(f"Row {index}: L1/L2/L3/L4 parent '{l1_code}/{l2_code}/{l3_code}/{l4_code}' not found for L5 code '{code}' ('{title}'). Skipping.")
                        skipped_rows += 1; continue
                else:
                     logger.warning(f"Row {index}: Code '{code}' has unhandled length {code_length}. Skipping.")
                     skipped_rows += 1; continue

                rows_processed += 1

            except Exception as hierarchy_error:
                 logger.error(f"Row {index}: Error building hierarchy for code '{code}' ('{title}')", exc_info=True)
                 skipped_rows += 1

        if not categories_level1:
            error_msg = f"No valid Level 1 categories found after processing {rows_processed + skipped_rows} rows from the file."
            logger.error(error_msg)
            raise ValueError(error_msg)

        # Create final taxonomy object
        taxonomy = Taxonomy(
            name="NAICS Taxonomy",
            version="2022", # Or extract from filename/content if possible
            description="North American Industry Classification System", # Or extract
            categories=categories_level1
        )

        # Log final stats including L5
        l1_count = len(taxonomy.categories)
        l2_count = sum(len(getattr(l1, 'children', {})) for l1 in taxonomy.categories.values())
        l3_count = sum(len(getattr(l2, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values())
        l4_count = sum(len(getattr(l3, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values() for l3 in getattr(l2, 'children', {}).values())
        l5_count = sum(len(getattr(l4, 'children', {})) for l1 in taxonomy.categories.values() for l2 in getattr(l1, 'children', {}).values() for l3 in getattr(l2, 'children', {}).values() for l4 in getattr(l3, 'children', {}).values())
        logger.info(f"Taxonomy hierarchy built with {l1_count} L1, {l2_count} L2, {l3_count} L3, {l4_count} L4, {l5_count} L5 categories from {rows_processed} processed rows ({skipped_rows} skipped).")

        return taxonomy

    except FileNotFoundError: # Raised explicitly above
        raise
    except ValueError as ve: # Raised explicitly above for parsing/column errors
         logger.error(f"Value error processing taxonomy Excel file '{file_path}': {ve}", exc_info=False)
         raise # Re-raise specific error
    except Exception as e:
        logger.error(f"Unexpected error processing taxonomy Excel file '{file_path}': {e}", exc_info=True)
        raise ValueError(f"Could not process taxonomy Excel file: {e}") from e


# --- create_sample_taxonomy function remains unchanged ---
# (It's a fallback and doesn't need L5 for this update)
def create_sample_taxonomy() -> Taxonomy:
    """
    Create a sample NAICS taxonomy for testing or fallback.
    (This function remains unchanged from the original provided code)
    """
    # Level 4 categories
    level4_categories_11 = {
        "111110": TaxonomyLevel4(id="111110", name="Soybean Farming", description="...", children={}), # Add empty children
        "111120": TaxonomyLevel4(id="111120", name="Oilseed (except Soybean) Farming", description="...", children={}),
    }
    # ... other sample L4 categories ...
    level4_categories_23 = { "236115": TaxonomyLevel4(id="236115", name="New Single-Family Housing Construction", description="...", children={}) }
    level4_categories_51 = { "513210": TaxonomyLevel4(id="513210", name="Software Publishers", description="...", children={}) }

    # Create level 3 categories
    level3_categories_11 = {
        "1111": TaxonomyLevel3(id="1111", name="Oilseed and Grain Farming", description="...", children=level4_categories_11),
    }
    level3_categories_23 = { "2361": TaxonomyLevel3(id="2361", name="Residential Building Construction", description="...", children=level4_categories_23) }
    level3_categories_51 = { "5132": TaxonomyLevel3(id="5132", name="Software Publishers", description="...", children=level3_categories_51) }


    # Create level 2 categories
    level2_categories_11 = {
        "111": TaxonomyLevel2(id="111", name="Crop Production", description="...", children=level3_categories_11),
    }
    level2_categories_23 = { "236": TaxonomyLevel2(id="236", name="Construction of Buildings", description="...", children=level3_categories_23) }
    level2_categories_51 = { "513": TaxonomyLevel2(id="513", name="Publishing Industries", description="...", children=level3_categories_51) }

    # Create level 1 categories
    level1_categories = {
        "11": TaxonomyLevel1(id="11", name="Agriculture, Forestry, Fishing and Hunting", description="...", children=level2_categories_11),
        "23": TaxonomyLevel1(id="23", name="Construction", description="...", children=level2_categories_23),
        "51": TaxonomyLevel1(id="51", name="Information", description="...", children=level2_categories_51)
    }

    # Create taxonomy
    taxonomy = Taxonomy(
        name="NAICS Taxonomy (Sample)",
        version="2022_Sample",
        description="Sample North American Industry Classification System",
        categories=level1_categories
    )
    logger.warning("Generated and using a sample taxonomy.")
    return taxonomy


if __name__ == "__main__":
     # --- ADDED: Import logging for test block ---
     import logging
     # --- END ADDED ---
     # Example usage for testing
     print("Testing taxonomy loader...")
     try:
         # Ensure settings are available or provide a default path
         if 'settings' not in locals() and 'settings' not in globals():
              class MockSettings:
                  TAXONOMY_DATA_DIR = os.path.join(os.path.dirname(__file__), '../../data/taxonomy')
              settings = MockSettings()
              print(f"Using mock settings for TAXONOMY_DATA_DIR: {settings.TAXONOMY_DATA_DIR}")
         else:
              if not hasattr(settings, 'TAXONOMY_DATA_DIR') or not settings.TAXONOMY_DATA_DIR:
                   settings.TAXONOMY_DATA_DIR = os.path.join(os.path.dirname(__file__), '../../data/taxonomy')
              print(f"Using settings.TAXONOMY_DATA_DIR: {settings.TAXONOMY_DATA_DIR}")

         # --- Ensure log directory exists for testing ---
         log_test_dir = "./logs_test"
         os.makedirs(log_test_dir, exist_ok=True)
         from core.logging_config import setup_logging
         setup_logging(log_level=logging.DEBUG, log_to_file=True, log_dir=log_test_dir, async_logging=False)
         # ---

         # Force reload from Excel (or CSV fallback)
         tax = load_taxonomy(force_reload=True)
         print(f"\nLoaded taxonomy: {tax.name} - Version: {tax.version}")
         print(f"Number of L1 categories: {len(tax.categories)}")

         # Verification for a specific L5 code (example)
         print("\n--- Verification for L5 Code (e.g., 311111) ---")
         try:
             l1 = tax.categories.get('31')
             l2 = l1.children.get('311') if l1 else None
             l3 = l2.children.get('3111') if l2 else None
             l4 = l3.children.get('31111') if l3 else None
             l5 = l4.children.get('311111') if l4 else None
             if l5:
                 print(f"L5 '311111' FOUND. Name: '{l5.name}'")
             else:
                 print("L5 '311111' NOT FOUND (or parent missing).")
         except Exception as e:
             print(f"Error during L5 verification: {e}")

     except Exception as e:
         print(f"\nError during testing: {e}")
         import traceback
         traceback.print_exc()
</file>

<file path='app/utils/text_processing.py'>
import re
import unicodedata
from typing import List, Dict, Any

def normalize_vendor_name(name: str) -> str:
    """
    Normalize a vendor name by removing special characters, 
    standardizing whitespace, and converting to lowercase.
    
    Args:
        name: Vendor name to normalize
        
    Returns:
        Normalized vendor name
    """
    if not name or not isinstance(name, str):
        return ""
    
    # Convert to lowercase
    normalized = name.lower()
    
    # Normalize unicode characters
    normalized = unicodedata.normalize('NFKD', normalized)
    normalized = ''.join([c for c in normalized if not unicodedata.combining(c)])
    
    # Remove legal entity indicators
    entity_indicators = [
        r'\bllc\b', r'\binc\b', r'\bltd\b', r'\bcorp\b', r'\bcorporation\b',
        r'\bco\b', r'\bcompany\b', r'\bgroup\b', r'\bholdings\b', r'\bservices\b',
        r'\bsolutions\b', r'\bsystems\b', r'\btechnologies\b', r'\btech\b',
        r'\benterprises\b', r'\blimited\b', r'\bpartners\b', r'\bassociates\b',
        r'\bgmbh\b', r'\bplc\b', r'\bpte\b', r'\bpty\b', r'\bag\b', r'\bsa\b',
        r'\bsrl\b', r'\bs\.r\.l\b', r'\bs\.p\.a\b', r'\bs\.a\b', r'\bs\.a\.s\b',
        r'\bb\.v\b', r'\blp\b', r'\bllp\b', r'\blp\b', r'\bl\.p\b', r'\bl\.l\.c\b',
        r'\bl\.l\.p\b', r'\bincorporated\b'
    ]
    
    for indicator in entity_indicators:
        normalized = re.sub(indicator, '', normalized)
    
    # Remove special characters and replace with space
    normalized = re.sub(r'[^\w\s]', ' ', normalized)
    
    # Replace multiple spaces with a single space
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # Remove leading/trailing whitespace
    normalized = normalized.strip()
    
    return normalized

def normalize_vendor_names(vendors: List[str]) -> List[str]:
    """
    Normalize a list of vendor names.
    
    Args:
        vendors: List of vendor names
        
    Returns:
        List of normalized vendor names
    """
    return [normalize_vendor_name(vendor) for vendor in vendors]

def extract_vendor_names_from_dataframe(df: Any, column_name: str = 'vendor_name') -> List[str]:
    """
    Extract vendor names from a pandas DataFrame.
    
    Args:
        df: Pandas DataFrame
        column_name: Name of the column containing vendor names
        
    Returns:
        List of vendor names
    """
    if column_name not in df.columns:
        # Try to find a column that might contain vendor names
        potential_columns = [col for col in df.columns if 'vendor' in col.lower() or 'company' in col.lower()]
        
        if potential_columns:
            column_name = potential_columns[0]
        else:
            # Use the first column as a fallback
            column_name = df.columns[0]
    
    # Extract vendor names
    vendors = df[column_name].astype(str).tolist()
    
    # Filter out empty or NaN values
    vendors = [vendor for vendor in vendors if vendor and vendor.lower() != 'nan']
    
    return vendors

</file>

<file path='docker-compose.yml'>
services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.web
    ports:
      - "${WEB_PORT:-8001}:8000"
    volumes:
      # - ./app:/app          # <-- REMOVE OR COMMENT OUT THIS LINE FOR PRODUCTION/STAGING
                              # Keep it if you need hot-reloading of backend code during dev
      - ./data:/data        # Keep this for persistent data
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/vendor_classification
      - REDIS_URL=redis://redis:6379/0
      # API keys are hardcoded in config.py - consider moving to env vars
      - PYTHONUNBUFFERED=1 # Ensure logs appear immediately
      - LOG_LEVEL=DEBUG    # Set log level explicitly
    depends_on:
      db:
        condition: service_healthy # Wait for DB to be healthy before initializing
      redis:
        condition: service_started # Wait for Redis to start
    networks:
      - app-network
    # --- MODIFIED COMMAND ---
    # Run initialization script THEN start Uvicorn
    command: sh -c "echo '>>> [Web CMD Start] Waiting briefly for DB...' && sleep 5 && echo '>>> [Web CMD Start] Running DB Initialization...' && python core/initialize_db.py && echo '>>> [Web CMD Start] Starting Uvicorn...' && uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload --log-level debug"
    # --- END MODIFIED COMMAND ---
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s # Increase start period slightly to allow for init

  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    volumes:
      # - ./app:/app # Consider removing this too if worker code doesn't need hot-reload
      - ./data:/data
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/vendor_classification
      - REDIS_URL=redis://redis:6379/0
      # API keys are hardcoded in config.py
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=DEBUG
    depends_on:
      db:
        condition: service_healthy # Wait for DB
      redis:
        condition: service_started # Wait for Redis
    networks:
      - app-network
    # Consider adding a simple healthcheck if possible (e.g., celery status)

  db:
    image: postgres:14
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=vendor_classification
    ports:
      - "5433:5432"
    networks:
      - app-network
    # Add healthcheck for the database
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d vendor_classification"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7
    ports:
      - "6379:6379"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  app-network:
    driver: bridge

volumes:
  postgres_data:
</file>

<file path='docs/1_response_solution design.md'>
# docs/1_response_solution design.md
# NAICS Vendor Classification System Design Document

## 1. Executive Summary

#### Problem Statement
Organizations struggle to efficiently classify their vendors according to industry standards like NAICS (North American Industry Classification System). Manual classification is time-consuming, inconsistent, and prone to errors, making it difficult to maintain accurate vendor databases for analytics, reporting, and compliance purposes.

#### Proposed Solution Overview
Naicsvendorclassification.com is a specialized web application that automates the vendor classification process using advanced AI. The system ingests vendor lists from spreadsheets, normalizes the data, and leverages large language models to accurately categorize vendors according to a four-level hierarchical taxonomy. For vendors that cannot be immediately classified, the system conducts automated web searches to gather additional information before making a determination.

#### Key Benefits and Success Metrics
**Benefits:**
- Reduces vendor classification time from days/weeks to hours
- Provides consistent, standardized categorization
- Enhances data quality for procurement analytics and reporting
- Minimizes manual effort through intelligent automation
- Securely processes vendor information

**Success Metrics:**
- Classification accuracy (>95% target)
- Processing throughput (1,000+ vendors per hour)
- Reduction in manual classification effort (>80%)
- System reliability and uptime (99.9%)
- User satisfaction ratings (>4.5/5)

## 2. System Architecture

#### High-level Architecture Diagram

```
          
                                                                    
  Web Interface   Application Core    Processing Engine   
                                                                    
          
                                                               
                                                               
                                                               
          
                                                                    
  File Storage    Security Layer           External APIs       
                                                - OpenRouter        
            - Tavily Search     
                                                  
```

#### Key Components and Their Interactions

1. **Web Interface**
   - Simple, intuitive UI for file upload and download
   - Progress monitoring dashboard
   - Authentication system
   - Email notification integration

2. **Application Core**
   - Job management and orchestration
   - File handling and validation
   - User session management
   - Email notification service

3. **Processing Engine**
   - Data ingestion and normalization
   - Vendor batching logic
   - Classification workflow orchestration
   - LLM prompt engineering and response handling
   - Result generation and validation

4. **File Storage**
   - Secure storage for input files
   - Temporary processing data
   - Output files and logs
   - Usage statistics

5. **Security Layer**
   - Authentication and authorization
   - Data sanitization
   - PII/sensitive data filtering
   - Encryption management

6. **External APIs**
   - OpenRouter for vendor classification
   - Tavily Search API for unknown vendor research

#### Technology Stack Recommendations

**Backend:**
- Python 3.11+ for core processing
- FastAPI for RESTful API endpoints
- Pydantic for data validation and modeling
- Celery for asynchronous task processing
- Redis for job queue and caching
- Docker and Docker Compose for containerization

**Frontend:**
- HTML, CSS, JavaScript (Vanilla JS used in current implementation)
- Bootstrap for responsive styling

**Cloud Infrastructure (Example - AWS):**
- EC2 for application hosting
- S3 for file storage
- SES for email delivery
- CloudWatch for monitoring
- IAM for access control

**External Services:**
- OpenRouter for language model processing
- Tavily API for web searches

#### Data Flow Diagram

**Main Processing Flow:**
```
                
                                                                                
  Upload     Validation &   Normalize &   Process     Generate  
  Excel          Sanitization       Deduplicate       Batches         Output    
                                                                                
                
                                                               
                                                               

                                                                                          
                               Classification Processing                                  
                                                                                          
                         
                                                                                 
     Level 1       Level 2       Level 3       Level 4               
   Classification     Classification     Classification     Classification           
                                                                                 
                         
                                                                                      
                                                                                      
                         
    Validate &        Validate &        Validate &        Validate &             
     Process           Process           Process           Process               
    Responses         Responses         Responses         Responses              
                         
                                                                                          

                                               
                                               
                                       
                                                        
                                         Handle Unknown 
                                         Vendors with   
                                         Tavily Search  
                                                        
                                       
```

## 3. Detailed Technical Specifications

#### Component Specifications

**1. File Ingestion and Preprocessing Component**

This component handles the initial file upload, validation, and data extraction:

- **Functionality:**
  - Accept Excel files (.xlsx, .xls) containing vendor data
  - Validate file format and required columns (primarily `vendor_name`)
  - Extract vendor names and specified optional context fields (e.g., `vendor_address`, `vendor_website`, `internal_category`, `parent_company`, `spend_category`, `optional_example_good_serviced_purchased`)
  - Normalize vendor names (standardize case, remove duplicates, etc.)
  - Store sanitized data for further processing

- **Input Handling:**
  ```python
  def process_input_file(file_path: str) -> Dict[str, Any]:
      """Process the input Excel file and extract sanitized vendor data."""
      try:
          # Read Excel file
          df = pd.read_excel(file_path)

          # Check required columns
          if 'vendor_name' not in df.columns: # Case-insensitive check done in implementation
              raise ValueError("Required column 'vendor_name' not found")

          # Extract vendor name and optional context fields
          vendors_data = []
          # ... logic to extract based on file_service.py ...

          # Normalize vendor names (title case, remove duplicates)
          normalized_vendors_data = normalize_vendor_data(vendors_data) # Preserves other fields
          unique_vendors_map = {entry['vendor_name']: entry for entry in normalized_vendors_data}
          unique_vendors_list = list(unique_vendors_map.values())

          return {
              "total_records": len(df),
              "unique_vendors": len(unique_vendors_list),
              "vendors_data": unique_vendors_list # List of dicts with unique names and context
          }
      except Exception as e:
          logging.error(f"Error processing input file: {e}")
          raise
  ```

**2. Vendor Classification Component**

This component manages the hierarchical classification process:

- **Functionality:**
  - Create batches of vendors (including context data) for processing
  - Generate appropriate prompts for the LLM using vendor name and optional context
  - Send batches to OpenRouter API
  - Validate responses using Pydantic models and clean/parse JSON
  - Handle the 4-level hierarchical classification process
  - Manage unknown vendor resolution via Tavily Search

- **Classification Workflow:**
  ```python
  async def classify_vendors(vendors_data: List[Dict[str, Any]], taxonomy: Taxonomy) -> Dict[str, Any]:
      """Execute the full vendor classification workflow."""
      # Initialize results storage based on unique vendors
      unique_vendors_map = {vd['vendor_name']: vd for vd in vendors_data}
      results = {vendor_name: {} for vendor_name in unique_vendors_map.keys()}
      stats = {"api_calls": 0, "tokens": 0, "tavily_searches": 0} # Simplified stats example

      # Level 1 classification for all unique vendors
      # Batches contain full vendor dicts
      level1_batches_data = create_batches(list(unique_vendors_map.values()), batch_size=settings.BATCH_SIZE)
      level1_results = await process_level(level1_batches_data, 1, None, taxonomy, llm_service, stats) # Pass services/stats

      # Update results with Level 1 classifications
      for vendor_name, classification in level1_results.items():
          results[vendor_name]["level1"] = classification

      # Process subsequent levels (2-4) based on previous level groupings
      vendors_to_process_next_names = list(unique_vendors_map.keys())
      for level in range(2, 5):
          # Group vendor *names* by previous level classification
          grouped_vendors_names = group_by_parent_category(results, level-1, vendors_to_process_next_names)
          vendors_classified_in_level_names = []

          # Process each group separately
          for parent_category_id, group_vendor_names in grouped_vendors_names.items():
              if not group_vendor_names: continue
              # Get full data for vendors in this group
              group_vendor_data = [unique_vendors_map[name] for name in group_vendor_names if name in unique_vendors_map]
              level_batches_data = create_batches(group_vendor_data, batch_size=settings.BATCH_SIZE)
              level_results = await process_level(
                  level_batches_data, level, parent_category_id, taxonomy, llm_service, stats # Pass services/stats
              )

              # Update results with this level's classifications
              for vendor_name, classification in level_results.items():
                  results[vendor_name][f"level{level}"] = classification
                  if not classification.get("classification_not_possible", False):
                      vendors_classified_in_level_names.append(vendor_name)

          vendors_to_process_next_names = vendors_classified_in_level_names # Update list for next iteration

      # Handle unknown vendors that couldn't be classified
      unknown_vendors_data_to_search = identify_unknown_vendors(results, unique_vendors_map)
      if unknown_vendors_data_to_search:
          unknown_results = await process_unknown_vendors(unknown_vendors_data_to_search, taxonomy, llm_service, search_service, stats) # Pass services/stats
          # Update results with findings from Tavily searches
          for vendor_name, search_result in unknown_results.items():
              results[vendor_name]["search_results"] = search_result

      return {"classifications": results, "stats": stats}
  ```

**3. LLM Integration Component**

This component handles all interactions with OpenRouter:

- **Functionality:**
  - Format appropriate prompts based on taxonomy level and vendor data (name + optional context)
  - Send requests to OpenRouter API
  - Parse and validate JSON responses, handling potential LLM formatting issues (e.g., markdown fences, extra text)
  - Handle errors and retries
  - Track token usage and performance

- **Sample Prompt Generation (Level 1 - Updated):**
  ```python
  def create_classification_prompt(
      vendors_data: List[Dict[str, Any]], # List of vendor dicts
      level: int,
      parent_category: Optional[str] = None,
      taxonomy: Taxonomy,
      batch_id: str = "unique-id"
  ) -> str:
      """Create an appropriate prompt for the current classification level."""
      vendor_list_str = ""
      for i, vendor_entry in enumerate(vendors_data):
          vendor_name = vendor_entry.get('vendor_name', f'UnknownVendor_{i}')
          # Add optional fields (address, website, example, internal_cat, parent_co, spend_cat) if present
          # ... logic from llm_service.py ...
          vendor_list_str += f"\n{i+1}. Vendor Name: {vendor_name}"
          # ... add context lines ...

      if level == 1:
          categories = get_level1_categories(taxonomy)
          categories_str = "\n".join(f"- {cat.id}: {cat.name}" for cat in categories)

          prompt = f"""
          You are a vendor classification expert. Below is a list of vendors with optional context.
          Please classify each vendor according to the following Level 1 categories:

          {categories_str}

          For each vendor, provide:
          1. The most appropriate category ID and name
          2. A confidence level (0.0-1.0)
          Use the provided context (Examples, Address, Website, etc.) if available.

          If you cannot determine a category with reasonable confidence, mark it as "classification_not_possible".

          Vendor list:
          {vendor_list_str}

          **Output Format:** Respond *only* with a valid JSON object matching this exact schema. Do not include any text before or after the JSON object.
          ```json
          {{
            "level": 1,
            "batch_id": "{batch_id}",
            "parent_category_id": null,
            "classifications": [
              {{
                "vendor_name": "Vendor Name",
                "category_id": "ID or N/A",
                "category_name": "Category Name or N/A",
                "confidence": 0.95,
                "classification_not_possible": false,
                "classification_not_possible_reason": null
              }}
              // ... more classifications
            ]
          }}
          ```
          Ensure every vendor from the list is included in the `classifications` array with the exact vendor name provided. Ensure the `batch_id` in the response matches "{batch_id}".
          """
      else:
          # Similar logic for levels 2-4, including the explicit JSON output instruction
          # ...

      return prompt
  ```

**4. Tavily Search Integration Component**

This component handles research for unknown vendors:

- **Functionality:**
  - Format appropriate search queries for unknown vendors
  - Send requests to Tavily API
  - Process search results
  - Feed relevant information back to LLM for classification attempts
  - Track search usage and effectiveness

- **Tavily Integration:**
  ```python
  async def search_vendor_information(vendor_name: str) -> Dict[str, Any]:
      """Search for information about an unknown vendor using Tavily API."""
      search_query = f"{vendor_name} company business type industry"

      try:
          # Use httpx directly as in search_service.py
          payload = {
              "api_key": settings.TAVILY_API_KEY,
              "query": search_query,
              # ... other parameters ...
          }
          async with httpx.AsyncClient() as client:
              response = await client.post(f"{settings.TAVILY_API_BASE}/search", json=payload, timeout=30.0) # Assuming TAVILY_API_BASE is set
              response.raise_for_status()
              search_results = response.json()

          # Extract relevant information from search results
          processed_results = {
              "vendor": vendor_name,
              "search_query": search_query,
              "sources": [
                  {
                      "title": result.get("title", ""),
                      "url": result.get("url", ""),
                      "content": result.get("content", "")[:1500] # Increased limit for LLM
                  }
                  for result in search_results.get("results", []) if result.get("url")
              ],
              "summary": search_results.get("answer", ""),
              "error": None
          }

          return processed_results

      except Exception as e:
          logging.error(f"Tavily API error for vendor '{vendor_name}': {e}")
          return {
              "vendor": vendor_name,
              "error": str(e),
              "search_query": search_query,
              "sources": []
          }
  ```

**5. Result Generation Component**

This component compiles and formats the final output:

- **Functionality:**
  - Aggregate classification results from all levels
  - Format data according to output specifications, including original optional fields provided in input
  - Generate the output Excel file
  - Create logs and usage statistics

- **Result Compilation:**
  ```python
  def generate_output_file(
      original_vendor_data: List[Dict[str, Any]], # Original list of dicts from input
      classification_results: Dict[str, Dict], # Results keyed by unique, normalized name
      output_path: str
  ) -> None:
      """Generate the final output Excel file with classification results."""
      # Prepare data for Excel
      output_data = []

      for original_entry in original_vendor_data:
          vendor_name = original_entry.get('vendor_name') # Use the normalized name
          result = classification_results.get(vendor_name, {})

          # Combine original context with classification results
          row = {
              "vendor_name": vendor_name,
              # Include original optional fields (address, website, example, etc.) from original_entry
              "vendor_address": original_entry.get("vendor_address", ""),
              "vendor_website": original_entry.get("vendor_website", ""),
              "internal_category": original_entry.get("internal_category", ""),
              "parent_company": original_entry.get("parent_company", ""),
              "spend_category": original_entry.get("spend_category", ""),
              "Optional_example_good_serviced_purchased": original_entry.get("example", ""),
              # Classification results
              "level1_category_id": result.get("level1", {}).get("category_id", ""),
              "level1_category_name": result.get("level1", {}).get("category_name", ""),
              "level2_category_id": result.get("level2", {}).get("category_id", ""),
              "level2_category_name": result.get("level2", {}).get("category_name", ""),
              "level3_category_id": result.get("level3", {}).get("category_id", ""),
              "level3_category_name": result.get("level3", {}).get("category_name", ""),
              "level4_category_id": result.get("level4", {}).get("category_id", ""),
              "level4_category_name": result.get("level4", {}).get("category_name", ""),
              # Determine final confidence/status based on logic in file_service.py
              # ... final confidence, classification_not_possible, notes/reason ...
              "sources": ", ".join(
                  source.get("url", "") for source in result.get("search_results", {}).get("sources", []) if isinstance(source, dict) and source.get("url")
              ) if isinstance(result.get("search_results", {}).get("sources"), list) else ""
          }
          output_data.append(row)

      # Create DataFrame and write to Excel using explicit column order
      output_columns = [
          "vendor_name", "vendor_address", "vendor_website", "internal_category",
          "parent_company", "spend_category", "Optional_example_good_serviced_purchased",
          "level1_category_id", "level1_category_name", "level2_category_id", "level2_category_name",
          "level3_category_id", "level3_category_name", "level4_category_id", "level4_category_name",
          "final_confidence", "classification_not_possible", "classification_notes_or_reason", "sources"
      ]
      df = pd.DataFrame(output_data, columns=output_columns)
      df.to_excel(output_path, index=False)
  ```

#### API Definitions and Interfaces

**1. REST API Endpoints**

```
POST /api/v1/upload
- Purpose: Upload vendor Excel file for processing
- Request: multipart/form-data with file and company_name
- Response: {job_id: str, status: str, message: str, created_at: datetime, progress: float, current_stage: str}

GET /api/v1/jobs/{job_id}
- Purpose: Check job status
- Response: {
    job_id: str,
    status: str,
    progress: float,
    current_stage: str,
    created_at: datetime,
    updated_at: datetime,
    estimated_completion: Optional[datetime],
    error_message: Optional[str]
  }

GET /api/v1/jobs/{job_id}/download
- Purpose: Download job results
- Response: Excel file stream

POST /api/v1/jobs/{job_id}/notify
- Purpose: Request email notification when job completes
- Request: {email: str}
- Response: {success: bool, message: str}

GET /api/v1/jobs/{job_id}/stats
- Purpose: Get job processing statistics
- Response: {
    vendors_processed: int,
    unique_vendors: int,
    api_calls: int,
    tokens_used: int,
    tavily_searches: int,
    processing_time: float
  }

POST /token
- Purpose: Authenticate user and get JWT token
- Request: OAuth2PasswordRequestForm (username, password)
- Response: {access_token: str, token_type: str, username: str}

GET /health
- Purpose: Health check endpoint
- Response: {status: str, ...}
```

**2. Internal Component Interfaces**

```python
# TaskQueue Interface (Simplified via Celery)
# LLM Service Interface
class LLMService:
    async def classify_batch(
        self,
        batch_data: List[Dict[str, Any]], # List of vendor dicts
        level: int,
        taxonomy: Taxonomy,
        parent_category: Optional[str] = None
    ) -> Dict[str, Any]:
        """Send a batch of vendors (with context) to LLM for classification."""
        pass

    async def process_search_results(
        self,
        vendor_data: Dict[str, Any], # Vendor dict with context
        search_results: Dict[str, Any],
        taxonomy: Taxonomy
    ) -> Dict[str, Any]:
        """Process search results to determine classification."""
        pass

# Search Service Interface
class SearchService:
    async def search_vendor(self, vendor_name: str) -> Dict[str, Any]:
        """Search for information about a vendor."""
        pass
```

#### Data Models and Schema

**1. Taxonomy Model**

```python
from pydantic import BaseModel, Field
from typing import Dict, List, Optional

class TaxonomyCategory(BaseModel):
    id: str
    name: str
    description: Optional[str] = None

class TaxonomyLevel4(TaxonomyCategory):
    pass

class TaxonomyLevel3(TaxonomyCategory):
    children: Dict[str, TaxonomyLevel4]

class TaxonomyLevel2(TaxonomyCategory):
    children: Dict[str, TaxonomyLevel3]

class TaxonomyLevel1(TaxonomyCategory):
    children: Dict[str, TaxonomyLevel2]

class Taxonomy(BaseModel):
    name: str
    version: str
    description: Optional[str] = None
    categories: Dict[str, TaxonomyLevel1]
```

**2. Classification Models**

```python
class VendorClassification(BaseModel):
    vendor_name: str
    category_id: str
    category_name: str
    confidence: float = Field(ge=0.0, le=1.0)
    notes: Optional[str] = None
    classification_not_possible: bool = False
    classification_not_possible_reason: Optional[str] = None
    sources: Optional[List[Dict[str, str]]] = None

class ClassificationBatchResponse(BaseModel):
    level: int = Field(ge=1, le=4)
    batch_id: str
    parent_category_id: Optional[str] = None
    classifications: List[VendorClassification]
```

**3. Job Models**

```python
from enum import Enum
from datetime import datetime
from sqlalchemy import Column, String, Float, DateTime, Enum as SQLEnum, JSON, Text
from sqlalchemy.sql import func
from core.database import Base # Assuming Base is defined in database.py

class JobStatus(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingStage(str, Enum):
    INGESTION = "ingestion"
    NORMALIZATION = "normalization"
    CLASSIFICATION_L1 = "classification_level_1"
    CLASSIFICATION_L2 = "classification_level_2"
    CLASSIFICATION_L3 = "classification_level_3"
    CLASSIFICATION_L4 = "classification_level_4"
    SEARCH = "search_unknown_vendors"
    RESULT_GENERATION = "result_generation"

class Job(Base): # SQLAlchemy model
    __tablename__ = "jobs"

    id = Column(String, primary_key=True, index=True)
    company_name = Column(String, nullable=False)
    input_file_name = Column(String, nullable=False)
    output_file_name = Column(String, nullable=True)
    status = Column(String, default=JobStatus.PENDING.value)
    current_stage = Column(String, default=ProcessingStage.INGESTION.value)
    progress = Column(Float, default=0.0)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now(), server_default=func.now())
    completed_at = Column(DateTime(timezone=True), nullable=True)
    notification_email = Column(String, nullable=True)
    error_message = Column(Text, nullable=True)
    stats = Column(JSON, default={})
    created_by = Column(String, nullable=False) # Store username of creator

    # Methods to update status/progress/completion/failure
```

**4. Usage Statistics Model**

```python
from typing import Any # For start/end time flexibility

class ApiUsage(BaseModel):
    openrouter_calls: int = 0 # Renamed from azure_openai_calls
    openrouter_prompt_tokens: int = 0 # Renamed
    openrouter_completion_tokens: int = 0 # Renamed
    openrouter_total_tokens: int = 0 # Renamed
    tavily_search_calls: int = 0
    cost_estimate_usd: float = 0.0

class ProcessingStats(BaseModel): # Used within the Job model's JSON field
    job_id: str
    company_name: str
    start_time: Any # Can be datetime or ISO string
    end_time: Optional[Any] = None
    processing_duration_seconds: Optional[float] = None
    total_vendors: int = 0
    unique_vendors: int = 0
    successfully_classified: int = 0 # This may need refinement (e.g., initially vs after search)
    classification_not_possible: int = 0 # See above
    tavily_searches: int = 0
    tavily_search_successful_classifications: int = 0
    api_usage: ApiUsage = Field(default_factory=ApiUsage)
```

#### Security Considerations

**1. Data Protection**

- **Input Data Sanitization:**
  - Automated scanning for PII (SSNs, credit cards, etc.) - *Future Enhancement*
  - Field validation to ensure only necessary data is retained (e.g., vendor name, optional context)
  - Input scrubbing to remove potential injection attacks

- **Storage Security:**
  - All data stored with AES-256 encryption at rest (S3 default or equivalent)
  - Storage bucket policies restricting access
  - Temporary file management with secure deletion

- **Access Controls:**
  - Strict IAM roles and permissions (if using cloud provider)
  - Least privilege access principles
  - Regular access audits

**2. API Security**

- **Authentication:**
  - JWT-based authentication for all API endpoints
  - Token expiration and rotation
  - Rate limiting to prevent abuse

- **External API Protection:**
  - Secure storage of API keys (currently hardcoded, move to environment variables or secrets manager)
  - Regular rotation of API credentials
  - API access monitoring and alerting

**3. Compliance and Privacy**

- **Data Retention:**
  - Automated purging of data after processing (configurable retention period) - *Future Enhancement*
  - Clear data handling policies
  - Audit logs for all data access and operations

- **Transmission Security:**
  - TLS 1.3 for all data in transit
  - Secure headers configuration
  - CORS policy implementation

#### Performance Requirements

**1. Scalability**
- Support for processing files with up to 10,000 vendor entries
- Ability to handle multiple concurrent jobs via Celery workers
- Dynamic batch sizing based on system load (currently fixed)

**2. Processing Speed**
- Average processing time target: <5 seconds per vendor
- Complete job turnaround target: < 1 hour for files with up to 1,000 vendors

**3. API Usage Efficiency**
- Optimal batch sizing to minimize API calls (currently fixed at 5)
- Caching of common vendors to reduce duplicate searches - *Future Enhancement*
- Intelligent retry mechanisms for failed API calls (using Tenacity)

**4. Resource Requirements**
- Minimum EC2 instance (or equivalent): t3.medium for web service and worker (separate or combined depends on load)
- Recommended: t3.large for production workloads
- Memory: Minimum 4GB RAM per service
- Storage: 20GB base + ~1MB per job (input/output/logs)

## 4. Implementation Plan

#### Development Phases

**Phase 1: Core Infrastructure (2 weeks)** - Done
**Phase 2: Data Processing Pipeline (3 weeks)** - Done
**Phase 3: LLM Integration (2 weeks)** - Done (using OpenRouter)
**Phase 4: Unknown Vendor Resolution (2 weeks)** - Done (using Tavily)
**Phase 5: Web Interface and Job Management (2 weeks)** - Done (basic UI/Job tracking)
**Phase 6: Testing and Optimization (2 weeks)** - Ongoing
**Phase 7: Deployment and Monitoring (1 week)** - Done (basic Docker deployment)

#### Dependencies and Prerequisites

**1. Technical Dependencies** - Met
**2. Data Dependencies** - Met (using provided NAICS JSON)
**3. Knowledge Requirements** - Met by current team
**4. External Services Setup** - Met (OpenRouter/Tavily keys hardcoded for now)

## 5. Risks and Mitigation Strategies

#### Technical Risks

| Risk | Impact | Likelihood | Mitigation Strategy |
|------|--------|------------|---------------------|
| LLM response inconsistency/invalid category/format | High | Medium | Implement structured prompts with explicit JSON-only instruction, JSON response format request, robust JSON parsing (handling fences/extra text), post-validation against taxonomy, error handling for invalid JSON/structure, review workflows for low-confidence classifications. |
| API rate limiting or downtime | High | Medium | Use Tenacity for robust retries with exponential backoff, implement API call monitoring, consider fallback mechanisms if critical. |
| Performance bottlenecks with large files | Medium | Medium | Use Celery for asynchronous processing, optimize Pandas operations, monitor resource usage, consider database indexing. |
| Data format inconsistencies in input | Medium | High | Implement robust input validation (e.g., checking for `vendor_name`), flexible parsing (case-insensitive columns), clear error messages to user. |

#### Security Risks

| Risk | Impact | Likelihood | Mitigation Strategy |
|------|--------|------------|---------------------|
| Sensitive data exposure in input/output | Critical | Medium | Remove unnecessary columns during ingestion (`file_service.py`), implement data sanitization checks (*Future*), strict access controls, encryption at rest/transit. |
| API credential compromise | High | Low | Move API keys from config to environment variables/secrets manager, implement credential rotation, add access logging. |
| Unauthorized system access | High | Low | Implement strong authentication (JWT), role-based access (*Future*), security monitoring, regular dependency scanning. |
| Data retention compliance issues | Medium | Medium | Create clear data retention policies with automated enforcement (*Future*), ensure secure deletion of job files. |

#### Operational Risks

| Risk | Impact | Likelihood | Mitigation Strategy |
|------|--------|------------|---------------------|
| Cost overruns from API usage | Medium | Medium | Implement detailed usage tracking per job (stats), monitor API costs, optimize batch sizes/prompts, set budget alerts. |
| Classification accuracy below expectations | High | Medium | Develop feedback mechanisms (*Future*), continuous prompt improvement based on errors/low confidence results, allow manual override (*Future*). |
| User adoption challenges | Medium | Medium | Create intuitive UI, provide clear instructions, add error handling feedback, offer user support channel. |
| Dependency on external APIs | High | Medium | Add caching mechanisms (*Future*), monitor API status, have contingency plans if APIs become unavailable. |

#### Mitigation Approaches

**For LLM Classification Accuracy:**
- Validate LLM category IDs against the loaded taxonomy at each level.
- Set confidence thresholds for flagging uncertain results.
- Log failed/low-confidence classifications for prompt tuning.
- *Future:* Implement user feedback loop or manual review queue.

**For API Dependency Issues:**
- Use Tenacity for retries on network/server errors.
- Implement health checks for external services (*Future*).
- Log detailed API request/response metrics for troubleshooting.

**For Security and Compliance:**
- Regularly update dependencies (e.g., `pip-audit`).
- Move secrets out of code into environment variables or a secrets manager.
- Implement data lifecycle management (*Future*).

**For Cost Management:**
- Track token usage per job in the `Job.stats` field.
- Analyze usage patterns to potentially optimize prompts or batching.
- Set up billing alerts for cloud provider and API services.

</file>

<file path='frontend/vue_frontend/README.md'>
# vue_frontend

This template should help get you started developing with Vue 3 in Vite.

## Recommended IDE Setup

[VSCode](https://code.visualstudio.com/) + [Volar](https://marketplace.visualstudio.com/items?itemName=Vue.volar) (and disable Vetur).

## Type Support for `.vue` Imports in TS

TypeScript cannot handle type information for `.vue` imports by default, so we replace the `tsc` CLI with `vue-tsc` for type checking. In editors, we need [Volar](https://marketplace.visualstudio.com/items?itemName=Vue.volar) to make the TypeScript language service aware of `.vue` types.

## Customize configuration

See [Vite Configuration Reference](https://vite.dev/config/).

## Project Setup

```sh
npm install
```

### Compile and Hot-Reload for Development

```sh
npm run dev
```

### Type-Check, Compile and Minify for Production

```sh
npm run build
```

</file>

<file path='frontend/vue_frontend/env.d.ts'>
/// <reference types="vite/client" />

</file>

<file path='frontend/vue_frontend/index.html'>
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <link rel="icon" href="/favicon.ico"> <!-- Make sure favicon.ico exists in public/ -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NAICS Vendor Classification</title>
  </head>
  <body>
    <div id="app">
        <!-- Vue app mounts here -->
        <div style="text-align: center; padding: 50px; font-family: sans-serif; color: #666;">Loading Application...</div>
    </div>

    <!-- Vite injects built JS here -->
    <script type="module" src="/src/main.ts"></script>
  </body>
</html>
</file>

<file path='frontend/vue_frontend/postcss.config.js'>
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}

</file>

<file path='frontend/vue_frontend/src/App.vue'>
<template>
  <div class="flex flex-col min-h-screen">
  <Navbar
      :is-logged-in="authStore.isAuthenticated"
      :username="authStore.username"
      @logout="handleLogout"
  />

  <!-- main content area that grows -->
  <main role="main" class="flex-grow w-full mx-auto">
      <!-- Render based on viewStore -->
      <LandingPage v-if="viewStore.currentView === 'landing'" @login-successful="handleLoginSuccess" />
      <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8" v-else-if="viewStore.currentView === 'app'">
      <AppContent />
      </div>
      <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8" v-else-if="viewStore.currentView === 'admin'">
      <!-- Placeholder for Admin Content -->
      <UserManagement />
      </div>
  </main>

  <Footer />
  </div>
</template>

<script setup lang="ts">
import { onMounted, watch } from 'vue';
import Navbar from './components/Navbar.vue';
import LandingPage from './components/LandingPage.vue';
import AppContent from './components/AppContent.vue';
import Footer from './components/Footer.vue';
import UserManagement from './components/UserManagement.vue'; // Import the new component
import { useAuthStore } from './stores/auth';
import { useJobStore } from './stores/job';
import { useViewStore } from './stores/view'; // Import the view store

const authStore = useAuthStore();
const jobStore = useJobStore();
const viewStore = useViewStore(); // Use the view store

const handleLogout = () => {
  authStore.logout();
  jobStore.clearJob();
  viewStore.setView('landing'); // Go to landing page on logout
  // No need to manually clear URL, logout reloads page
};

const handleLoginSuccess = () => {
  console.log('Login successful, App.vue notified.');
  viewStore.setView('app'); // Go to app content on login
  const urlParams = new URLSearchParams(window.location.search);
  const jobIdFromUrl = urlParams.get('job_id');
  if (jobIdFromUrl) {
      console.log(`App.vue: Found Job ID in URL after login: ${jobIdFromUrl}. Setting in store.`);
      jobStore.setCurrentJobId(jobIdFromUrl);
  }
  // Fetch user details after login to ensure superuser status is up-to-date
  authStore.fetchCurrentUserDetails();
};

// Watch auth state to set initial view
watch(() => authStore.isAuthenticated, (isAuth) => {
  if (isAuth && viewStore.currentView === 'landing') {
      viewStore.setView('app');
  } else if (!isAuth) {
      viewStore.setView('landing');
  }
}, { immediate: true }); // Run immediately on mount

onMounted(() => {
  authStore.checkAuthStatus(); // This will trigger the watcher above
  // If authenticated, potentially fetch user details again
  if (authStore.isAuthenticated) {
      authStore.fetchCurrentUserDetails();
      const urlParams = new URLSearchParams(window.location.search);
      const jobIdFromUrl = urlParams.get('job_id');
      if (jobIdFromUrl && !jobStore.currentJobId) {
      console.log(`App.vue: Found Job ID in URL on mount: ${jobIdFromUrl}. Setting in store.`);
      jobStore.setCurrentJobId(jobIdFromUrl);
      }
  }
});
</script>
</file>

<file path='frontend/vue_frontend/src/assets/base.css'>
/* color palette from <https://github.com/vuejs/theme> */
:root {
  --vt-c-white: #ffffff;
  --vt-c-white-soft: #f8f8f8;
  --vt-c-white-mute: #f2f2f2;

  --vt-c-black: #181818;
  --vt-c-black-soft: #222222;
  --vt-c-black-mute: #282828;

  --vt-c-indigo: #2c3e50;

  --vt-c-divider-light-1: rgba(60, 60, 60, 0.29);
  --vt-c-divider-light-2: rgba(60, 60, 60, 0.12);
  --vt-c-divider-dark-1: rgba(84, 84, 84, 0.65);
  --vt-c-divider-dark-2: rgba(84, 84, 84, 0.48);

  --vt-c-text-light-1: var(--vt-c-indigo);
  --vt-c-text-light-2: rgba(60, 60, 60, 0.66);
  --vt-c-text-dark-1: var(--vt-c-white);
  --vt-c-text-dark-2: rgba(235, 235, 235, 0.64);
}

/* semantic color variables for this project */
:root {
  --color-background: var(--vt-c-white);
  --color-background-soft: var(--vt-c-white-soft);
  --color-background-mute: var(--vt-c-white-mute);

  --color-border: var(--vt-c-divider-light-2);
  --color-border-hover: var(--vt-c-divider-light-1);

  --color-heading: var(--vt-c-text-light-1);
  --color-text: var(--vt-c-text-light-1);

  --section-gap: 160px;
}

@media (prefers-color-scheme: dark) {
  :root {
    --color-background: var(--vt-c-black);
    --color-background-soft: var(--vt-c-black-soft);
    --color-background-mute: var(--vt-c-black-mute);

    --color-border: var(--vt-c-divider-dark-2);
    --color-border-hover: var(--vt-c-divider-dark-1);

    --color-heading: var(--vt-c-text-dark-1);
    --color-text: var(--vt-c-text-dark-2);
  }
}

*,
*::before,
*::after {
  box-sizing: border-box;
  margin: 0;
  font-weight: normal;
}

body {
  min-height: 100vh;
  color: var(--color-text);
  background: var(--color-background);
  transition:
    color 0.5s,
    background-color 0.5s;
  line-height: 1.6;
  font-family:
    Inter,
    -apple-system,
    BlinkMacSystemFont,
    'Segoe UI',
    Roboto,
    Oxygen,
    Ubuntu,
    Cantarell,
    'Fira Sans',
    'Droid Sans',
    'Helvetica Neue',
    sans-serif;
  font-size: 15px;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

</file>

<file path='frontend/vue_frontend/src/assets/main.css'>
@import './base.css';

#app {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  font-weight: normal;
}

a,
.green {
  text-decoration: none;
  color: hsla(160, 100%, 37%, 1);
  transition: 0.4s;
  padding: 3px;
}

@media (hover: hover) {
  a:hover {
    background-color: hsla(160, 100%, 37%, 0.2);
  }
}

@media (min-width: 1024px) {
  body {
    display: flex;
    place-items: center;
  }

  #app {
    display: grid;
    grid-template-columns: 1fr 1fr;
    padding: 0 2rem;
  }
}

</file>

<file path='frontend/vue_frontend/src/assets/styles.css'>
@tailwind base;
@tailwind components;
@tailwind utilities;

/* Optional: Add any global base styles here if needed */
body {
  @apply font-sans antialiased bg-light text-gray-800; /* Example base styles using tailwind */
   padding-top: 4rem; /* Adjust for fixed navbar height */
}
</file>

<file path='frontend/vue_frontend/src/components/AppContent.vue'>
<template>
  <div class="space-y-8 md:space-y-12"> <!-- Adds vertical space between children -->
      <!-- Upload Form Section -->
      <section aria-labelledby="upload-heading">
          <div class="max-w-2xl mx-auto">
              <h2 id="upload-heading" class="sr-only">Upload Vendor File</h2> <!-- Screen reader heading -->
              <UploadForm @upload-successful="handleUploadSuccess" />
          </div>
      </section>

       <!-- Job Status Section (for currently selected job) -->
      <section aria-labelledby="status-heading" v-if="jobStore.currentJobId">
          <div class="max-w-4xl mx-auto">
              <h2 id="status-heading" class="sr-only">Current Job Status and Results</h2> <!-- Screen reader heading -->
              <JobStatus :key="jobStore.currentJobId" />
              <!-- key forces re-render if job ID changes -->
          </div>
      </section>

      <!-- Placeholder if no job active and logged in -->
      <div v-else class="text-center py-16 text-gray-500 bg-white rounded-lg shadow border border-gray-200 max-w-4xl mx-auto">
          <p class="text-lg mb-2">No active job selected.</p>
          <p>Upload a file above to start a new classification, or select a job from the history below.</p>
      </div>

      <!-- Job History Section -->
      <section aria-labelledby="history-heading">
          <div class="max-w-6xl mx-auto"> <!-- Wider container for table -->
               <h2 id="history-heading" class="sr-only">Job History</h2> <!-- Screen reader heading -->
               <JobHistory />
          </div>
      </section>

  </div>
</template>

<script setup lang="ts">
import UploadForm from './UploadForm.vue';
import JobStatus from './JobStatus.vue';
import JobHistory from './JobHistory.vue'; // <-- Import JobHistory
import { useJobStore } from '@/stores/job';

const jobStore = useJobStore();

const handleUploadSuccess = (jobId: string) => {
  console.log(`AppContent: Received upload-successful event for job ${jobId}`);
  jobStore.setCurrentJobId(jobId);
  // Optionally trigger a refresh of the history list after a short delay
  setTimeout(() => {
      jobStore.fetchJobHistory({ limit: 100 });
  }, 1500);
};
</script>

<style scoped>
/* No scoped styles needed */
</style>
</file>

<file path='frontend/vue_frontend/src/components/Footer.vue'>
<template>
  <footer class="mt-auto py-4 bg-gray-100 border-t border-gray-200">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
      <span class="text-sm text-gray-500"> {{ currentYear }} Vendor Classification Inc. All Rights Reserved.</span>
    </div>
  </footer>
</template>

<script setup lang="ts">
import { computed } from 'vue';
const currentYear = computed(() => new Date().getFullYear());
</script>

<style scoped>
/* Footer is simple, scoped styles likely not needed */
</style>
</file>

<file path='frontend/vue_frontend/src/components/JobHistory.vue'>
<template>
    <div class="mt-10 bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200">
        <h4 class="text-xl font-semibold mb-0">Job History</h4>
      </div>
      <div class="p-6 sm:p-8">
        <!-- Loading State -->
        <div v-if="historyLoading" class="text-center text-gray-500 py-8">
          <svg class="animate-spin inline-block h-6 w-6 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
          <span>Loading job history...</span>
        </div>
  
        <!-- Error State -->
        <div v-else-if="historyError" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
          <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
          <span>Error loading history: {{ historyError }}</span>
        </div>
  
        <!-- Empty State -->
        <div v-else-if="!jobHistory || jobHistory.length === 0" class="text-center text-gray-500 py-8">
          <p>No job history found.</p>
          <p class="text-sm">Upload a file to start your first job.</p>
        </div>
  
        <!-- History Table -->
        <div v-else class="overflow-x-auto">
          <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
              <tr>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Job ID
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Company
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Status
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Created
                </th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                  Actions
                </th>
              </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
              <tr v-for="job in jobHistory" :key="job.id" class="hover:bg-gray-50 cursor-pointer" @click="selectJob(job.id)">
                <td class="px-4 py-3 whitespace-nowrap text-xs font-mono text-gray-700">
                  {{ job.id.substring(0, 8) }}...
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-800 font-medium">
                  {{ job.company_name }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap">
                  <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="getStatusBadgeClass(job.status)">
                    {{ job.status }}
                  </span>
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-500">
                  {{ formatDateTime(job.created_at) }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-right text-sm font-medium">
                  <button
                    v-if="job.status === 'completed'"
                    @click.stop="downloadResults(job.id, $event)"
                    :disabled="isDownloadLoading(job.id)"
                    class="text-primary hover:text-primary-hover disabled:opacity-50 disabled:cursor-not-allowed inline-flex items-center"
                    title="Download Results"
                  >
                     <svg v-if="isDownloadLoading(job.id)" class="animate-spin h-4 w-4 text-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                     </svg>
                     <ArrowDownTrayIcon v-else class="h-4 w-4" />
                    <!-- <span class="ml-1">Download</span> -->
                  </button>
                  <span v-else-if="job.status === 'failed'" class="text-red-500 text-xs italic" title="Job Failed">Failed</span>
                  <span v-else class="text-gray-400 text-xs italic" title="Processing or Pending">In Progress</span>
                  <!-- Add View Details button if needed -->
                  <!-- <button @click.stop="selectJob(job.id)" class="text-indigo-600 hover:text-indigo-900 ml-3">View</button> -->
                </td>
              </tr>
            </tbody>
          </table>
        </div>
  
        <!-- TODO: Add Pagination Controls if needed -->
  
      </div>
    </div>
  </template>
  
  <script setup lang="ts">
  import { computed, onMounted, ref } from 'vue';
  import { useJobStore } from '@/stores/job';
  import { ExclamationTriangleIcon, ArrowDownTrayIcon } from '@heroicons/vue/20/solid';
  import apiService from '@/services/api';
  
  const jobStore = useJobStore();
  
  const jobHistory = computed(() => jobStore.jobHistory);
  const historyLoading = computed(() => jobStore.historyLoading);
  const historyError = computed(() => jobStore.historyError);
  
  // State for managing individual download button loading
  const downloadingJobs = ref<Set<string>>(new Set());
  const downloadErrors = ref<Record<string, string | null>>({});
  
  const isDownloadLoading = (jobId: string) => downloadingJobs.value.has(jobId);
  
  const fetchHistory = async () => {
    await jobStore.fetchJobHistory({ limit: 100 }); // Fetch latest 100 jobs on mount
  };
  
  const selectJob = (jobId: string) => {
    console.log(`JobHistory: Selecting job ${jobId}`);
    jobStore.setCurrentJobId(jobId);
    // Optional: Scroll to the top or to the JobStatus component
    window.scrollTo({ top: 0, behavior: 'smooth' });
  };
  
  const formatDateTime = (isoString: string | null | undefined): string => {
    if (!isoString) return 'N/A';
    try {
      return new Date(isoString).toLocaleString(undefined, {
        year: 'numeric', month: 'short', day: 'numeric',
        hour: 'numeric', minute: '2-digit', hour12: true
      });
    } catch {
      return 'Invalid Date';
    }
  };
  
  const getStatusBadgeClass = (status: string | undefined) => {
    switch (status) {
      case 'completed': return 'bg-green-100 text-green-800';
      case 'failed': return 'bg-red-100 text-red-800';
      case 'processing': return 'bg-blue-100 text-blue-800';
      default: return 'bg-gray-100 text-gray-800';
    }
  };
  
  const downloadResults = async (jobId: string, event: Event) => {
     event.stopPropagation(); // Prevent row click when clicking button
     if (!jobId || downloadingJobs.value.has(jobId)) return;
  
     downloadingJobs.value.add(jobId);
     downloadErrors.value[jobId] = null;
  
    try {
      const { blob, filename } = await apiService.downloadResults(jobId);
      const url = window.URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.style.display = 'none';
      a.href = url;
      a.download = filename;
      document.body.appendChild(a);
      a.click();
      window.URL.revokeObjectURL(url);
      document.body.removeChild(a);
    } catch (error: any) {
      console.error(`Download failed for job ${jobId}:`, error);
      downloadErrors.value[jobId] = `Download failed: ${error.message || 'Error'}`;
      // Optionally clear the error message after a delay
      setTimeout(() => { downloadErrors.value[jobId] = null; }, 5000);
    } finally {
      downloadingJobs.value.delete(jobId);
    }
  };
  
  
  onMounted(() => {
    fetchHistory();
  });
  </script>
  
  <style scoped>
  /* Add specific styles if needed */
  tbody tr:hover {
    background-color: #f9fafb; /* Tailwind gray-50 */
  }
  </style>
</file>

<file path='frontend/vue_frontend/src/components/JobStats.vue'>
    <template>
        <div class="mt-6 bg-gray-50 rounded-lg p-6 border border-gray-200 shadow-inner">
          <h5 class="text-lg font-semibold text-gray-700 mb-5 border-b border-gray-200 pb-3">
              Processing Statistics
              <!-- ADDED: Display Target Level -->
              <span v-if="jobTargetLevel" class="text-sm font-normal text-gray-500 ml-2">(Target Level: {{ jobTargetLevel }})</span>
          </h5>
          <div v-if="isLoading" class="text-center text-gray-500 py-4">
                <svg class="animate-spin inline-block h-5 w-5 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                   <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                   <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                </svg>
                <span>Loading stats...</span>
          </div>
          <div v-else-if="error" class="p-3 bg-yellow-100 border border-yellow-300 text-yellow-800 rounded-md text-sm">
              {{ error }}
          </div>
          <div v-else-if="stats" class="grid grid-cols-1 sm:grid-cols-2 gap-x-8 gap-y-4 text-sm">
              <!-- Column 1: Vendor & Classification Stats -->
              <div class="space-y-2.5">
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Total Vendors:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.total_vendors?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Unique Vendors:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.unique_vendors?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- UPDATED: Display L5 Success (Keep label static for now, but value is correct) -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Successfully Classified (L5):</strong>
                      <span class="text-green-700 font-semibold">{{ stats.successfully_classified_l5?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- UPDATED: Display L5 Search Success (Corrected field name) -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Search Assisted (L5):</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.search_successful_classifications_l5?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <!-- Keep L4 for reference if desired -->
                   <p class="flex justify-between text-xs text-gray-500">
                      <strong class="font-normal">Ref: Classified (L4):</strong>
                      <span class="font-normal">{{ stats.successfully_classified_l4?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Invalid Category Errors:</strong>
                      <span :class="(stats.invalid_category_errors ?? 0) > 0 ? 'text-red-600 font-semibold' : 'text-gray-800 font-semibold'">
                          {{ stats.invalid_category_errors?.toLocaleString() ?? 'N/A' }}
                      </span>
                  </p>
              </div>
               <!-- Column 2: API & Time Stats -->
               <div class="space-y-2.5">
                  <!-- UPDATED: Access nested fields -->
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">LLM API Calls:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.openrouter_calls?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">LLM Tokens Used:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.openrouter_total_tokens?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between">
                      <strong class="text-gray-600 font-medium">Search API Calls:</strong>
                      <span class="text-gray-800 font-semibold">{{ stats.api_usage?.tavily_search_calls?.toLocaleString() ?? 'N/A' }}</span>
                  </p>
                  <p class="flex justify-between pt-2 mt-1 border-t border-gray-200">
                      <strong class="text-gray-600 font-medium">Processing Time:</strong>
                      <!-- UPDATED: Use correct field name -->
                      <span class="text-gray-800 font-semibold">{{ formattedTime }}</span>
                  </p>
               </div>
          </div>
           <div v-else class="text-gray-500 text-center py-4 text-sm">No statistics available for this job.</div>
        </div>
      </template>
    
    <script setup lang="ts">
    import { ref, onMounted, watch, computed } from 'vue';
    import apiService, { type JobStatsData } from '@/services/api';
    // ADDED: Import job store to access target level
    import { useJobStore } from '@/stores/job';
    
    // Define the component props
    interface Props {
        jobId: string | null | undefined; // Allow jobId to be potentially null or undefined
    }
    const props = defineProps<Props>();
    
    // ADDED: Access job store
    const jobStore = useJobStore();
    
    // Reactive state variables
    const stats = ref<JobStatsData | null>(null); // Use the imported type
    const isLoading = ref(false);
    const error = ref<string | null>(null);
    
    // ADDED: Computed property to get target level from store
    const jobTargetLevel = computed(() => jobStore.jobDetails?.target_level);
    
    // Computed property to format processing time nicely
    const formattedTime = computed(() => {
        // UPDATED: Access correct field name
        if (stats.value?.processing_duration_seconds == null) return 'N/A'; // Check for null or undefined
        const seconds = stats.value.processing_duration_seconds;
        if (seconds < 0) return 'N/A'; // Handle potential negative values if they occur
        if (seconds < 1) return `${(seconds * 1000).toFixed(0)} ms`;
        if (seconds < 60) return `${seconds.toFixed(1)} seconds`;
        const minutes = Math.floor(seconds / 60);
        const remainingSeconds = (seconds % 60).toFixed(0);
        return `${minutes} min ${remainingSeconds} sec`;
    });
    
    /**
     * Fetches job statistics from the API for the given job ID.
     * @param {string | null | undefined} id - The job ID to fetch stats for.
     */
    const fetchStats = async (id: string | null | undefined) => {
      // Only proceed if id is a non-empty string
      if (!id) {
          console.log("JobStats: fetchStats called with no ID, skipping."); // Logging
          stats.value = null; // Clear previous stats if ID is null/undefined
          error.value = null;
          isLoading.value = false;
          return;
      }
    
      isLoading.value = true;
      error.value = null;
      // Don't clear stats immediately, only on successful fetch or error
      // stats.value = null;
    
      try {
          console.log(`JobStats: Fetching stats for job ID: ${id}`); // Logging
          // The API service now returns the updated structure
          stats.value = await apiService.getJobStats(id);
          // LOGGING: Log the received stats structure after fetch
          console.log(`JobStats: Received and assigned stats:`, JSON.parse(JSON.stringify(stats.value)));
      } catch (err: any) {
          console.error(`JobStats: Error fetching stats for ${id}:`, err); // Logging
          error.value = err.message || 'Failed to load statistics.';
          stats.value = null; // Clear stats on error
      } finally {
          isLoading.value = false;
      }
    };
    
    // Fetch stats when the component mounts
    onMounted(() => {
        console.log(`JobStats: Mounted with initial jobId: ${props.jobId}`); // Logging
        fetchStats(props.jobId);
    });
    
    // Watch for changes in the jobId prop and refetch stats
    watch(() => props.jobId,
      (newJobId: string | null | undefined) => {
        console.log(`JobStats: Watched jobId changed to: ${newJobId}`); // Logging
        fetchStats(newJobId); // fetchStats handles null/undefined check internally
      },
      { immediate: false } // Don't run immediately, onMounted handles initial fetch
    );
    </script>
    
    <style scoped>
      .shadow-inner {
          box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.05);
      }
      /* Add any other specific styles if needed */
    </style>
</file>

<file path='frontend/vue_frontend/src/components/JobStatus.vue'>
<template>
    <div v-if="jobStore.currentJobId" class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
      <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200">
        <h4 class="text-xl font-semibold mb-0">Job Status</h4>
      </div>
      <div class="p-6 sm:p-8 space-y-6"> <!-- Increased spacing -->

        <!-- Error Message -->
        <div v-if="errorMessage" class="p-3 bg-yellow-100 border border-yellow-300 text-yellow-800 rounded-md text-sm flex items-center">
            <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-yellow-600 flex-shrink-0"/>
            <span>{{ errorMessage }}</span>
        </div>

        <!-- Job ID & Status Row -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm border-b border-gray-100 pb-4">
          <div>
             <strong class="text-gray-600 block mb-1">Job ID:</strong>
             <!-- Display the full ID for clarity during debugging -->
             <span class="text-gray-900 font-mono text-xs bg-gray-100 px-2 py-1 rounded break-all">{{ jobStore.currentJobId }}</span>
          </div>
           <div class="flex items-center space-x-2">
             <strong class="text-gray-600">Status:</strong>
             <span class="px-2.5 py-0.5 rounded-full text-xs font-bold uppercase tracking-wide" :class="statusBadgeClass">
                 <!-- Use jobDetails.status directly -->
                 {{ jobDetails?.status || 'Loading...' }}
             </span>
           </div>
        </div>

        <!-- Stage & Error (if failed) -->
        <div class="text-sm">
            <strong class="text-gray-600 block mb-1">Current Stage:</strong>
            <span class="text-gray-800 font-medium">{{ formattedStage }}</span>
            <!-- Use jobDetails.error_message -->
            <div v-if="jobDetails?.status === 'failed' && jobDetails?.error_message" class="mt-3 p-4 bg-red-50 border border-red-200 text-red-800 rounded-md text-xs shadow-sm">
              <strong class="block mb-1 font-semibold">Error Details:</strong>
              <p class="whitespace-pre-wrap">{{ jobDetails.error_message }}</p> <!-- Preserve whitespace -->
            </div>
        </div>

        <!-- Progress Bar -->
        <div>
          <label class="block text-sm font-medium text-gray-600 mb-1.5">Progress:</label>
          <div class="w-full bg-gray-200 rounded-full h-3 overflow-hidden"> <!-- Slimmer progress bar -->
            <div
              class="h-3 rounded-full transition-all duration-500 ease-out"
              :class="progressColorClass"
              :style="{ width: progressPercent + '%' }"
              ></div>
          </div>
          <div class="text-right text-xs text-gray-500 mt-1">{{ progressPercent }}% Complete</div>
        </div>

        <!-- Timestamps Row -->
        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-xs text-gray-500 border-t border-gray-100 pt-5">
            <div>
                 <strong class="block text-gray-600 mb-0.5">Created:</strong>
                 <span>{{ formattedCreatedAt }}</span>
            </div>
            <div>
                 <strong class="block text-gray-600 mb-0.5">Updated:</strong>
                 <span>{{ formattedUpdatedAt }}</span>
            </div>
             <div>
                <strong class="block text-gray-600 mb-0.5">Est. Completion:</strong>
                <span>{{ formattedEstimatedCompletion }}</span>
            </div>
        </div>

        <!-- Notification Section -->
        <div v-if="canRequestNotify" class="pt-5 border-t border-gray-100">
            <label for="notificationEmail" class="block text-sm font-medium text-gray-700 mb-1.5">Get Notified Upon Completion</label>
            <div class="flex flex-col sm:flex-row sm:space-x-2">
                <input type="email"
                       class="mb-2 sm:mb-0 flex-grow block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100"
                       id="notificationEmail"
                       placeholder="Enter your email"
                       v-model="notificationEmail"
                       :disabled="isNotifyLoading" />
                <button
                    type="button"
                    @click="requestNotification"
                    :disabled="isNotifyLoading || !notificationEmail"
                    class="w-full sm:w-auto inline-flex justify-center items-center px-4 py-2 border border-gray-300 rounded-md shadow-sm text-sm font-medium text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed"
                >
                    <svg v-if="isNotifyLoading" class="animate-spin -ml-1 mr-2 h-4 w-4 text-gray-700" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                       <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                       <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                     <EnvelopeIcon v-else class="h-4 w-4 mr-2 -ml-1 text-gray-500"/>
                    {{ isNotifyLoading ? 'Sending...' : 'Notify Me' }}
                </button>
            </div>
            <!-- Notification Feedback -->
            <p v-if="notifyMessage" :class="notifyMessageIsError ? 'text-red-600' : 'text-green-600'" class="mt-2 text-xs">{{ notifyMessage }}</p>
        </div>

        <!-- Download Section -->
        <div v-if="jobDetails?.status === 'completed'" class="pt-5 border-t border-gray-100">
          <button @click="downloadResults"
                  class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-green-600 hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-green-500 disabled:opacity-50 disabled:cursor-not-allowed"
                  :disabled="isDownloadLoading">
             <!-- Spinner SVG -->
             <svg v-if="isDownloadLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
             </svg>
             <!-- Download Icon -->
             <ArrowDownTrayIcon v-else class="h-5 w-5 mr-2 -ml-1" />
            {{ isDownloadLoading ? ' Preparing Download...' : 'Download Results' }}
          </button>
          <p v-if="downloadError" class="mt-2 text-xs text-red-600 text-center">{{ downloadError }}</p>
        </div>

        <!-- Stats Section (Rendered within JobStatus when complete) -->
         <!-- Use jobDetails.id -->
         <JobStats v-if="jobDetails?.status === 'completed' && jobDetails?.id" :job-id="jobDetails.id" />

      </div>
    </div>
      <div v-else class="text-center py-10 text-gray-500">
        <!-- Message shown when no job is selected -->
        <!-- Select a job from the history or upload a new file. -->
      </div>
  </template>

  <script setup lang="ts">
  import { ref, computed, onMounted, onUnmounted, watch } from 'vue';
  import apiService from '@/services/api';
  import { useJobStore, type JobDetails } from '@/stores/job';
  import JobStats from './JobStats.vue';
  import { EnvelopeIcon, ArrowDownTrayIcon, ExclamationTriangleIcon } from '@heroicons/vue/20/solid';

  const POLLING_INTERVAL = 5000; // Poll every 5 seconds
  const jobStore = useJobStore();
  // Use jobDetails directly from the store
  const jobDetails = computed(() => jobStore.jobDetails);
  const isLoading = ref(false); // Tracks if a poll request is currently in flight
  const errorMessage = ref<string | null>(null); // Stores polling or general errors
  const pollingIntervalId = ref<number | null>(null); // Stores the ID from setInterval

  // --- Notification State ---
  const notificationEmail = ref('');
  const isNotifyLoading = ref(false);
  const notifyMessage = ref<string | null>(null);
  const notifyMessageIsError = ref(false);

  // --- Download State ---
  const isDownloadLoading = ref(false);
  const downloadError = ref<string | null>(null);

  // --- Computed Properties ---
  const formattedStage = computed(() => {
    const stage = jobDetails.value?.current_stage;
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'Completed';
    if (status === 'failed') return 'Failed';
    if (status === 'pending') return 'Pending Start';
    if (!stage) return 'Loading...';

    // Map internal stage names to user-friendly display names
    const stageNames: { [key: string]: string } = {
      'ingestion': 'Ingesting File',
      'normalization': 'Normalizing Data',
      'classification_level_1': 'Classifying (L1)',
      'classification_level_2': 'Classifying (L2)',
      'classification_level_3': 'Classifying (L3)',
      'classification_level_4': 'Classifying (L4)',
      'search_unknown_vendors': 'Researching Vendors',
      'result_generation': 'Generating Results',
    };
    return stageNames[stage] || stage; // Fallback to raw stage name if not mapped
  });

  const progressPercent = computed(() => {
    const status = jobDetails.value?.status;
    const progress = jobDetails.value?.progress ?? 0;
    if (status === 'completed') return 100;
    if (status === 'pending') return 0; // Show 0% for pending
    // Ensure progress is between 0 and 100
    return Math.max(0, Math.min(100, Math.round(progress * 100)));
  });

  const statusBadgeClass = computed(() => {
      switch (jobDetails.value?.status) {
          case 'completed': return 'bg-green-100 text-green-800';
          case 'failed': return 'bg-red-100 text-red-800';
          case 'processing': return 'bg-blue-100 text-blue-800 animate-pulse'; // Add pulse for processing
          default: return 'bg-gray-100 text-gray-800'; // Pending or loading
      }
  });

  const progressColorClass = computed(() => {
    const status = jobDetails.value?.status;
    if (status === 'completed') return 'bg-green-500';
    if (status === 'failed') return 'bg-red-500';
    // Use primary color and pulse animation for processing/pending
    return 'bg-primary animate-pulse';
  });

  const formatDateTime = (isoString: string | null | undefined): string => {
      if (!isoString) return 'N/A';
      try {
          // Use user's locale and common options
          return new Date(isoString).toLocaleString(undefined, {
              year: 'numeric', month: 'short', day: 'numeric',
              hour: 'numeric', minute: '2-digit', hour12: true
          });
      } catch {
          return 'Invalid Date';
      }
  };

  const formattedCreatedAt = computed(() => formatDateTime(jobDetails.value?.created_at));
  const formattedUpdatedAt = computed(() => formatDateTime(jobDetails.value?.updated_at));

  const formattedEstimatedCompletion = computed(() => {
      const status = jobDetails.value?.status;
      if (status === 'completed' && jobDetails.value?.completed_at) {
          return formatDateTime(jobDetails.value.completed_at);
      }
      // Assuming backend provides 'estimated_completion' during processing
      // --- CHECK FIELD NAME ---
      // Check if the field is 'estimated_completion' or something else in JobDetails
      const estCompletion = jobDetails.value?.estimated_completion; // Use the actual field name
      if (status === 'processing' && estCompletion) {
          return `${formatDateTime(estCompletion)} (est.)`;
      }
      // --- END CHECK ---
      if (status === 'processing') return 'Calculating...';
      if (status === 'failed') return 'N/A';
      if (status === 'pending') return 'Pending Start';
      return 'N/A';
  });

  const canRequestNotify = computed(() => {
      const status = jobDetails.value?.status;
      return status === 'pending' || status === 'processing';
  });

  // --- Methods ---
  const pollJobStatus = async (jobId: string | null | undefined) => {
     // Check if we should still be polling this job
     if (!jobId || jobStore.currentJobId !== jobId) {
         console.log(`JobStatus: [pollJobStatus] Stopping polling because jobId (${jobId}) doesn't match store (${jobStore.currentJobId}) or is null.`); // LOGGING
         stopPolling();
         return;
     }

     // Avoid concurrent polls
     if (isLoading.value) {
         console.log(`JobStatus: [pollJobStatus] Skipping poll for ${jobId} as another poll is already in progress.`); // LOGGING
         return;
     }

     isLoading.value = true;
     console.log(`JobStatus: [pollJobStatus] Polling status for job ${jobId}...`); // LOGGING
    try {
        const data = await apiService.getJobStatus(jobId);
        // IMPORTANT: Check if the job ID is still the current one *after* the API call returns
        if (jobStore.currentJobId === jobId) {
            console.log(`JobStatus: [pollJobStatus] Received status data for ${jobId}: Status=${data.status}, Progress=${data.progress}, Stage=${data.current_stage}`); // LOGGING
            jobStore.updateJobDetails(data); // Update the store
            errorMessage.value = null; // Clear previous errors on successful poll

            // Stop polling if job is completed or failed
            if (data.status === 'completed' || data.status === 'failed') {
                console.log(`JobStatus: [pollJobStatus] Job ${jobId} reached terminal state (${data.status}). Stopping polling.`); // LOGGING
                stopPolling();
            }
        } else {
             console.log(`JobStatus: [pollJobStatus] Job ID changed from ${jobId} to ${jobStore.currentJobId} during API call. Ignoring stale data.`); // LOGGING
             // Don't update the store with stale data
        }
    } catch (error: any) {
        console.error(`JobStatus: [pollJobStatus] Error polling status for ${jobId}:`, error); // LOGGING
        // Only set error if the failed poll was for the *current* job ID
        if (jobStore.currentJobId === jobId) {
            errorMessage.value = `Polling Error: ${error.message || 'Failed to fetch status.'}`;
        }
        // Consider retrying after a longer interval before stopping completely
        stopPolling(); // Stop polling on error for now
    } finally {
        // Only set isLoading to false if the poll was for the current job
        if (jobStore.currentJobId === jobId) {
            isLoading.value = false;
        }
    }
  };

  const startPolling = (jobId: string | null | undefined) => {
    if (!jobId) {
        console.log("JobStatus: [startPolling] Cannot start polling, no jobId provided."); // LOGGING
        return;
    }
    stopPolling(); // Ensure any existing polling is stopped first
    console.log(`JobStatus: [startPolling] Starting polling for job ${jobId}.`); // LOGGING
    pollJobStatus(jobId); // Poll immediately

    pollingIntervalId.value = window.setInterval(() => {
        console.log(`JobStatus: [setInterval] Checking poll condition for ${jobId}. Current store ID: ${jobStore.currentJobId}, Status: ${jobStore.jobDetails?.status}`); // LOGGING
        // Check condition inside interval as well
        if (jobStore.currentJobId === jobId && jobStore.jobDetails?.status !== 'completed' && jobStore.jobDetails?.status !== 'failed') {
            pollJobStatus(jobId);
        } else {
            console.log(`JobStatus: [setInterval] Condition not met, stopping polling.`); // LOGGING
            // Stop if job ID changed or job finished between polls
            stopPolling();
        }
    }, POLLING_INTERVAL);
  };

  const stopPolling = () => {
    if (pollingIntervalId.value !== null) {
        console.log(`JobStatus: [stopPolling] Stopping polling interval ID ${pollingIntervalId.value}.`); // LOGGING
        clearInterval(pollingIntervalId.value);
        pollingIntervalId.value = null;
    } else {
        // console.log(`JobStatus: [stopPolling] No active polling interval to stop.`); // LOGGING (Optional)
    }
  };

  const requestNotification = async () => {
     // Use jobDetails.id
     const currentId = jobDetails.value?.id;
     if (!currentId || !notificationEmail.value) return;
     isNotifyLoading.value = true;
     notifyMessage.value = null;
     notifyMessageIsError.value = false;
     console.log(`JobStatus: Requesting notification for ${currentId} to ${notificationEmail.value}`); // LOGGING
    try {
        const response = await apiService.requestNotification(currentId, notificationEmail.value);
        console.log(`JobStatus: Notification request successful:`, response); // LOGGING
        notifyMessage.value = response.message || 'Notification request sent!';
        notificationEmail.value = ''; // Clear input on success
    } catch (error: any) {
        console.error(`JobStatus: Notification request failed:`, error); // LOGGING
        notifyMessage.value = `Error: ${error.message || 'Failed to send request.'}`;
        notifyMessageIsError.value = true;
    } finally {
        isNotifyLoading.value = false;
        // Clear message after a delay
        setTimeout(() => { notifyMessage.value = null; }, 5000);
    }
  };

  const downloadResults = async () => {
     // Use jobDetails.id
     const currentId = jobDetails.value?.id;
     if (!currentId) return;
     isDownloadLoading.value = true;
     downloadError.value = null;
     console.log(`JobStatus: Attempting download for ${currentId}`); // LOGGING
    try {
        const { blob, filename } = await apiService.downloadResults(currentId);
        console.log(`JobStatus: Download blob received, filename: ${filename}`); // LOGGING
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
        console.log(`JobStatus: Download triggered for ${filename}`); // LOGGING
    } catch (error: any) {
        console.error(`JobStatus: Download failed:`, error); // LOGGING
        downloadError.value = `Download failed: ${error.message || 'Could not download results.'}`;
    } finally {
        isDownloadLoading.value = false;
    }
  };

  // --- Lifecycle Hooks ---
  onMounted(() => {
      console.log(`JobStatus: Mounted. Current job ID from store: ${jobStore.currentJobId}`); // LOGGING
      if (jobStore.currentJobId) {
          errorMessage.value = null;
          // Fetch initial details if not already present or if status is unknown/stale
          // Use jobDetails.id for comparison
          if (!jobDetails.value || jobDetails.value.id !== jobStore.currentJobId || (jobDetails.value.status !== 'completed' && jobDetails.value.status !== 'failed')) {
              console.log(`JobStatus: Fetching initial details or starting polling for ${jobStore.currentJobId}`); // LOGGING
              startPolling(jobStore.currentJobId);
          } else {
               console.log(`JobStatus: Job ${jobStore.currentJobId} already in terminal state (${jobDetails.value.status}), not polling.`); // LOGGING
          }
      }
  });

  onUnmounted(() => {
      console.log("JobStatus: Unmounted, stopping polling."); // LOGGING
      stopPolling();
  });

  // --- Watchers ---
  // Watch for changes in the store's currentJobId
  watch(() => jobStore.currentJobId, (newJobId: string | null | undefined) => {
      console.log(`JobStatus: Watched currentJobId changed to: ${newJobId}`); // LOGGING
      if (newJobId) {
          // Reset component state when job ID changes
          errorMessage.value = null;
          downloadError.value = null;
          notifyMessage.value = null;
          notificationEmail.value = '';
          isDownloadLoading.value = false;
          isNotifyLoading.value = false;

          // Fetch details or start polling if needed for the new job
          // Use jobDetails.id for comparison
          if (!jobStore.jobDetails || jobStore.jobDetails.id !== newJobId || (jobStore.jobDetails.status !== 'completed' && jobStore.jobDetails.status !== 'failed')) {
               console.log(`JobStatus: Starting polling due to job ID change to ${newJobId}`); // LOGGING
               startPolling(newJobId);
          } else {
               // If the new job is already completed/failed, don't poll
               console.log(`JobStatus: Job ${newJobId} already in terminal state (${jobStore.jobDetails.status}), not polling.`); // LOGGING
               stopPolling(); // Ensure polling is stopped
          }
      } else {
          // Job ID was cleared
          console.log("JobStatus: Job ID cleared, stopping polling."); // LOGGING
          stopPolling();
      }
  });

  // Watch for the job status changing to a terminal state
  watch(() => jobStore.jobDetails?.status, (newStatus: JobDetails['status'] | undefined) => {
      console.log(`JobStatus: Watched job status changed to: ${newStatus}`); // LOGGING
      if (newStatus === 'completed' || newStatus === 'failed') {
          console.log(`JobStatus: Job reached terminal state (${newStatus}), stopping polling.`); // LOGGING
          stopPolling();
      }
  });

  </script>
</file>

<file path='frontend/vue_frontend/src/components/LandingPage.vue'>
<template>
    <div>
      <!-- Hero Section -->
      <section id="hero" class="relative overflow-hidden bg-gradient-to-br from-primary via-blue-700 to-indigo-800 text-white pt-24 pb-20 md:pt-32 md:pb-28">
        <!-- Subtle overlay -->
        <div class="absolute inset-0 bg-black opacity-10"></div>
        <div class="relative max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 text-center z-10">
          <h1 class="text-4xl md:text-5xl lg:text-6xl font-bold tracking-tight mb-5">
            Stop Guessing. Start Classifying.
          </h1>
          <p class="text-lg md:text-xl text-indigo-100 mb-10 max-w-3xl mx-auto">
            Transform your messy vendor lists into actionable, NAICS-classified data in minutes, not weeks. Leverage AI for unmatched accuracy and gain deeper procurement insights.
          </p>
          <div class="flex flex-col sm:flex-row justify-center items-center space-y-4 sm:space-y-0 sm:space-x-4">
            <a href="#loginCardAnchor"
               @click.prevent="scrollToLogin"
               class="inline-block bg-white text-primary font-semibold py-3 px-8 rounded-md shadow-lg hover:bg-gray-100 transition duration-300 text-lg transform hover:scale-105">
              Get Started Now
            </a>
             <!-- Optional Secondary CTA -->
             <!--
             <a href="#features" @click.prevent="scrollToFeatures"
               class="inline-block text-white font-medium py-3 px-8 rounded-md hover:bg-white/10 transition duration-300">
              Learn More
            </a>
            -->
          </div>
        </div>
         <!-- Optional: Subtle background pattern or graphic -->
         <!-- <div class="absolute bottom-0 left-0 w-full h-32 bg-gradient-to-t from-light to-transparent"></div> -->
      </section>
  
      <!-- Logos / Social Proof (Placeholder) -->
      <section class="py-8 bg-gray-50">
          <div class="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8">
              <p class="text-center text-sm font-semibold text-gray-500 tracking-wider uppercase">
                  Trusted by leading procurement teams
              </p>
              <div class="mt-6 grid grid-cols-2 gap-4 md:grid-cols-4 lg:grid-cols-5 items-center">
                  <!-- Replace with actual logos -->
                  <div class="flex justify-center col-span-1"> <span class="text-gray-400 text-2xl italic">Logo Inc</span> </div>
                  <div class="flex justify-center col-span-1"> <span class="text-gray-400 text-2xl italic">VendorBase</span> </div>
                  <div class="flex justify-center col-span-1"> <span class="text-gray-400 text-2xl italic">SupplyCo</span> </div>
                  <div class="flex justify-center col-span-1"> <span class="text-gray-400 text-2xl italic">Analytics Ltd</span> </div>
                  <div class="flex justify-center col-span-1 hidden lg:flex"> <span class="text-gray-400 text-2xl italic">Insight Corp</span> </div>
              </div>
          </div>
      </section>
  
      <!-- Problem / Solution Section -->
      <section id="problem-solution" class="py-16 md:py-20">
          <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 grid md:grid-cols-2 gap-12 items-center">
              <!-- Problem -->
              <div class="text-center md:text-left">
                   <span class="inline-block bg-red-100 text-red-700 text-xs font-semibold px-3 py-1 rounded-full mb-3">The Challenge</span>
                   <h2 class="text-3xl font-semibold mb-4">Manual Classification is Broken</h2>
                   <p class="text-gray-600 mb-6">
                       Spending hours manually assigning NAICS codes? Dealing with inconsistent data, costly errors, and missed insights? Your team's valuable time is wasted on tedious tasks instead of strategic sourcing.
                   </p>
                   <ul class="space-y-2 text-left text-gray-600">
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-red-500 mr-2 mt-0.5 flex-shrink-0" /><span>Time-consuming & resource-intensive</span></li>
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-red-500 mr-2 mt-0.5 flex-shrink-0" /><span>Prone to inconsistencies and errors</span></li>
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-red-500 mr-2 mt-0.5 flex-shrink-0" /><span>Leads to unreliable reporting & analytics</span></li>
                   </ul>
              </div>
               <!-- Solution -->
               <div class="text-center md:text-left bg-gradient-to-br from-blue-50 to-primary/10 p-8 rounded-lg shadow-lg border border-primary/20">
                   <span class="inline-block bg-green-100 text-green-700 text-xs font-semibold px-3 py-1 rounded-full mb-3">The Solution</span>
                   <h2 class="text-3xl font-semibold text-primary mb-4">AI-Powered Automation</h2>
                   <p class="text-gray-700 mb-6">
                       NAICS Classify uses cutting-edge AI to analyze your vendor data, perform intelligent web searches, and deliver accurate, multi-level NAICS codes automatically. Reclaim your time and unlock data potential.
                   </p>
                    <ul class="space-y-2 text-left text-gray-700">
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-green-500 mr-2 mt-0.5 flex-shrink-0" /><span>Classify thousands of vendors in minutes</span></li>
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-green-500 mr-2 mt-0.5 flex-shrink-0" /><span>Achieve >95% accuracy with AI & search</span></li>
                       <li class="flex items-start"><CheckCircleIcon class="h-5 w-5 text-green-500 mr-2 mt-0.5 flex-shrink-0" /><span>Get consistent, reliable data for insights</span></li>
                   </ul>
              </div>
          </div>
      </section>
  
      <!-- How It Works (Refined Visually) -->
      <section id="how-it-works" class="py-16 bg-gray-50">
         <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <h2 class="text-3xl font-semibold text-center mb-12">Get Results in 3 Simple Steps</h2>
            <div class="relative">
                <!-- Connecting Line (optional decorative element) -->
                 <div class="hidden md:block absolute top-1/2 left-0 w-full h-0.5 bg-gray-300 transform -translate-y-1/2 -z-1"></div>
  
                <div class="grid md:grid-cols-3 gap-8 relative">
                    <!-- Step 1 -->
                    <div class="bg-white p-6 rounded-lg shadow-lg border border-gray-200 text-center z-10">
                         <div class="relative inline-block">
                             <div class="inline-flex items-center justify-center w-16 h-16 mb-5 font-bold text-2xl text-white bg-primary rounded-full shadow-md">1</div>
                             <!-- Arrow pointing right (decorative) -->
                             <div class="hidden md:block absolute top-1/2 left-full ml-4 transform -translate-y-1/2 text-gray-300">
                                 <ArrowRightIcon class="h-8 w-8" />
                             </div>
                         </div>
                         <h4 class="text-lg font-semibold text-gray-800 mb-3">Upload Your List</h4>
                         <p class="text-sm text-gray-600">Drag & drop or select your Excel file (.xlsx/.xls) with a 'vendor_name' column. Add optional context columns for even better results.</p>
                    </div>
                     <!-- Step 2 -->
                     <div class="bg-white p-6 rounded-lg shadow-lg border border-gray-200 text-center z-10">
                        <div class="relative inline-block">
                             <div class="inline-flex items-center justify-center w-16 h-16 mb-5 font-bold text-2xl text-white bg-primary rounded-full shadow-md">2</div>
                              <!-- Arrow pointing right (decorative) -->
                             <div class="hidden md:block absolute top-1/2 left-full ml-4 transform -translate-y-1/2 text-gray-300">
                                 <ArrowRightIcon class="h-8 w-8" />
                             </div>
                         </div>
                        <h4 class="text-lg font-semibold text-gray-800 mb-3">AI Processing Magic</h4>
                        <p class="text-sm text-gray-600">Our intelligent engine analyzes names, utilizes context, performs web searches if needed, and assigns accurate NAICS codes.</p>
                     </div>
                     <!-- Step 3 -->
                     <div class="bg-white p-6 rounded-lg shadow-lg border border-gray-200 text-center z-10">
                         <div class="inline-flex items-center justify-center w-16 h-16 mb-5 font-bold text-2xl text-white bg-primary rounded-full shadow-md">3</div>
                        <h4 class="text-lg font-semibold text-gray-800 mb-3">Download Enriched Data</h4>
                        <p class="text-sm text-gray-600">Receive your original spreadsheet back, now enriched with precise Level 1-4 NAICS classifications, ready for analysis.</p>
                     </div>
                </div>
            </div>
         </div>
      </section>
  
      <!-- Features Section (Improved Layout) -->
      <section id="features" class="py-16 md:py-20">
          <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
              <h2 class="text-3xl font-semibold text-center mb-12">Powerful Features, Tangible Benefits</h2>
              <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-8">
                  <!-- Feature Card -->
                  <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <BeakerIcon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Unmatched AI Accuracy</h5>
                       </div>
                       <p class="text-sm text-gray-600">Leverages advanced LLMs trained on vast datasets for industry-leading classification precision.</p>
                  </div>
                  <!-- Feature Card -->
                   <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <MagnifyingGlassIcon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Intelligent Web Search</h5>
                       </div>
                       <p class="text-sm text-gray-600">Automatically researches ambiguous vendors online, filling data gaps and boosting classification rates.</p>
                  </div>
                  <!-- Feature Card -->
                   <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <Bars4Icon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Multi-Level Hierarchy</h5>
                       </div>
                       <p class="text-sm text-gray-600">Provides detailed NAICS codes up to Level 4, enabling granular spend analysis and reporting.</p>
                  </div>
                   <!-- Feature Card -->
                  <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <ArrowUpTrayIcon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Simple Excel Workflow</h5>
                       </div>
                       <p class="text-sm text-gray-600">Integrates seamlessly with your existing spreadsheets. Just upload, process, and download.</p>
                  </div>
                   <!-- Feature Card -->
                  <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <ClockIcon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Rapid Processing</h5>
                       </div>
                       <p class="text-sm text-gray-600">Asynchronous backend handles large files quickly, freeing up your team's time immediately.</p>
                  </div>
                   <!-- Feature Card -->
                  <div class="bg-white p-6 rounded-lg shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
                       <div class="flex items-center mb-3">
                           <LockClosedIcon class="h-7 w-7 text-primary mr-3" />
                           <h5 class="text-lg font-semibold">Secure & Confidential</h5>
                       </div>
                       <p class="text-sm text-gray-600">Your data is encrypted and handled with strict security protocols throughout the process.</p>
                  </div>
              </div>
          </div>
      </section>
  
       <!-- Testimonial Section (Placeholder) -->
      <section id="testimonials" class="py-16 bg-gradient-to-r from-gray-50 to-blue-50">
           <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
               <h2 class="text-3xl font-semibold mb-8">Don't Just Take Our Word For It</h2>
               <div class="relative p-8 bg-white rounded-lg shadow-xl border border-gray-200">
                   <!-- Quote Icon -->
                   <div class="absolute top-0 left-0 -mt-4 -ml-4 text-primary/20">
                       <svg class="h-16 w-16" fill="currentColor" viewBox="0 0 32 32">
                           <path d="M9.333 7h-2.667c-1.835 0-3.333 1.498-3.333 3.333v11.333c0 1.835 1.498 3.333 3.333 3.333h6.667v-8h-4v-4c0-1.102.897-2 2-2h2v-4zM25.333 7h-2.667c-1.835 0-3.333 1.498-3.333 3.333v11.333c0 1.835 1.498 3.333 3.333 3.333h6.667v-8h-4v-4c0-1.102.897-2 2-2h2v-4z" />
                       </svg>
                   </div>
                  <blockquote class="relative italic text-gray-700 text-lg">
                     "NAICS Classify saved us countless hours. What used to take a week now takes minutes, and the accuracy is fantastic. It's essential for our vendor management."
                   </blockquote>
                   <footer class="mt-6">
                       <p class="font-semibold text-gray-900">Jane Doe</p>
                       <p class="text-sm text-gray-500">Procurement Manager, Tech Solutions Inc.</p>
                   </footer>
               </div>
           </div>
      </section>
  
  
      <!-- Login Card Section -->
      <section id="loginCardAnchor" class="py-16">
          <div class="max-w-lg mx-auto px-4">
             <Login @login-successful="handleLogin" />
          </div>
      </section>
    </div>
  </template>
  
  <script setup lang="ts">
  import Login from './Login.vue';
  // Import necessary icons from Heroicons (ensure @heroicons/vue is installed)
  import {
      CheckCircleIcon,
      ArrowRightIcon,
      BeakerIcon,
      MagnifyingGlassIcon,
      Bars4Icon,
      ArrowUpTrayIcon,
      ClockIcon,
      LockClosedIcon
  } from '@heroicons/vue/24/outline'; // Using outline style
  
  const emit = defineEmits(['login-successful']);
  
  const handleLogin = () => {
    emit('login-successful');
  };
  
  // Helper for scrolling smoothly to sections
  const smoothScrollTo = (elementId: string) => {
      const element = document.getElementById(elementId);
        if (element) {
            const navbarHeight = 70; // Estimate navbar height
            const elementPosition = element.getBoundingClientRect().top;
            const offsetPosition = elementPosition + window.pageYOffset - navbarHeight;
            window.scrollTo({ top: offsetPosition, behavior: 'smooth' });
        }
  }
  
  const scrollToLogin = (event: Event) => {
        event.preventDefault();
        smoothScrollTo('loginCardAnchor');
  }
  
  // Optional: Scroll to features
  // const scrollToFeatures = (event: Event) => {
  //       event.preventDefault();
  //       smoothScrollTo('features');
  // }
  
  </script>
  
  <style scoped>
   /* Optional: Add subtle background patterns or animations if desired */
   /* Example: Subtle pattern for hero */
   /* #hero::before {
       content: '';
       position: absolute;
       inset: 0;
       background-image: url('path/to/your/subtle-pattern.svg');
       background-repeat: repeat;
       opacity: 0.05;
       z-index: 0;
   } */
  
   /* Ensure high z-index for step numbers if using lines */
   #how-it-works .relative > .grid { /* Target the grid directly inside the relative div */
       position: relative; /* Ensure grid items can be positioned relative to this */
       z-index: 10; /* Place grid items above the line */
   }
   #how-it-works .relative > .absolute { /* Target the line */
      z-index: 1; /* Place line behind grid items */
   }
  </style>
</file>

<file path='frontend/vue_frontend/src/components/Login.vue'>
<template>
  <div class="bg-white rounded-lg shadow-md mb-5 overflow-hidden border border-gray-200">
    <div class="bg-primary text-white p-4">
      <h4 class="text-xl font-semibold mb-0 text-center">Login to Access Service</h4>
    </div>
    <div class="p-6">
      <form @submit.prevent="handleLogin">
        <div class="mb-4">
          <label for="username" class="block text-sm font-medium text-gray-700 mb-1">Username</label>
          <input
              type="text"
              class="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-50"
              id="username"
              v-model="username"
              required
              :disabled="isLoading"
              placeholder="admin"
          >
          <p class="mt-1 text-xs text-gray-500">Default: admin</p>
        </div>
        <div class="mb-6">
          <label for="password" class="block text-sm font-medium text-gray-700 mb-1">Password</label>
          <input
              type="password"
              class="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-50"
              id="password"
              v-model="password"
              required
              :disabled="isLoading"
              placeholder="password"
          >
           <p class="mt-1 text-xs text-gray-500">Default: password</p>
        </div>
        <button type="submit" class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-primary hover:bg-primary-hover focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed" :disabled="isLoading">
           <svg v-if="isLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
              <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
              <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
           </svg>
           {{ isLoading ? ' Logging in...' : 'Login' }}
        </button>
         <!-- Tailwind Alert -->
        <div v-if="errorMessage" class="mt-4 p-3 bg-red-100 border border-red-300 text-red-700 rounded-md text-center text-sm">{{ errorMessage }}</div>
      </form>
    </div>
  </div>
</template>

<script setup lang="ts">
   // ... script remains the same ...
   import { ref } from 'vue';
   import { useAuthStore } from '@/stores/auth';

   const username = ref('admin');
   const password = ref('password');
   const isLoading = ref(false);
   const errorMessage = ref<string | null>(null);

   const authStore = useAuthStore();
   const emit = defineEmits(['login-successful']);

   const handleLogin = async () => {
     isLoading.value = true;
     errorMessage.value = null;
     try {
       await authStore.login(username.value, password.value);
       console.log('Login component: Login successful.');
       emit('login-successful');
     } catch (error: any) {
       console.error('Login component error:', error);
       errorMessage.value = error.message || 'An unexpected error occurred during login.';
     } finally {
       isLoading.value = false;
     }
   };
</script>

<style scoped>
 /* No scoped styles needed */
</style>
</file>

<file path='frontend/vue_frontend/src/components/Navbar.vue'>

<template>
  <nav class="bg-primary shadow-md fixed top-0 left-0 right-0 z-50">
  <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="flex items-center justify-between h-16">
      <!-- Branding -->
      <div class="flex-shrink-0">
          <a class="text-white text-xl font-bold flex items-center cursor-pointer" @click="goHome">
              <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" class="h-6 w-6 mr-2" viewBox="0 0 16 16">
              <path fill-rule="evenodd" d="M6 3.5A1.5 1.5 0 0 1 7.5 2h1A1.5 1.5 0 0 1 10 3.5v1A1.5 1.5 0 0 1 8.5 6v1H14a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-1 0V8h-5v.5a.5.5 0 0 1-1 0V8h-5v.5a.5.5 0 0 1-1 0v-1A.5.5 0 0 1 2 7h5.5V6A1.5 1.5 0 0 1 6 4.5zM8.5 7H14v1h-5.5zM2 8h5.5v1H2zm9.5 4.5a1.5 1.5 0 0 0-1.5-1.5h-1a1.5 1.5 0 0 0-1.5 1.5v1a1.5 1.5 0 0 0 1.5 1.5h1a1.5 1.5 0 0 0 1.5-1.5zm-1.5 2.5a.5.5 0 0 1-.5.5h-1a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 .5.5zM2 12.5a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-1a.5.5 0 0 1-.5-.5zM11 12.5a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-1a.5.5 0 0 1-.5-.5z"/>
              </svg>
          NAICS Classify
          </a>
      </div>

      <!-- User Info / Admin / Logout Section -->
      <div v-if="authStore.isAuthenticated" class="flex items-center space-x-4">
          <!-- Admin Link (Conditional) -->
          <button
          v-if="authStore.isSuperuser"
          @click="toggleAdminView"
          :class="[
              'px-3 py-1 text-sm font-medium rounded-md focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-primary focus:ring-white',
              isAdminViewActive ? 'bg-indigo-700 text-white' : 'text-gray-200 hover:bg-indigo-500 hover:text-white'
          ]"
          >
          Admin Panel
          </button>

          <!-- Welcome Message -->
          <span class="text-gray-200 text-sm hidden sm:inline">
          Welcome, <span class="font-semibold text-white">{{ authStore.username || 'User' }}</span>
          </span>

          <!-- Logout Button -->
          <button
          @click="emitLogout"
          class="px-3 py-1 border border-transparent text-sm font-medium rounded-md text-primary bg-white hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-primary focus:ring-white"
          >
          Logout
          </button>
      </div>
      <!-- Optional Login button if needed -->
      </div>
  </div>
  </nav>
</template>

<script setup lang="ts">
import { useAuthStore } from '@/stores/auth';
import { useViewStore } from '@/stores/view'; // Assuming a view store exists
import { computed } from 'vue';

const authStore = useAuthStore();
const viewStore = useViewStore(); // Use the view store

const emit = defineEmits(['logout']);
const emitLogout = () => emit('logout');

// Computed property to check if the admin view is currently active
const isAdminViewActive = computed(() => viewStore.currentView === 'admin');

const toggleAdminView = () => {
  if (viewStore.currentView === 'admin') {
      viewStore.setView('app'); // Switch back to the main app view
  } else {
      viewStore.setView('admin'); // Switch to the admin view
  }
};

const goHome = () => {
  // If logged in, go to app view, otherwise landing page (handled by App.vue)
  if (authStore.isAuthenticated) {
      viewStore.setView('app');
  }
  // If not logged in, clicking the brand might implicitly take them "home"
  // which is the landing page in the current App.vue setup.
  // If using router, this would be router.push('/');
};
</script>
</file>

<file path='frontend/vue_frontend/src/components/UploadForm.vue'>

<template>
  <div class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
    <div class="bg-primary text-white p-4 sm:p-5">
      <h4 class="text-xl font-semibold mb-0">Upload Vendor File</h4>
    </div>
    <div class="p-6 sm:p-8">
      <!-- Success Alert -->
      <div v-if="successMessage" class="mb-5 p-3 bg-green-100 border border-green-300 text-green-800 rounded-md text-sm flex items-center">
          <CheckCircleIcon class="h-5 w-5 mr-2 text-green-600 flex-shrink-0"/>
          <span>{{ successMessage }}</span>
      </div>
       <!-- Error Alert -->
      <div v-if="errorMessage" class="mb-5 p-3 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
           <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
          <span>{{ errorMessage }}</span>
      </div>

      <form @submit.prevent="handleUpload" enctype="multipart/form-data">
        <div class="mb-5">
          <label for="companyName" class="block text-sm font-medium text-gray-700 mb-1.5">
              Company Name <span class="text-red-500">*</span>
          </label>
          <input
            type="text"
            class="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100 disabled:cursor-not-allowed"
            id="companyName"
            v-model="companyName"
            required
            :disabled="isLoading"
            placeholder="e.g., Your Company Inc."
          />
        </div>

        <!-- ADDED: Target Level Selection -->
        <div class="mb-5">
          <label for="targetLevel" class="block text-sm font-medium text-gray-700 mb-1.5">
              Target Classification Level <span class="text-red-500">*</span>
          </label>
          <select
            id="targetLevel"
            v-model.number="selectedLevel"
            required
            :disabled="isLoading"
            class="block w-full px-3 py-2 border border-gray-300 bg-white rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:opacity-60 disabled:bg-gray-100 disabled:cursor-not-allowed"
          >
            <option value="1">Level 1 (Sector)</option>
            <option value="2">Level 2 (Subsector)</option>
            <option value="3">Level 3 (Industry Group)</option>
            <option value="4">Level 4 (NAICS Industry)</option>
            <option value="5">Level 5 (National Industry)</option>
          </select>
          <p class="mt-1 text-xs text-gray-500">Select the maximum NAICS level you want the classification to reach.</p>
        </div>
        <!-- END ADDED -->

        <div class="mb-6">
          <label for="vendorFile" class="block text-sm font-medium text-gray-700 mb-1.5">
              Vendor Excel File <span class="text-red-500">*</span>
          </label>
          <input
            type="file"
            class="block w-full text-sm text-gray-500 border border-gray-300 rounded-md cursor-pointer bg-gray-50 focus:outline-none focus:ring-primary focus:border-primary file:mr-4 file:py-2 file:px-4 file:rounded-l-md file:border-0 file:text-sm file:font-semibold file:bg-gray-100 file:text-gray-700 hover:file:bg-gray-200 disabled:opacity-60 disabled:cursor-not-allowed"
            id="vendorFile"
            ref="fileInputRef"
            @change="handleFileChange"
            accept=".xlsx,.xls"
            required
            :disabled="isLoading"
          />
          <p class="mt-2 text-xs text-gray-500">
              Requires '.xlsx' or '.xls'. Must contain 'vendor_name' column.
              <br/>Optional context columns enhance accuracy (address, website, example, etc.).
          </p>
        </div>
        <button type="submit" class="w-full flex justify-center items-center px-4 py-2.5 border border-transparent rounded-md shadow-sm text-base font-medium text-white bg-primary hover:bg-primary-hover focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary disabled:opacity-50 disabled:cursor-not-allowed" :disabled="isLoading || !selectedFile">
           <svg v-if="isLoading" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
              <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
              <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
           </svg>
           <ArrowUpTrayIcon v-else class="h-5 w-5 mr-2 -ml-1" />
          {{ isLoading ? ' Uploading & Processing...' : 'Upload and Process' }}
        </button>
      </form>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref } from 'vue';
import apiService from '@/services/api';
import { useJobStore } from '@/stores/job';
import { ArrowUpTrayIcon, CheckCircleIcon, ExclamationTriangleIcon } from '@heroicons/vue/20/solid'; // Using solid icons

const jobStore = useJobStore();
const companyName = ref('');
const selectedFile = ref<File | null>(null);
const fileInputRef = ref<HTMLInputElement | null>(null);
const isLoading = ref(false);
const successMessage = ref<string | null>(null);
const errorMessage = ref<string | null>(null);
// --- ADDED: State for selected level ---
const selectedLevel = ref<number>(5); // Default to Level 5
// --- END ADDED ---

const emit = defineEmits(['upload-successful']);

const handleFileChange = (event: Event) => {
  const target = event.target as HTMLInputElement;
  if (target.files && target.files.length > 0) {
    selectedFile.value = target.files[0];
    errorMessage.value = null;
    successMessage.value = null;
  } else {
    selectedFile.value = null;
  }
};

const handleUpload = async () => {
  if (!selectedFile.value || !companyName.value) {
    errorMessage.value = 'Please provide a company name and select a file.';
    successMessage.value = null;
    return;
  }
  // --- ADDED: Validate selected level ---
  if (selectedLevel.value < 1 || selectedLevel.value > 5) {
    errorMessage.value = 'Please select a valid target level (1-5).';
    successMessage.value = null;
    return;
  }
  // --- END ADDED ---

  isLoading.value = true;
  successMessage.value = null;
  errorMessage.value = null;
  jobStore.clearJob();
  const formData = new FormData();
  formData.append('company_name', companyName.value);
  formData.append('file', selectedFile.value);
  // --- ADDED: Append selected level to form data ---
  formData.append('target_level', selectedLevel.value.toString());
  // --- END ADDED ---

  try {
    const response = await apiService.uploadFile(formData); // apiService.uploadFile now handles FormData directly
    successMessage.value = `Upload successful! Job ${response.job_id} started (Target Level: ${selectedLevel.value}). Monitoring status below...`;
    emit('upload-successful', response.job_id);
    // Reset form fields
    companyName.value = '';
    selectedFile.value = null;
    selectedLevel.value = 5; // Reset level to default
    if (fileInputRef.value) {
        fileInputRef.value.value = '';
    }
  } catch (error: any) {
    errorMessage.value = error.message || 'An unexpected error occurred during upload.';
    successMessage.value = null;
  } finally {
    isLoading.value = false;
  }
};
</script>

<style scoped>
/* Style the file input button more effectively */
input[type="file"]::file-selector-button {
    /* Tailwind handles most, but you can add custom tweaks */
    cursor: pointer;
}
</style>
</file>

<file path='frontend/vue_frontend/src/components/UserFormModal.vue'>
<template>
  <TransitionRoot appear :show="show" as="template">
    <Dialog as="div" @close="closeModal" class="relative z-50">
      <!-- Backdrop -->
      <TransitionChild
        as="template"
        enter="duration-300 ease-out"
        enter-from="opacity-0"
        enter-to="opacity-100"
        leave="duration-200 ease-in"
        leave-from="opacity-100"
        leave-to="opacity-0"
      >
        <div class="fixed inset-0 bg-black/30 backdrop-blur-sm" aria-hidden="true" />
      </TransitionChild>

      <!-- Full-screen container to center the panel -->
      <div class="fixed inset-0 overflow-y-auto">
        <div class="flex min-h-full items-center justify-center p-4 text-center">
          <!-- Modal Panel -->
          <TransitionChild
            as="template"
            enter="duration-300 ease-out"
            enter-from="opacity-0 scale-95"
            enter-to="opacity-100 scale-100"
            leave="duration-200 ease-in"
            leave-from="opacity-100 scale-100"
            leave-to="opacity-0 scale-95"
          >
            <DialogPanel class="w-full max-w-lg transform overflow-hidden rounded-lg bg-white p-6 text-left align-middle shadow-xl transition-all">
              <DialogTitle as="h3" class="text-xl font-semibold leading-6 text-gray-800 border-b border-gray-200 pb-4 mb-5">
                {{ isEditing ? 'Edit User' : 'Create New User' }}
              </DialogTitle>

              <!-- Form -->
              <form @submit.prevent="submitForm" class="space-y-5">
                 <!-- Error Message within Modal -->
                 <div v-if="formError" class="p-3 bg-red-100 border border-red-300 text-red-700 rounded-md text-sm flex items-center space-x-2">
                    <ExclamationTriangleIcon class="h-5 w-5 text-red-600 flex-shrink-0"/>
                    <span>{{ formError }}</span>
                 </div>

                 <!-- Username (Readonly on Edit) -->
                 <div>
                   <label for="username" class="block text-sm font-medium text-gray-700 mb-1">Username <span class="text-red-500">*</span></label>
                   <input
                     type="text"
                     v-model="formData.username"
                     id="username"
                     required
                     class="mt-1 block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm disabled:bg-gray-100 disabled:text-gray-500 disabled:cursor-not-allowed"
                     :disabled="isEditing"
                     placeholder="e.g., jsmith"
                   >
                    <p v-if="isEditing" class="text-xs text-gray-500 mt-1">Username cannot be changed after creation.</p>
                 </div>

                 <!-- Email -->
                 <div>
                   <label for="email" class="block text-sm font-medium text-gray-700 mb-1">Email <span class="text-red-500">*</span></label>
                   <input
                     type="email"
                     v-model="formData.email"
                     id="email"
                     required
                     class="mt-1 block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm"
                     placeholder="e.g., j.smith@example.com"
                   >
                 </div>

                 <!-- Full Name -->
                 <div>
                   <label for="full_name" class="block text-sm font-medium text-gray-700 mb-1">Full Name</label>
                   <input
                     type="text"
                     v-model="formData.full_name"
                     id="full_name"
                     class="mt-1 block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm"
                     placeholder="e.g., John Smith"
                    >
                 </div>

                 <!-- Password -->
                  <div>
                   <label for="password" class="block text-sm font-medium text-gray-700 mb-1">
                       Password {{ isEditing ? '(Leave blank to keep unchanged)' : '' }}
                       <span v-if="!isEditing" class="text-red-500">*</span>
                    </label>
                   <input
                     type="password"
                     v-model="formData.password"
                     id="password"
                     :required="!isEditing"
                     minlength="8"
                     class="mt-1 block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-primary focus:border-primary sm:text-sm"
                     placeholder="Min. 8 characters"
                    >
                    <p v-if="isEditing && formData.password && formData.password.length < 8" class="text-xs text-red-500 mt-1">Password must be at least 8 characters if changing.</p>
                 </div>

                 <!-- Status Toggles -->
                 <div class="flex items-center space-x-8 pt-2">
                    <div class="flex items-center">
                       <Switch
                         :modelValue="Boolean(formData.is_active)" @update:modelValue="formData.is_active = $event"
                         :class="formData.is_active ? 'bg-primary' : 'bg-gray-200'"
                         class="relative inline-flex h-6 w-11 flex-shrink-0 cursor-pointer rounded-full border-2 border-transparent transition-colors duration-200 ease-in-out focus:outline-none focus:ring-2 focus:ring-primary focus:ring-offset-2"
                       >
                         <span class="sr-only">Active Status</span>
                         <span
                           aria-hidden="true"
                           :class="formData.is_active ? 'translate-x-5' : 'translate-x-0'"
                           class="pointer-events-none inline-block h-5 w-5 transform rounded-full bg-white shadow ring-0 transition duration-200 ease-in-out"
                         />
                       </Switch>
                       <label for="is_active_label" class="ml-3 block text-sm font-medium text-gray-700">Active</label>
                    </div>
                     <div class="flex items-center">
                       <Switch
                         :modelValue="Boolean(formData.is_superuser)" @update:modelValue="formData.is_superuser = $event"
                         :class="formData.is_superuser ? 'bg-indigo-600' : 'bg-gray-200'"
                         class="relative inline-flex h-6 w-11 flex-shrink-0 cursor-pointer rounded-full border-2 border-transparent transition-colors duration-200 ease-in-out focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2"
                       >
                         <span class="sr-only">Admin Status</span>
                         <span
                           aria-hidden="true"
                           :class="formData.is_superuser ? 'translate-x-5' : 'translate-x-0'"
                           class="pointer-events-none inline-block h-5 w-5 transform rounded-full bg-white shadow ring-0 transition duration-200 ease-in-out"
                         />
                       </Switch>
                       <label for="is_superuser_label" class="ml-3 block text-sm font-medium text-gray-700">Admin</label>
                    </div>
                 </div>

                <!-- Action Buttons -->
                <div class="mt-6 flex justify-end space-x-3 border-t border-gray-200 pt-5">
                  <button
                    type="button"
                    class="inline-flex justify-center rounded-md border border-gray-300 bg-white px-4 py-2 text-sm font-medium text-gray-700 shadow-sm hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-primary focus:ring-offset-2"
                    @click="closeModal"
                    :disabled="isSubmitting"
                  >
                    Cancel
                  </button>
                  <button
                    type="submit"
                    class="inline-flex justify-center items-center rounded-md border border-transparent bg-primary px-4 py-2 text-sm font-medium text-white shadow-sm hover:bg-primary-hover focus:outline-none focus:ring-2 focus:ring-primary focus:ring-offset-2 disabled:opacity-60"
                    :disabled="isSubmitting"
                  >
                     <svg v-if="isSubmitting" class="animate-spin -ml-1 mr-2 h-4 w-4 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                         <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                         <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                     </svg>
                    {{ isSubmitting ? 'Saving...' : (isEditing ? 'Update User' : 'Create User') }}
                  </button>
                </div>
              </form>
            </DialogPanel>
          </TransitionChild>
        </div>
      </div>
    </Dialog>
  </TransitionRoot>
</template>

<script setup lang="ts">
import { ref, watch, computed } from 'vue';
import {
  Dialog,
  DialogPanel,
  DialogTitle,
  TransitionRoot,
  TransitionChild,
  Switch // Import Switch component
} from '@headlessui/vue';
import { ExclamationTriangleIcon } from '@heroicons/vue/24/outline'; // Icon for errors
import type { UserResponse, UserCreateData, UserUpdateData } from '@/services/api';

interface Props {
  show: boolean;
  userToEdit: UserResponse | null;
}

const props = defineProps<Props>();
const emit = defineEmits(['close', 'save']);

// --- Form Data ---
interface FormDataState {
    username: string;
    email: string;
    full_name: string | null;
    password?: string;
    is_active: boolean;
    is_superuser: boolean;
}

const defaultFormData: FormDataState = {
    username: '',
    email: '',
    full_name: null,
    password: '',
    is_active: true,
    is_superuser: false,
};

// Helper to initialize or reset form data
const initializeFormData = (user: UserResponse | null): FormDataState => {
    if (user) {
        return {
            username: user.username,
            email: user.email,
            full_name: user.full_name || null,
            password: '', // Always clear password field on open
            is_active: user.is_active ?? true,
            is_superuser: user.is_superuser ?? false,
        };
    } else {
        return { ...defaultFormData };
    }
};

const formData = ref<FormDataState>(initializeFormData(props.userToEdit));
const formError = ref<string | null>(null);
const isSubmitting = ref(false);

const isEditing = computed(() => !!props.userToEdit);

// --- Watcher to populate form when userToEdit prop changes ---
// --- REMOVED immediate: true ---
watch(() => props.userToEdit, (newUser) => {
    console.log("UserFormModal: Watcher triggered for userToEdit:", newUser ? newUser.username : 'null');
    formData.value = initializeFormData(newUser); // Use the helper function to reset/populate
    formError.value = null; // Clear error when user changes
    isSubmitting.value = false; // Reset submitting state
});

// Watcher to reset state when modal is closed (show becomes false)
watch(() => props.show, (newVal, oldVal) => {
    // Only reset when closing (transitioning from true to false)
    if (oldVal === true && newVal === false) {
        console.log("UserFormModal: Watcher triggered for show=false. Resetting form.");
        // Reset form data to default when modal closes
        formData.value = { ...defaultFormData };
        formError.value = null;
        isSubmitting.value = false;
    }
    // Optionally populate when opening if needed, though the userToEdit watcher handles it
    // if (oldVal === false && newVal === true) {
    //    formData.value = initializeFormData(props.userToEdit);
    // }
});


const closeModal = () => {
  if (isSubmitting.value) return; // Prevent closing while submitting
  emit('close');
};

const submitForm = async () => {
    formError.value = null; // Clear previous errors
    isSubmitting.value = true;

    // Basic Frontend Validation
    if (!formData.value.username.trim()) {
        formError.value = "Username is required.";
        isSubmitting.value = false;
        return;
    }
    if (!formData.value.email.trim() || !/\S+@\S+\.\S+/.test(formData.value.email)) {
        formError.value = "A valid email address is required.";
        isSubmitting.value = false;
        return;
    }

    // Prepare data based on create or edit
    let dataToSend: UserCreateData | UserUpdateData;
    if (isEditing.value) {
        const updateData: UserUpdateData = {
            email: formData.value.email,
            full_name: formData.value.full_name?.trim() || null,
            is_active: formData.value.is_active,
            is_superuser: formData.value.is_superuser,
        };
        if (formData.value.password && formData.value.password.length >= 8) {
            updateData.password = formData.value.password;
        } else if (formData.value.password && formData.value.password.length > 0) {
             formError.value = "Password must be at least 8 characters long if changing.";
             isSubmitting.value = false;
             return;
        }
        dataToSend = updateData;
    } else {
        if (!formData.value.password || formData.value.password.length < 8) {
             formError.value = "Password is required and must be at least 8 characters long.";
             isSubmitting.value = false;
             return;
        }
        const createData: UserCreateData = {
            username: formData.value.username.trim(),
            email: formData.value.email.trim(),
            full_name: formData.value.full_name?.trim() || null,
            password: formData.value.password,
            is_active: formData.value.is_active,
            is_superuser: formData.value.is_superuser,
        };
         dataToSend = createData;
    }

    try {
        // Emit the save event - parent handles API call & closing/resetting isSubmitting
        await emit('save', dataToSend);
    } catch (err: any) {
        // If the parent re-throws the error after handling it
        console.error("Error during form submission (caught in modal):", err);
        formError.value = err.message || "Failed to save user.";
        isSubmitting.value = false; // Ensure button is re-enabled on error
    }
    // Do NOT set isSubmitting to false here if emit was successful, parent controls it.
};

</script>

<style scoped>
/* Add custom styles if needed, though Tailwind should cover most */
</style>
</file>

<file path='frontend/vue_frontend/src/components/UserManagement.vue'>
<template>
    <div class="bg-white rounded-lg shadow-lg overflow-hidden border border-gray-200">
    <div class="bg-gray-100 text-gray-800 p-4 sm:p-5 border-b border-gray-200 flex justify-between items-center">
        <h4 class="text-xl font-semibold mb-0">User Management</h4>
        <button
        @click="openCreateModal"
        class="inline-flex items-center px-4 py-2 border border-transparent rounded-md shadow-sm text-sm font-medium text-white bg-primary hover:bg-primary-hover focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary"
        >
        <PlusIcon class="h-5 w-5 mr-2 -ml-1" />
        Create User
        </button>
    </div>

    <div class="p-6 sm:p-8">
        <!-- Loading State -->
        <div v-if="isLoading" class="text-center text-gray-500 py-8">
        <svg class="animate-spin inline-block h-6 w-6 text-primary mr-2" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
        </svg>
        <span>Loading users...</span>
        </div>

        <!-- Error State -->
        <div v-else-if="error" class="p-4 bg-red-100 border border-red-300 text-red-800 rounded-md text-sm flex items-center">
        <ExclamationTriangleIcon class="h-5 w-5 mr-2 text-red-600 flex-shrink-0"/>
        <span>Error loading users: {{ error }}</span>
        </div>

        <!-- Empty State -->
        <div v-else-if="!users || users.length === 0" class="text-center text-gray-500 py-8">
        <p>No users found.</p>
        <p class="text-sm">Click 'Create User' to add the first user.</p>
        </div>

        <!-- User Table -->
        <div v-else class="overflow-x-auto">
        <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
            <tr>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Username</th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Email</th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Full Name</th>
                <th scope="col" class="px-4 py-3 text-center text-xs font-medium text-gray-500 uppercase tracking-wider">Active</th>
                <th scope="col" class="px-4 py-3 text-center text-xs font-medium text-gray-500 uppercase tracking-wider">Admin</th>
                <th scope="col" class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Created</th>
                <th scope="col" class="px-4 py-3 text-right text-xs font-medium text-gray-500 uppercase tracking-wider">Actions</th>
            </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
            <tr v-for="user in users" :key="user.id" class="hover:bg-gray-50">
                <td class="px-4 py-3 whitespace-nowrap text-sm font-medium text-gray-900">{{ user.username }}</td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-600">{{ user.email }}</td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-600">{{ user.full_name || '-' }}</td>
                <td class="px-4 py-3 whitespace-nowrap text-center">
                <span :class="user.is_active ? 'text-green-600' : 'text-red-600'">
                    <CheckCircleIcon v-if="user.is_active" class="h-5 w-5 inline-block" />
                    <XCircleIcon v-else class="h-5 w-5 inline-block" />
                </span>
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-center">
                    <span :class="user.is_superuser ? 'text-indigo-600' : 'text-gray-400'">
                    <ShieldCheckIcon v-if="user.is_superuser" class="h-5 w-5 inline-block" />
                    <ShieldExclamationIcon v-else class="h-5 w-5 inline-block" />
                    </span>
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-sm text-gray-500">
                    {{ formatDateTime(user.created_at) }}
                </td>
                <td class="px-4 py-3 whitespace-nowrap text-right text-sm font-medium space-x-2">
                <button @click="openEditModal(user)" class="text-indigo-600 hover:text-indigo-800" title="Edit User">
                    <PencilSquareIcon class="h-5 w-5 inline-block" />
                </button>
                <button
                    @click="confirmDelete(user)"
                    :disabled="user.username === authStore.username"
                    class="text-red-600 hover:text-red-800 disabled:opacity-50 disabled:cursor-not-allowed"
                    title="Delete User"
                >
                    <TrashIcon class="h-5 w-5 inline-block" />
                </button>
                </td>
            </tr>
            </tbody>
        </table>
        </div>
        <!-- TODO: Add Pagination Controls -->
    </div>

    <!-- Create/Edit User Modal -->
    <UserFormModal
        :show="showModal"
        :user-to-edit="userToEdit"
        @close="closeModal"
        @save="handleSaveUser"
    />

    </div>
</template>

<script setup lang="ts">
import { ref, onMounted } from 'vue';
import apiService, { type UserResponse, type UserCreateData, type UserUpdateData } from '@/services/api';
import { useAuthStore } from '@/stores/auth';
import UserFormModal from './UserFormModal.vue'; // Assume this component exists
import {
    PlusIcon, PencilSquareIcon, TrashIcon, CheckCircleIcon, XCircleIcon,
    ExclamationTriangleIcon, ShieldCheckIcon, ShieldExclamationIcon
} from '@heroicons/vue/24/outline'; // Use outline icons

const authStore = useAuthStore();
const users = ref<UserResponse[]>([]);
const isLoading = ref(false);
const error = ref<string | null>(null);
const showModal = ref(false);
const userToEdit = ref<UserResponse | null>(null);

const fetchUsers = async () => {
    isLoading.value = true;
    error.value = null;
    try {
    users.value = await apiService.getUsers();
    } catch (err: any) {
    error.value = err.message || 'Failed to load users.';
    } finally {
    isLoading.value = false;
    }
};

const openCreateModal = () => {
    userToEdit.value = null;
    showModal.value = true;
};

const openEditModal = (user: UserResponse) => {
    userToEdit.value = { ...user }; // Clone user data
    showModal.value = true;
};

const closeModal = () => {
    showModal.value = false;
    userToEdit.value = null;
};

const handleSaveUser = async (userData: UserCreateData | UserUpdateData) => {
    isLoading.value = true; // Consider a different loading state for modal actions
    error.value = null; // Clear main table error
    try {
        if (userToEdit.value) {
            // Update user
            await apiService.updateUser(userToEdit.value.id, userData as UserUpdateData);
        } else {
            // Create user
            await apiService.createUser(userData as UserCreateData);
        }
        closeModal();
        await fetchUsers(); // Refresh the user list
    } catch (err: any) {
        // Handle error (maybe display in modal or globally)
        console.error("Failed to save user:", err);
        // For now, log it, ideally show in modal
        alert(`Error saving user: ${err.message}`);
        // error.value = `Error saving user: ${err.message}`;
    } finally {
            isLoading.value = false;
    }
};

const confirmDelete = async (user: UserResponse) => {
    if (user.username === authStore.username) {
    alert("You cannot delete your own account.");
    return;
    }
    if (confirm(`Are you sure you want to delete user "${user.username}" (${user.email})? This action cannot be undone.`)) {
    isLoading.value = true; // Use main loading indicator for now
    error.value = null;
    try {
        await apiService.deleteUser(user.id);
        await fetchUsers(); // Refresh list
    } catch (err: any) {
        error.value = `Failed to delete user: ${err.message}`;
    } finally {
        isLoading.value = false;
    }
    }
};

    const formatDateTime = (isoString: string | null | undefined): string => {
    if (!isoString) return 'N/A';
    try {
        return new Date(isoString).toLocaleDateString(undefined, {
            year: 'numeric', month: 'short', day: 'numeric'
        });
    } catch { return 'Invalid Date'; }
};


onMounted(() => {
    fetchUsers();
});
</script>
</file>

<file path='frontend/vue_frontend/src/components/icons/IconCommunity.vue'>
<template>
  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor">
    <path
      d="M15 4a1 1 0 1 0 0 2V4zm0 11v-1a1 1 0 0 0-1 1h1zm0 4l-.707.707A1 1 0 0 0 16 19h-1zm-4-4l.707-.707A1 1 0 0 0 11 14v1zm-4.707-1.293a1 1 0 0 0-1.414 1.414l1.414-1.414zm-.707.707l-.707-.707.707.707zM9 11v-1a1 1 0 0 0-.707.293L9 11zm-4 0h1a1 1 0 0 0-1-1v1zm0 4H4a1 1 0 0 0 1.707.707L5 15zm10-9h2V4h-2v2zm2 0a1 1 0 0 1 1 1h2a3 3 0 0 0-3-3v2zm1 1v6h2V7h-2zm0 6a1 1 0 0 1-1 1v2a3 3 0 0 0 3-3h-2zm-1 1h-2v2h2v-2zm-3 1v4h2v-4h-2zm1.707 3.293l-4-4-1.414 1.414 4 4 1.414-1.414zM11 14H7v2h4v-2zm-4 0c-.276 0-.525-.111-.707-.293l-1.414 1.414C5.42 15.663 6.172 16 7 16v-2zm-.707 1.121l3.414-3.414-1.414-1.414-3.414 3.414 1.414 1.414zM9 12h4v-2H9v2zm4 0a3 3 0 0 0 3-3h-2a1 1 0 0 1-1 1v2zm3-3V3h-2v6h2zm0-6a3 3 0 0 0-3-3v2a1 1 0 0 1 1 1h2zm-3-3H3v2h10V0zM3 0a3 3 0 0 0-3 3h2a1 1 0 0 1 1-1V0zM0 3v6h2V3H0zm0 6a3 3 0 0 0 3 3v-2a1 1 0 0 1-1-1H0zm3 3h2v-2H3v2zm1-1v4h2v-4H4zm1.707 4.707l.586-.586-1.414-1.414-.586.586 1.414 1.414z"
    />
  </svg>
</template>

</file>

<file path='frontend/vue_frontend/src/components/icons/IconDocumentation.vue'>
<template>
  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="17" fill="currentColor">
    <path
      d="M11 2.253a1 1 0 1 0-2 0h2zm-2 13a1 1 0 1 0 2 0H9zm.447-12.167a1 1 0 1 0 1.107-1.666L9.447 3.086zM1 2.253L.447 1.42A1 1 0 0 0 0 2.253h1zm0 13H0a1 1 0 0 0 1.553.833L1 15.253zm8.447.833a1 1 0 1 0 1.107-1.666l-1.107 1.666zm0-14.666a1 1 0 1 0 1.107 1.666L9.447 1.42zM19 2.253h1a1 1 0 0 0-.447-.833L19 2.253zm0 13l-.553.833A1 1 0 0 0 20 15.253h-1zm-9.553-.833a1 1 0 1 0 1.107 1.666L9.447 14.42zM9 2.253v13h2v-13H9zm1.553-.833C9.203.523 7.42 0 5.5 0v2c1.572 0 2.961.431 3.947 1.086l1.107-1.666zM5.5 0C3.58 0 1.797.523.447 1.42l1.107 1.666C2.539 2.431 3.928 2 5.5 2V0zM0 2.253v13h2v-13H0zm1.553 13.833C2.539 15.431 3.928 15 5.5 15v-2c-1.92 0-3.703.523-5.053 1.42l1.107 1.666zM5.5 15c1.572 0 2.961.431 3.947 1.086l1.107-1.666C9.203 13.523 7.42 13 5.5 13v2zm5.053-11.914C11.539 2.431 12.928 2 14.5 2V0c-1.92 0-3.703.523-5.053 1.42l1.107 1.666zM14.5 2c1.573 0 2.961.431 3.947 1.086l1.107-1.666C18.203.523 16.421 0 14.5 0v2zm3.5.253v13h2v-13h-2zm1.553 12.167C18.203 13.523 16.421 13 14.5 13v2c1.573 0 2.961.431 3.947 1.086l1.107-1.666zM14.5 13c-1.92 0-3.703.523-5.053 1.42l1.107 1.666C11.539 15.431 12.928 15 14.5 15v-2z"
    />
  </svg>
</template>

</file>

<file path='frontend/vue_frontend/src/components/icons/IconEcosystem.vue'>
<template>
  <svg xmlns="http://www.w3.org/2000/svg" width="18" height="20" fill="currentColor">
    <path
      d="M11.447 8.894a1 1 0 1 0-.894-1.789l.894 1.789zm-2.894-.789a1 1 0 1 0 .894 1.789l-.894-1.789zm0 1.789a1 1 0 1 0 .894-1.789l-.894 1.789zM7.447 7.106a1 1 0 1 0-.894 1.789l.894-1.789zM10 9a1 1 0 1 0-2 0h2zm-2 2.5a1 1 0 1 0 2 0H8zm9.447-5.606a1 1 0 1 0-.894-1.789l.894 1.789zm-2.894-.789a1 1 0 1 0 .894 1.789l-.894-1.789zm2 .789a1 1 0 1 0 .894-1.789l-.894 1.789zm-1.106-2.789a1 1 0 1 0-.894 1.789l.894-1.789zM18 5a1 1 0 1 0-2 0h2zm-2 2.5a1 1 0 1 0 2 0h-2zm-5.447-4.606a1 1 0 1 0 .894-1.789l-.894 1.789zM9 1l.447-.894a1 1 0 0 0-.894 0L9 1zm-2.447.106a1 1 0 1 0 .894 1.789l-.894-1.789zm-6 3a1 1 0 1 0 .894 1.789L.553 4.106zm2.894.789a1 1 0 1 0-.894-1.789l.894 1.789zm-2-.789a1 1 0 1 0-.894 1.789l.894-1.789zm1.106 2.789a1 1 0 1 0 .894-1.789l-.894 1.789zM2 5a1 1 0 1 0-2 0h2zM0 7.5a1 1 0 1 0 2 0H0zm8.553 12.394a1 1 0 1 0 .894-1.789l-.894 1.789zm-1.106-2.789a1 1 0 1 0-.894 1.789l.894-1.789zm1.106 1a1 1 0 1 0 .894 1.789l-.894-1.789zm2.894.789a1 1 0 1 0-.894-1.789l.894 1.789zM8 19a1 1 0 1 0 2 0H8zm2-2.5a1 1 0 1 0-2 0h2zm-7.447.394a1 1 0 1 0 .894-1.789l-.894 1.789zM1 15H0a1 1 0 0 0 .553.894L1 15zm1-2.5a1 1 0 1 0-2 0h2zm12.553 2.606a1 1 0 1 0 .894 1.789l-.894-1.789zM17 15l.447.894A1 1 0 0 0 18 15h-1zm1-2.5a1 1 0 1 0-2 0h2zm-7.447-5.394l-2 1 .894 1.789 2-1-.894-1.789zm-1.106 1l-2-1-.894 1.789 2 1 .894-1.789zM8 9v2.5h2V9H8zm8.553-4.894l-2 1 .894 1.789 2-1-.894-1.789zm.894 0l-2-1-.894 1.789 2 1 .894-1.789zM16 5v2.5h2V5h-2zm-4.553-3.894l-2-1-.894 1.789 2 1 .894-1.789zm-2.894-1l-2 1 .894 1.789 2-1L8.553.106zM1.447 5.894l2-1-.894-1.789-2 1 .894 1.789zm-.894 0l2 1 .894-1.789-2-1-.894 1.789zM0 5v2.5h2V5H0zm9.447 13.106l-2-1-.894 1.789 2 1 .894-1.789zm0 1.789l2-1-.894-1.789-2 1 .894 1.789zM10 19v-2.5H8V19h2zm-6.553-3.894l-2-1-.894 1.789 2 1 .894-1.789zM2 15v-2.5H0V15h2zm13.447 1.894l2-1-.894-1.789-2 1 .894 1.789zM18 15v-2.5h-2V15h2z"
    />
  </svg>
</template>

</file>

<file path='frontend/vue_frontend/src/components/icons/IconSupport.vue'>
<template>
  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor">
    <path
      d="M10 3.22l-.61-.6a5.5 5.5 0 0 0-7.666.105 5.5 5.5 0 0 0-.114 7.665L10 18.78l8.39-8.4a5.5 5.5 0 0 0-.114-7.665 5.5 5.5 0 0 0-7.666-.105l-.61.61z"
    />
  </svg>
</template>

</file>

<file path='frontend/vue_frontend/src/components/icons/IconTooling.vue'>
<!-- This icon is from <https://github.com/Templarian/MaterialDesign>, distributed under Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0) license-->
<template>
  <svg
    xmlns="http://www.w3.org/2000/svg"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    aria-hidden="true"
    role="img"
    class="iconify iconify--mdi"
    width="24"
    height="24"
    preserveAspectRatio="xMidYMid meet"
    viewBox="0 0 24 24"
  >
    <path
      d="M20 18v-4h-3v1h-2v-1H9v1H7v-1H4v4h16M6.33 8l-1.74 4H7v-1h2v1h6v-1h2v1h2.41l-1.74-4H6.33M9 5v1h6V5H9m12.84 7.61c.1.22.16.48.16.8V18c0 .53-.21 1-.6 1.41c-.4.4-.85.59-1.4.59H4c-.55 0-1-.19-1.4-.59C2.21 19 2 18.53 2 18v-4.59c0-.32.06-.58.16-.8L4.5 7.22C4.84 6.41 5.45 6 6.33 6H7V5c0-.55.18-1 .57-1.41C7.96 3.2 8.44 3 9 3h6c.56 0 1.04.2 1.43.59c.39.41.57.86.57 1.41v1h.67c.88 0 1.49.41 1.83 1.22l2.34 5.39z"
      fill="currentColor"
    ></path>
  </svg>
</template>

</file>

<file path='frontend/vue_frontend/src/main.ts'>
// frontend/vue_frontend/src/main.ts
import { createApp } from 'vue';
import { createPinia } from 'pinia';
import App from './App.vue';
import './assets/styles.css';

// --- REMOVED Unused Import ---
// import { useViewStore } from './stores/view';
// --- END REMOVED ---

console.log("[main.ts] Initializing Vue application...");

const app = createApp(App);
const pinia = createPinia(); // Create Pinia instance

console.log("[main.ts] Installing Pinia state management...");
app.use(pinia); // Use Pinia

// Stores are typically initialized within components or App.vue setup

console.log("[main.ts] Mounting the application to the '#app' element...");
app.mount('#app');

console.log("[main.ts] Vue application mounted successfully.");
</file>

<file path='frontend/vue_frontend/src/services/api.ts'>
// <file path='frontend/vue_frontend/src/services/api.ts'>
import axios, {
    type AxiosInstance,
    type InternalAxiosRequestConfig,
    type AxiosError // Import AxiosError type
} from 'axios';
import { useAuthStore } from '@/stores/auth'; // Adjust path as needed
import type { JobDetails } from '@/stores/job'; // Adjust path as needed

// --- Define API Response Interfaces ---

// Matches backend schemas/user.py -> UserResponse
export interface UserResponse {
    email: string;
    full_name: string | null;
    is_active: boolean | null;
    is_superuser: boolean | null;
    username: string;
    id: string; // UUID as string
    created_at: string; // ISO Date string
    updated_at: string; // ISO Date string
}

// Matches backend schemas/user.py -> UserCreate (for request body)
export interface UserCreateData {
    email: string;
    full_name?: string | null;
    is_active?: boolean | null;
    is_superuser?: boolean | null;
    username: string;
    password?: string; // Password required on create
}

// Matches backend schemas/user.py -> UserUpdate (for request body)
export interface UserUpdateData {
    email?: string | null;
    full_name?: string | null;
    password?: string | null; // Optional password update
    is_active?: boolean | null;
    is_superuser?: boolean | null;
}


// Matches backend response for /token (modified to include user object)
interface AuthResponse {
    access_token: string;
    token_type: string;
    user: UserResponse; // Include the user details
}

// Matches backend response for /api/v1/upload
interface UploadResponse {
    job_id: string;
    status: string;
    message: string;
    created_at: string;
    progress: number;
    current_stage: string;
}

// Matches backend response for /api/v1/jobs/{job_id}/notify
interface NotifyResponse {
    success: boolean;
    message: string;
}

// Matches backend response for /api/v1/jobs/ (list endpoint)
// Should align with app/schemas/job.py -> JobResponse
export interface JobResponse {
    id: string;
    company_name: string;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string;
    created_at: string; // ISO Date string
    updated_at?: string | null;
    completed_at?: string | null;
    output_file_name?: string | null;
    input_file_name: string;
    created_by: string;
    error_message?: string | null;
    target_level: number; // Ensure target_level is included here
}

// --- UPDATED JobStatsData Interface ---
// Matches backend models/classification.py -> ProcessingStats and console log
export interface JobStatsData {
    job_id: string;
    company_name: string;
    start_time: string | null; // Assuming ISO string
    end_time: string | null; // Assuming ISO string
    processing_duration_seconds: number | null; // Renamed from processing_time
    total_vendors: number | null; // Added
    unique_vendors: number | null; // Added (was present in console)
    successfully_classified_l4: number | null; // Keep for reference
    successfully_classified_l5: number | null; // Keep L5 count
    classification_not_possible_initial: number | null; // Added
    invalid_category_errors: number | null; // Added (was present in console)
    search_attempts: number | null; // Added
    search_successful_classifications_l1: number | null; // Added
    search_successful_classifications_l5: number | null; // Renamed from search_assisted_l5
    api_usage: { // Nested structure
        openrouter_calls: number | null;
        openrouter_prompt_tokens: number | null;
        openrouter_completion_tokens: number | null;
        openrouter_total_tokens: number | null;
        tavily_search_calls: number | null;
        cost_estimate_usd: number | null;
    } | null; // Allow api_usage itself to be null if not populated
}
// --- END UPDATED JobStatsData Interface ---


// Structure for download result helper
interface DownloadResult {
    blob: Blob;
    filename: string;
}

// Parameters for the job history list endpoint
interface GetJobsParams {
    status?: string;
    start_date?: string; // ISO string format
    end_date?: string; // ISO string format
    skip?: number;
    limit?: number;
}

// --- Axios Instance Setup ---

const axiosInstance: AxiosInstance = axios.create({
    baseURL: '/api/v1', // Assumes Vite dev server proxies /api/v1 to your backend
    timeout: 60000, // 60 seconds timeout
    headers: {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
    },
});

// --- Request Interceptor (Add Auth Token) ---
axiosInstance.interceptors.request.use(
    (config: InternalAxiosRequestConfig) => {
        const authStore = useAuthStore();
        const token = authStore.getToken();
        // No need for noAuthUrls here as login uses base axios
        if (token && config.url) {
            // LOGGING: Log token presence and target URL
            console.log(`[api.ts Request Interceptor] Adding token for URL: ${config.url}`);
            config.headers.Authorization = `Bearer ${token}`;
        } else {
            console.log(`[api.ts Request Interceptor] No token found or no URL for config: ${config.url}`);
        }
        return config;
    },
    (error: AxiosError) => {
        console.error('[api.ts Request Interceptor] Error:', error);
        return Promise.reject(error);
    }
);

// --- Response Interceptor (Handle Errors) ---
axiosInstance.interceptors.response.use(
    (response) => {
        // LOGGING: Log successful response status and URL
        console.log(`[api.ts Response Interceptor] Success for URL: ${response.config.url} | Status: ${response.status}`);
        return response;
    },
    (error: AxiosError) => {
        console.error('[api.ts Response Interceptor] Error:', error.config?.url, error.response?.status, error.message);
        const authStore = useAuthStore();

        if (error.response) {
            const { status, data } = error.response;

            // Handle 401 Unauthorized (except for login attempts)
            // Login uses base axios, so this interceptor won't catch its 401
            if (status === 401) {
                console.warn('[api.ts Response Interceptor] Received 401 Unauthorized. Logging out.');
                authStore.logout(); // Trigger logout action
                // No reload here, let the component handle redirection or UI change
                return Promise.reject(new Error('Session expired. Please log in again.'));
            }

            // Extract detailed error message from response data
            let detailMessage = 'An error occurred.';
            const responseData = data as any;

            if (responseData?.detail) {
                    if (Array.isArray(responseData.detail)) {
                        detailMessage = `Validation Error: ${responseData.detail.map((err: any) => `${err.loc?.join('.') ?? 'field'}: ${err.msg}`).join('; ')}`;
                    } else if (typeof responseData.detail === 'string') {
                        detailMessage = responseData.detail;
                    } else {
                        try { detailMessage = JSON.stringify(responseData.detail); } catch { /* ignore */ }
                    }
            } else if (typeof data === 'string' && data.length > 0 && data.length < 300) {
                detailMessage = data;
            } else if (error.message) {
                detailMessage = error.message;
            }

            const errorMessage = `Error ${status}: ${detailMessage}`;
            console.error(`[api.ts Response Interceptor] Rejecting with error: ${errorMessage}`); // LOGGING
            return Promise.reject(new Error(errorMessage));

        } else if (error.request) {
            console.error('[api.ts Response Interceptor] Network error or no response received:', error.request);
            return Promise.reject(new Error('Network error or server did not respond. Please check connection.'));
        } else {
            console.error('[api.ts Response Interceptor] Axios setup error:', error.message);
            return Promise.reject(new Error(`Request setup error: ${error.message}`));
        }
    }
);


// --- API Service Object ---

const apiService = {
    /**
        * Logs in a user. Uses base axios for specific headers.
        */
    async login(usernameInput: string, passwordInput: string): Promise<AuthResponse> {
        const params = new URLSearchParams();
        params.append('username', usernameInput);
        params.append('password', passwordInput);
        console.log(`[api.ts login] Attempting login for user: ${usernameInput}`); // LOGGING
        // Use base axios to avoid default JSON headers and ensure correct Content-Type
        const response = await axios.post<AuthResponse>('/token', params, {
            headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
        });
        console.log(`[api.ts login] Login successful for user: ${usernameInput}`); // LOGGING
        return response.data;
    },

    /**
        * Uploads the vendor file.
        */
    async uploadFile(formData: FormData): Promise<UploadResponse> {
        console.log('[api.ts uploadFile] Attempting file upload...'); // LOGGING
        // This uses axiosInstance, so /api/v1 prefix is added automatically
        const response = await axiosInstance.post<UploadResponse>('/upload', formData, {
                headers: { 'Content-Type': undefined } // Let browser set Content-Type for FormData
        });
        console.log(`[api.ts uploadFile] Upload successful, job ID: ${response.data.job_id}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches the status and details of a specific job.
        */
    async getJobStatus(jobId: string): Promise<JobDetails> {
        console.log(`[api.ts getJobStatus] Fetching status for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobDetails>(`/jobs/${jobId}`);
        console.log(`[api.ts getJobStatus] Received status for job ${jobId}:`, response.data.status); // LOGGING
        return response.data;
    },

    /**
        * Fetches statistics for a specific job.
        */
    async getJobStats(jobId: string): Promise<JobStatsData> { // Use the updated interface here
        console.log(`[api.ts getJobStats] Fetching stats for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get<JobStatsData>(`/jobs/${jobId}/stats`);
        // LOGGING: Log the received stats structure
        console.log(`[api.ts getJobStats] Received stats for job ${jobId}:`, JSON.parse(JSON.stringify(response.data)));
        return response.data;
    },

    /**
        * Requests email notification for a job completion.
        */
    async requestNotification(jobId: string, email: string): Promise<NotifyResponse> {
        console.log(`[api.ts requestNotification] Requesting notification for job ${jobId} to email ${email}`); // LOGGING
        const response = await axiosInstance.post<NotifyResponse>(`/jobs/${jobId}/notify`, { email });
        console.log(`[api.ts requestNotification] Notification request response:`, response.data.success); // LOGGING
        return response.data;
    },

    /**
        * Downloads the results file for a completed job.
        */
    async downloadResults(jobId: string): Promise<DownloadResult> {
        console.log(`[api.ts downloadResults] Requesting download for job ID: ${jobId}`); // LOGGING
        const response = await axiosInstance.get(`/jobs/${jobId}/download`, {
            responseType: 'blob',
        });
        const disposition = response.headers['content-disposition'];
        let filename = `results_${jobId}.xlsx`;
        if (disposition?.includes('attachment')) {
            const filenameMatch = disposition.match(/filename\*?=(?:(?:"((?:[^"\\]|\\.)*)")|(?:([^;\n]*)))/i);
            if (filenameMatch?.[1]) { filename = filenameMatch[1].replace(/\\"/g, '"'); }
            else if (filenameMatch?.[2]) {
                    const utf8Match = filenameMatch[2].match(/^UTF-8''(.*)/i);
                    if (utf8Match?.[1]) { try { filename = decodeURIComponent(utf8Match[1]); } catch (e) { filename = utf8Match[1]; } }
                    else { filename = filenameMatch[2]; }
            }
        }
        console.log(`[api.ts downloadResults] Determined download filename: ${filename}`); // LOGGING
        return { blob: response.data as Blob, filename };
    },

    /**
        * Fetches a list of jobs for the current user, with optional filtering/pagination.
        */
    async getJobs(params: GetJobsParams = {}): Promise<JobResponse[]> {
        const cleanedParams = Object.fromEntries(
            Object.entries(params).filter(([, value]) => value !== undefined && value !== null)
        );
        console.log('[api.ts getJobs] Fetching job list with params:', cleanedParams); // LOGGING
        const response = await axiosInstance.get<JobResponse[]>('/jobs/', { params: cleanedParams });
        console.log(`[api.ts getJobs] Received ${response.data.length} jobs.`); // LOGGING
        return response.data;
    },

    // --- User Management API Methods ---

    /**
        * Fetches the current logged-in user's details.
        */
    async getCurrentUser(): Promise<UserResponse> {
        console.log('[api.ts getCurrentUser] Fetching current user details...'); // LOGGING
        const response = await axiosInstance.get<UserResponse>('/users/me');
        console.log(`[api.ts getCurrentUser] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Fetches a list of all users (admin only).
        */
    async getUsers(skip: number = 0, limit: number = 100): Promise<UserResponse[]> {
        console.log(`[api.ts getUsers] Fetching user list (skip: ${skip}, limit: ${limit})...`); // LOGGING
        const response = await axiosInstance.get<UserResponse[]>('/users/', { params: { skip, limit } });
         console.log(`[api.ts getUsers] Received ${response.data.length} users.`); // LOGGING
        return response.data;
    },

        /**
        * Fetches a specific user by ID (admin or self).
        */
        async getUserById(userId: string): Promise<UserResponse> {
        console.log(`[api.ts getUserById] Fetching user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.get<UserResponse>(`/users/${userId}`);
        console.log(`[api.ts getUserById] Received user: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Creates a new user (admin only).
        */
    async createUser(userData: UserCreateData): Promise<UserResponse> {
        console.log(`[api.ts createUser] Attempting to create user: ${userData.username}`); // LOGGING
        const response = await axiosInstance.post<UserResponse>('/users/', userData);
        console.log(`[api.ts createUser] User created successfully: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Updates a user (admin or self).
        */
    async updateUser(userId: string, userData: UserUpdateData): Promise<UserResponse> {
        console.log(`[api.ts updateUser] Attempting to update user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.put<UserResponse>(`/users/${userId}`, userData);
        console.log(`[api.ts updateUser] User updated successfully: ${response.data.username}`); // LOGGING
        return response.data;
    },

    /**
        * Deletes a user (admin only).
        */
    async deleteUser(userId: string): Promise<{ message: string }> {
        console.log(`[api.ts deleteUser] Attempting to delete user ID: ${userId}`); // LOGGING
        const response = await axiosInstance.delete<{ message: string }>(`/users/${userId}`);
        console.log(`[api.ts deleteUser] User delete response: ${response.data.message}`); // LOGGING
        return response.data;
    },
    // --- END User Management API Methods ---
};

export default apiService;
</file>

<file path='frontend/vue_frontend/src/stores/auth.ts'>
// frontend/vue_frontend/src/stores/auth.ts
import { defineStore } from 'pinia';
import { ref, computed } from 'vue';
import apiService from '@/services/api'; // Adjust path as needed
import type { UserResponse } from '@/services/api'; // Import the UserResponse type

// Use the imported UserResponse interface directly
// interface User {
//     username: string;
//     email: string;
//     id: string; // Use string for UUID compatibility
//     is_active: boolean;
//     is_superuser: boolean;
//     full_name: string | null;
//     created_at: string;
//     updated_at: string;
// }

export const useAuthStore = defineStore('auth', () => {
    // --- State ---
    const token = ref<string | null>(localStorage.getItem('authToken'));
    // Use UserResponse type for user state
    const user = ref<UserResponse | null>(null);
    const loading = ref(false);
    const error = ref<string | null>(null);

    // --- Getters ---
    const isAuthenticated = computed(() => !!token.value && !!user.value); // Check user too
    const username = computed(() => user.value?.username || null);
    const isSuperuser = computed(() => user.value?.is_superuser || false); // Getter for superuser status

    // --- Actions ---
    async function login(usernameInput: string, passwordInput: string): Promise<void> {
        loading.value = true;
        error.value = null;
        try {
            console.log('AuthStore: Attempting login...');
            // API now returns user details in 'user' field
            const response = await apiService.login(usernameInput, passwordInput);
            token.value = response.access_token;
            user.value = response.user; // Store the full user object

            // Persist to localStorage
            if (token.value) {
                localStorage.setItem('authToken', token.value);
            } else {
                localStorage.removeItem('authToken');
            }
            if (user.value) {
                localStorage.setItem('authUser', JSON.stringify(user.value));
            } else {
                localStorage.removeItem('authUser');
            }

            console.log('AuthStore: Login successful.');
        } catch (err: any) {
            console.error('AuthStore: Login failed:', err);
            token.value = null;
            user.value = null;
            localStorage.removeItem('authToken');
            localStorage.removeItem('authUser');
            error.value = err.message || 'Login failed. Please check credentials.';
            throw err;
        } finally {
            loading.value = false;
        }
    }

    function logout(): void {
        console.log('AuthStore: Logging out...');
        token.value = null;
        user.value = null;
        error.value = null;
        localStorage.removeItem('authToken');
        localStorage.removeItem('authUser');
        // Optionally redirect or clear other stores if needed
        console.log('AuthStore: Logout complete.');
            // Reload to ensure clean state, especially if routing depends on auth
            window.location.reload();
    }

    function checkAuthStatus(): void {
        console.log('AuthStore: Checking auth status from localStorage...');
        const storedToken = localStorage.getItem('authToken');
        const storedUserJson = localStorage.getItem('authUser');

        if (storedToken && storedUserJson) {
            try {
                const parsedUser: UserResponse = JSON.parse(storedUserJson);
                // Basic validation
                if (parsedUser && parsedUser.id && parsedUser.username) {
                    token.value = storedToken;
                    user.value = parsedUser;
                    console.log('AuthStore: Session restored from localStorage.');
                } else {
                    console.warn('AuthStore: Invalid user data in localStorage. Logging out.');
                    logout(); // Call logout to clear everything
                }
            } catch (e) {
                console.error('AuthStore: Error parsing stored user data. Logging out.');
                logout(); // Call logout to clear everything
            }
        } else {
            console.log('AuthStore: No token or user found in localStorage.');
            // Ensure state matches localStorage absence
            if (token.value || user.value) {
                token.value = null;
                user.value = null;
                error.value = null; // Clear any lingering errors
            }
        }
    }

    // Action to get the current token (useful for API service interceptor)
    function getToken(): string | null {
        return token.value;
    }

    // --- ADDED: Action to fetch user details (e.g., after login or on refresh) ---
    async function fetchCurrentUserDetails(): Promise<void> {
            if (!isAuthenticated.value) {
                console.log("AuthStore: Not authenticated, cannot fetch user details.");
                return;
            }
            loading.value = true;
            error.value = null;
            try {
                console.log("AuthStore: Fetching current user details...");
                const currentUserDetails = await apiService.getCurrentUser(); // Assuming apiService has this
                user.value = currentUserDetails;
                localStorage.setItem('authUser', JSON.stringify(user.value)); // Update local storage
                console.log("AuthStore: Current user details updated.", currentUserDetails);
            } catch (err: any) {
                console.error("AuthStore: Failed to fetch current user details:", err);
                error.value = err.message || "Failed to load user details.";
                // Optional: Logout if fetching user details fails?
                // logout();
            } finally {
                loading.value = false;
            }
        }


    return {
        token,
        user,
        loading,
        error,
        isAuthenticated,
        username,
        isSuperuser, // Expose the new getter
        login,
        logout,
        checkAuthStatus,
        getToken,
        fetchCurrentUserDetails, // Expose the new action
    };
});

</file>

<file path='frontend/vue_frontend/src/stores/counter.ts'>
import { ref, computed } from 'vue'
import { defineStore } from 'pinia'

export const useCounterStore = defineStore('counter', () => {
  const count = ref(0)
  const doubleCount = computed(() => count.value * 2)
  function increment() {
    count.value++
  }

  return { count, doubleCount, increment }
})

</file>

<file path='frontend/vue_frontend/src/stores/job.ts'>
// <file path='frontend/vue_frontend/src/stores/job.ts'>
import { defineStore } from 'pinia';
import { ref } from 'vue';
import apiService, { type JobResponse } from '@/services/api'; // Import JobResponse type

// Define the structure of the job details object based on your API response
export interface JobDetails {
    id: string; // Changed from job_id to match JobResponse schema
    status: 'pending' | 'processing' | 'completed' | 'failed';
    progress: number;
    current_stage: string; // Consider using specific stage literals if known
    created_at: string | null;
    updated_at: string | null;
    completed_at?: string | null; // Optional completion time
    estimated_completion?: string | null; // Added optional field (backend doesn't provide this explicitly yet)
    error_message: string | null;
    target_level: number; // ADDED: Ensure target_level is part of the details
    // Add other fields returned by /api/v1/jobs/{job_id} if needed
    // Match JobResponse fields where applicable
    company_name?: string;
    input_file_name?: string;
    output_file_name?: string | null;
    created_by?: string;
}

export const useJobStore = defineStore('job', () => {
    // --- State ---
    const currentJobId = ref<string | null>(null);
    const jobDetails = ref<JobDetails | null>(null);
    const isLoading = ref(false); // For tracking polling/loading state for CURRENT job
    const error = ref<string | null>(null); // For storing errors related to fetching CURRENT job status

    // --- ADDED: Job History State ---
    const jobHistory = ref<JobResponse[]>([]);
    const historyLoading = ref(false);
    const historyError = ref<string | null>(null);
    // --- END ADDED ---

    // --- Actions ---
    function setCurrentJobId(jobId: string | null): void {
        console.log(`JobStore: Setting currentJobId from '${currentJobId.value}' to '${jobId}'`); // LOGGING
        if (currentJobId.value !== jobId) {
            currentJobId.value = jobId;
            // Clear details when ID changes (to null or a new ID) to force refresh
            jobDetails.value = null;
            console.log(`JobStore: Cleared jobDetails due to ID change.`); // LOGGING
            error.value = null; // Clear errors
            isLoading.value = false; // Reset loading state

            // Update URL to reflect the current job ID or clear it
            try {
                 const url = new URL(window.location.href);
                 if (jobId) {
                     url.searchParams.set('job_id', jobId);
                     console.log(`JobStore: Updated URL searchParam 'job_id' to ${jobId}`); // LOGGING
                 } else {
                     url.searchParams.delete('job_id');
                     console.log(`JobStore: Removed 'job_id' from URL searchParams.`); // LOGGING
                 }
                 // Use replaceState to avoid polluting history
                 window.history.replaceState({}, '', url.toString());
            } catch (e) {
                 console.error("JobStore: Failed to update URL:", e);
            }
        }
         // If the same job ID is set again, force a refresh of details
         else if (jobId !== null) {
             console.log(`JobStore: Re-setting same job ID ${jobId}, clearing details to force refresh.`); // LOGGING
             jobDetails.value = null;
             error.value = null;
             isLoading.value = false;
         }
    }

    function updateJobDetails(details: JobDetails): void {
        // Only update if the details are for the currently tracked job
        if (details && details.id === currentJobId.value) { // Match 'id' field from JobResponse/JobDetails
            // LOGGING: Include target_level in log
            console.log(`JobStore: Updating jobDetails for ${currentJobId.value} with status ${details.status}, progress ${details.progress}, target_level ${details.target_level}`);
            jobDetails.value = { ...details }; // Create new object for reactivity
            error.value = null; // Clear error on successful update
        } else if (details) {
            console.warn(`JobStore: Received details for job ${details.id}, but currently tracking ${currentJobId.value}. Ignoring update.`); // LOGGING
        } else {
            console.warn(`JobStore: updateJobDetails called with invalid details object.`); // LOGGING
        }
    }

    function setLoading(loading: boolean): void {
        isLoading.value = loading;
    }

    function setError(errorMessage: string | null): void {
        error.value = errorMessage;
    }

    function clearJob(): void {
        console.log('JobStore: Clearing job state.'); // LOGGING
        setCurrentJobId(null); // This also clears details, error, loading and URL param
        // --- ADDED: Clear history too on full clear? Optional. ---
        // jobHistory.value = [];
        // historyLoading.value = false;
        // historyError.value = null;
        // --- END ADDED ---
    }

    // --- ADDED: Job History Actions ---
    async function fetchJobHistory(params = {}): Promise<void> {
        console.log('JobStore: Fetching job history with params:', params); // LOGGING
        historyLoading.value = true;
        historyError.value = null;
        try {
            const jobs = await apiService.getJobs(params);
            jobHistory.value = jobs;
            console.log(`JobStore: Fetched ${jobs.length} jobs.`); // LOGGING
        } catch (err: any) {
            console.error('JobStore: Failed to fetch job history:', err); // LOGGING
            historyError.value = err.message || 'Failed to load job history.';
            jobHistory.value = []; // Clear history on error
        } finally {
            historyLoading.value = false;
        }
    }
    // --- END ADDED ---


    return {
        currentJobId,
        jobDetails,
        isLoading,
        error,
        // History state & actions
        jobHistory,
        historyLoading,
        historyError,
        fetchJobHistory,
        // Existing actions
        setCurrentJobId,
        updateJobDetails,
        setLoading,
        setError,
        clearJob,
    };
});
</file>

<file path='frontend/vue_frontend/src/stores/view.ts'>
// frontend/vue_frontend/src/stores/view.ts
import { defineStore } from 'pinia';
import { ref } from 'vue';

export const useViewStore = defineStore('view', () => {
    // --- State ---
    // 'landing', 'app', 'admin'
    const currentView = ref<'landing' | 'app' | 'admin'>('landing');

    // --- Actions ---
    function setView(view: 'landing' | 'app' | 'admin'): void {
        console.log(`ViewStore: Setting view to '${view}'`);
        currentView.value = view;
    }

    return {
        currentView,
        setView,
    };
});
</file>

<file path='frontend/vue_frontend/tailwind.config.js'>
// frontend/vue_frontend/tailwind.config.js
/** @type {import('tailwindcss').Config} */
import colors from 'tailwindcss/colors' // <-- Import default colors

export default {
  content: [
    "./index.html",
    "./src/**/*.{vue,js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {
      colors: {
        // Change primary to Indigo
        'primary': colors.indigo[600],         // #4f46e5
        'primary-hover': colors.indigo[700],    // #4338ca
        // Keep others or customize further
        'secondary': colors.gray[600],          // #4b5563
        'light': colors.gray[100],            // #f3f4f6
        'dark': colors.gray[800],             // #1f2937
        // You can add more custom colors here
      },
      fontFamily: {
         sans: ['Inter', 'sans-serif'],
      },
    },
  },
  plugins: [],
}
</file>

<file path='frontend/vue_frontend/vite.config.ts'>
// frontend/vue_frontend/vite.config.ts
// No changes needed based on errors, assuming tsconfig.node.json is set up correctly
import { fileURLToPath, URL } from 'node:url'

import { defineConfig } from 'vite'
import vue from '@vitejs/plugin-vue'
import vueDevTools from 'vite-plugin-vue-devtools'

// https://vite.dev/config/
export default defineConfig({
  plugins: [
    vue(),
    vueDevTools(),
  ],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url))
    }
  },
  server: {
    port: 5173, // Default Vite port
    proxy: {
      // Proxy API requests starting with /api or /token to the backend server
      // Change target based on where your FastAPI backend runs locally
      '/api': {
        target: 'http://localhost:8001', // Your FastAPI backend port from run_local.sh
        changeOrigin: true,
        // secure: false, // Uncomment if backend uses self-signed certs
        // rewrite: (path) => path.replace(/^\/api/, '/api') // Keep prefix if backend expects it
      },
       '/token': { // Proxy the /token endpoint separately if it's not under /api
        target: 'http://localhost:8001',
        changeOrigin: true,
        // secure: false,
      }
    }
  }
})
</file>

<file path='frontend/vue_frontend/yarn.lock'>
# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
# yarn lockfile v1


"@alloc/quick-lru@^5.2.0":
  version "5.2.0"
  resolved "https://registry.npmjs.org/@alloc/quick-lru/-/quick-lru-5.2.0.tgz"
  integrity sha512-UrcABB+4bUrFABwbluTIBErXwvbsU/V7TZWfmbgJfbkwiBuziS9gxdODUyuiecfdGQ85jglMW6juS3+z5TsKLw==

"@ampproject/remapping@^2.2.0":
  version "2.3.0"
  resolved "https://registry.npmjs.org/@ampproject/remapping/-/remapping-2.3.0.tgz"
  integrity sha512-30iZtAPgz+LTIYoeivqYo853f02jBYSd5uGnGpkFV0M3xOt9aN73erkgYAmZU43x4VfqcnLxW9Kpg3R5LC4YYw==
  dependencies:
    "@jridgewell/gen-mapping" "^0.3.5"
    "@jridgewell/trace-mapping" "^0.3.24"

"@antfu/utils@^0.7.10":
  version "0.7.10"
  resolved "https://registry.npmjs.org/@antfu/utils/-/utils-0.7.10.tgz"
  integrity sha512-+562v9k4aI80m1+VuMHehNJWLOFjBnXn3tdOitzD0il5b7smkSBal4+a3oKiQTbrwMmN/TBUMDvbdoWDehgOww==

"@babel/code-frame@^7.26.2":
  version "7.26.2"
  resolved "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.26.2.tgz"
  integrity sha512-RJlIHRueQgwWitWgF8OdFYGZX328Ax5BCemNGlqHfplnRT9ESi8JkFlvaVYbS+UubVY6dpv87Fs2u5M29iNFVQ==
  dependencies:
    "@babel/helper-validator-identifier" "^7.25.9"
    js-tokens "^4.0.0"
    picocolors "^1.0.0"

"@babel/compat-data@^7.26.8":
  version "7.26.8"
  resolved "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.26.8.tgz"
  integrity sha512-oH5UPLMWR3L2wEFLnFJ1TZXqHufiTKAiLfqw5zkhS4dKXLJ10yVztfil/twG8EDTA4F/tvVNw9nOl4ZMslB8rQ==

"@babel/core@^7.0.0", "@babel/core@^7.0.0-0", "@babel/core@^7.23.0":
  version "7.26.10"
  resolved "https://registry.npmjs.org/@babel/core/-/core-7.26.10.tgz"
  integrity sha512-vMqyb7XCDMPvJFFOaT9kxtiRh42GwlZEg1/uIgtZshS5a/8OaduUfCi7kynKgc3Tw/6Uo2D+db9qBttghhmxwQ==
  dependencies:
    "@ampproject/remapping" "^2.2.0"
    "@babel/code-frame" "^7.26.2"
    "@babel/generator" "^7.26.10"
    "@babel/helper-compilation-targets" "^7.26.5"
    "@babel/helper-module-transforms" "^7.26.0"
    "@babel/helpers" "^7.26.10"
    "@babel/parser" "^7.26.10"
    "@babel/template" "^7.26.9"
    "@babel/traverse" "^7.26.10"
    "@babel/types" "^7.26.10"
    convert-source-map "^2.0.0"
    debug "^4.1.0"
    gensync "^1.0.0-beta.2"
    json5 "^2.2.3"
    semver "^6.3.1"

"@babel/generator@^7.26.10", "@babel/generator@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/generator/-/generator-7.27.0.tgz"
  integrity sha512-VybsKvpiN1gU1sdMZIp7FcqphVVKEwcuj02x73uvcHE0PTihx1nlBcowYWhDwjpoAXRv43+gDzyggGnn1XZhVw==
  dependencies:
    "@babel/parser" "^7.27.0"
    "@babel/types" "^7.27.0"
    "@jridgewell/gen-mapping" "^0.3.5"
    "@jridgewell/trace-mapping" "^0.3.25"
    jsesc "^3.0.2"

"@babel/helper-annotate-as-pure@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.25.9.tgz"
  integrity sha512-gv7320KBUFJz1RnylIg5WWYPRXKZ884AGkYpgpWW02TH66Dl+HaC1t1CKd0z3R4b6hdYEcmrNZHUmfCP+1u3/g==
  dependencies:
    "@babel/types" "^7.25.9"

"@babel/helper-compilation-targets@^7.26.5":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.27.0.tgz"
  integrity sha512-LVk7fbXml0H2xH34dFzKQ7TDZ2G4/rVTOrq9V+icbbadjbVxxeFeDsNHv2SrZeWoA+6ZiTyWYWtScEIW07EAcA==
  dependencies:
    "@babel/compat-data" "^7.26.8"
    "@babel/helper-validator-option" "^7.25.9"
    browserslist "^4.24.0"
    lru-cache "^5.1.1"
    semver "^6.3.1"

"@babel/helper-create-class-features-plugin@^7.25.9", "@babel/helper-create-class-features-plugin@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/helper-create-class-features-plugin/-/helper-create-class-features-plugin-7.27.0.tgz"
  integrity sha512-vSGCvMecvFCd/BdpGlhpXYNhhC4ccxyvQWpbGL4CWbvfEoLFWUZuSuf7s9Aw70flgQF+6vptvgK2IfOnKlRmBg==
  dependencies:
    "@babel/helper-annotate-as-pure" "^7.25.9"
    "@babel/helper-member-expression-to-functions" "^7.25.9"
    "@babel/helper-optimise-call-expression" "^7.25.9"
    "@babel/helper-replace-supers" "^7.26.5"
    "@babel/helper-skip-transparent-expression-wrappers" "^7.25.9"
    "@babel/traverse" "^7.27.0"
    semver "^6.3.1"

"@babel/helper-member-expression-to-functions@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-member-expression-to-functions/-/helper-member-expression-to-functions-7.25.9.tgz"
  integrity sha512-wbfdZ9w5vk0C0oyHqAJbc62+vet5prjj01jjJ8sKn3j9h3MQQlflEdXYvuqRWjHnM12coDEqiC1IRCi0U/EKwQ==
  dependencies:
    "@babel/traverse" "^7.25.9"
    "@babel/types" "^7.25.9"

"@babel/helper-module-imports@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.25.9.tgz"
  integrity sha512-tnUA4RsrmflIM6W6RFTLFSXITtl0wKjgpnLgXyowocVPrbYrLUXSBXDgTs8BlbmIzIdlBySRQjINYs2BAkiLtw==
  dependencies:
    "@babel/traverse" "^7.25.9"
    "@babel/types" "^7.25.9"

"@babel/helper-module-transforms@^7.26.0":
  version "7.26.0"
  resolved "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.26.0.tgz"
  integrity sha512-xO+xu6B5K2czEnQye6BHA7DolFFmS3LB7stHZFaOLb1pAwO1HWLS8fXA+eh0A2yIvltPVmx3eNNDBJA2SLHXFw==
  dependencies:
    "@babel/helper-module-imports" "^7.25.9"
    "@babel/helper-validator-identifier" "^7.25.9"
    "@babel/traverse" "^7.25.9"

"@babel/helper-optimise-call-expression@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-optimise-call-expression/-/helper-optimise-call-expression-7.25.9.tgz"
  integrity sha512-FIpuNaz5ow8VyrYcnXQTDRGvV6tTjkNtCK/RYNDXGSLlUD6cBuQTSw43CShGxjvfBTfcUA/r6UhUCbtYqkhcuQ==
  dependencies:
    "@babel/types" "^7.25.9"

"@babel/helper-plugin-utils@^7.10.4", "@babel/helper-plugin-utils@^7.25.9", "@babel/helper-plugin-utils@^7.26.5":
  version "7.26.5"
  resolved "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.26.5.tgz"
  integrity sha512-RS+jZcRdZdRFzMyr+wcsaqOmld1/EqTghfaBGQQd/WnRdzdlvSZ//kF7U8VQTxf1ynZ4cjUcYgjVGx13ewNPMg==

"@babel/helper-replace-supers@^7.26.5":
  version "7.26.5"
  resolved "https://registry.npmjs.org/@babel/helper-replace-supers/-/helper-replace-supers-7.26.5.tgz"
  integrity sha512-bJ6iIVdYX1YooY2X7w1q6VITt+LnUILtNk7zT78ykuwStx8BauCzxvFqFaHjOpW1bVnSUM1PN1f0p5P21wHxvg==
  dependencies:
    "@babel/helper-member-expression-to-functions" "^7.25.9"
    "@babel/helper-optimise-call-expression" "^7.25.9"
    "@babel/traverse" "^7.26.5"

"@babel/helper-skip-transparent-expression-wrappers@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-skip-transparent-expression-wrappers/-/helper-skip-transparent-expression-wrappers-7.25.9.tgz"
  integrity sha512-K4Du3BFa3gvyhzgPcntrkDgZzQaq6uozzcpGbOO1OEJaI+EJdqWIMTLgFgQf6lrfiDFo5FU+BxKepI9RmZqahA==
  dependencies:
    "@babel/traverse" "^7.25.9"
    "@babel/types" "^7.25.9"

"@babel/helper-string-parser@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.25.9.tgz"
  integrity sha512-4A/SCr/2KLd5jrtOMFzaKjVtAei3+2r/NChoBNoZ3EyP/+GlhoaEGoWOZUmFmoITP7zOJyHIMm+DYRd8o3PvHA==

"@babel/helper-validator-identifier@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.25.9.tgz"
  integrity sha512-Ed61U6XJc3CVRfkERJWDz4dJwKe7iLmmJsbOGu9wSloNSFttHV0I8g6UAgb7qnK5ly5bGLPd4oXZlxCdANBOWQ==

"@babel/helper-validator-option@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.25.9.tgz"
  integrity sha512-e/zv1co8pp55dNdEcCynfj9X7nyUKUXoUEwfXqaZt0omVOmDe9oOTdKStH4GmAw6zxMFs50ZayuMfHDKlO7Tfw==

"@babel/helpers@^7.26.10":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/helpers/-/helpers-7.27.0.tgz"
  integrity sha512-U5eyP/CTFPuNE3qk+WZMxFkp/4zUzdceQlfzf7DdGdhp+Fezd7HD+i8Y24ZuTMKX3wQBld449jijbGq6OdGNQg==
  dependencies:
    "@babel/template" "^7.27.0"
    "@babel/types" "^7.27.0"

"@babel/parser@^7.25.3", "@babel/parser@^7.26.10", "@babel/parser@^7.26.9", "@babel/parser@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/parser/-/parser-7.27.0.tgz"
  integrity sha512-iaepho73/2Pz7w2eMS0Q5f83+0RKI7i4xmiYeBmDzfRVbQtTOG7Ts0S4HzJVsTMGI9keU8rNfuZr8DKfSt7Yyg==
  dependencies:
    "@babel/types" "^7.27.0"

"@babel/plugin-proposal-decorators@^7.23.0":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/plugin-proposal-decorators/-/plugin-proposal-decorators-7.25.9.tgz"
  integrity sha512-smkNLL/O1ezy9Nhy4CNosc4Va+1wo5w4gzSZeLe6y6dM4mmHfYOCPolXQPHQxonZCF+ZyebxN9vqOolkYrSn5g==
  dependencies:
    "@babel/helper-create-class-features-plugin" "^7.25.9"
    "@babel/helper-plugin-utils" "^7.25.9"
    "@babel/plugin-syntax-decorators" "^7.25.9"

"@babel/plugin-syntax-decorators@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/plugin-syntax-decorators/-/plugin-syntax-decorators-7.25.9.tgz"
  integrity sha512-ryzI0McXUPJnRCvMo4lumIKZUzhYUO/ScI+Mz4YVaTLt04DHNSjEUjKVvbzQjZFLuod/cYEc07mJWhzl6v4DPg==
  dependencies:
    "@babel/helper-plugin-utils" "^7.25.9"

"@babel/plugin-syntax-import-attributes@^7.22.5":
  version "7.26.0"
  resolved "https://registry.npmjs.org/@babel/plugin-syntax-import-attributes/-/plugin-syntax-import-attributes-7.26.0.tgz"
  integrity sha512-e2dttdsJ1ZTpi3B9UYGLw41hifAubg19AtCu/2I/F1QNVclOBr1dYpTdmdyZ84Xiz43BS/tCUkMAZNLv12Pi+A==
  dependencies:
    "@babel/helper-plugin-utils" "^7.25.9"

"@babel/plugin-syntax-import-meta@^7.10.4":
  version "7.10.4"
  resolved "https://registry.npmjs.org/@babel/plugin-syntax-import-meta/-/plugin-syntax-import-meta-7.10.4.tgz"
  integrity sha512-Yqfm+XDx0+Prh3VSeEQCPU81yC+JWZ2pDPFSS4ZdpfZhp4MkFMaDC1UqseovEKwSUpnIL7+vK+Clp7bfh0iD7g==
  dependencies:
    "@babel/helper-plugin-utils" "^7.10.4"

"@babel/plugin-syntax-jsx@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.25.9.tgz"
  integrity sha512-ld6oezHQMZsZfp6pWtbjaNDF2tiiCYYDqQszHt5VV437lewP9aSi2Of99CK0D0XB21k7FLgnLcmQKyKzynfeAA==
  dependencies:
    "@babel/helper-plugin-utils" "^7.25.9"

"@babel/plugin-syntax-typescript@^7.25.9":
  version "7.25.9"
  resolved "https://registry.npmjs.org/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.25.9.tgz"
  integrity sha512-hjMgRy5hb8uJJjUcdWunWVcoi9bGpJp8p5Ol1229PoN6aytsLwNMgmdftO23wnCLMfVmTwZDWMPNq/D1SY60JQ==
  dependencies:
    "@babel/helper-plugin-utils" "^7.25.9"

"@babel/plugin-transform-typescript@^7.22.15":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/plugin-transform-typescript/-/plugin-transform-typescript-7.27.0.tgz"
  integrity sha512-fRGGjO2UEGPjvEcyAZXRXAS8AfdaQoq7HnxAbJoAoW10B9xOKesmmndJv+Sym2a+9FHWZ9KbyyLCe9s0Sn5jtg==
  dependencies:
    "@babel/helper-annotate-as-pure" "^7.25.9"
    "@babel/helper-create-class-features-plugin" "^7.27.0"
    "@babel/helper-plugin-utils" "^7.26.5"
    "@babel/helper-skip-transparent-expression-wrappers" "^7.25.9"
    "@babel/plugin-syntax-typescript" "^7.25.9"

"@babel/template@^7.26.9", "@babel/template@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/template/-/template-7.27.0.tgz"
  integrity sha512-2ncevenBqXI6qRMukPlXwHKHchC7RyMuu4xv5JBXRfOGVcTy1mXCD12qrp7Jsoxll1EV3+9sE4GugBVRjT2jFA==
  dependencies:
    "@babel/code-frame" "^7.26.2"
    "@babel/parser" "^7.27.0"
    "@babel/types" "^7.27.0"

"@babel/traverse@^7.25.9", "@babel/traverse@^7.26.10", "@babel/traverse@^7.26.5", "@babel/traverse@^7.26.9", "@babel/traverse@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/traverse/-/traverse-7.27.0.tgz"
  integrity sha512-19lYZFzYVQkkHkl4Cy4WrAVcqBkgvV2YM2TU3xG6DIwO7O3ecbDPfW3yM3bjAGcqcQHi+CCtjMR3dIEHxsd6bA==
  dependencies:
    "@babel/code-frame" "^7.26.2"
    "@babel/generator" "^7.27.0"
    "@babel/parser" "^7.27.0"
    "@babel/template" "^7.27.0"
    "@babel/types" "^7.27.0"
    debug "^4.3.1"
    globals "^11.1.0"

"@babel/types@^7.25.9", "@babel/types@^7.26.10", "@babel/types@^7.26.9", "@babel/types@^7.27.0":
  version "7.27.0"
  resolved "https://registry.npmjs.org/@babel/types/-/types-7.27.0.tgz"
  integrity sha512-H45s8fVLYjbhFH62dIJ3WtmJ6RSPt/3DRO0ZcT2SUiYiQyz3BLVb9ADEnLl91m74aQPS3AzzeajZHYOalWe3bg==
  dependencies:
    "@babel/helper-string-parser" "^7.25.9"
    "@babel/helper-validator-identifier" "^7.25.9"

"@esbuild/darwin-arm64@0.25.1":
  version "0.25.1"
  resolved "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.1.tgz"
  integrity sha512-5hEZKPf+nQjYoSr/elb62U19/l1mZDdqidGfmFutVUjjUZrOazAtwK+Kr+3y0C/oeJfLlxo9fXb1w7L+P7E4FQ==

"@headlessui/vue@^1.7.23":
  version "1.7.23"
  resolved "https://registry.npmjs.org/@headlessui/vue/-/vue-1.7.23.tgz"
  integrity sha512-JzdCNqurrtuu0YW6QaDtR2PIYCKPUWq28csDyMvN4zmGccmE7lz40Is6hc3LA4HFeCI7sekZ/PQMTNmn9I/4Wg==
  dependencies:
    "@tanstack/vue-virtual" "^3.0.0-beta.60"

"@heroicons/vue@^2.2.0":
  version "2.2.0"
  resolved "https://registry.npmjs.org/@heroicons/vue/-/vue-2.2.0.tgz"
  integrity sha512-G3dbSxoeEKqbi/DFalhRxJU4mTXJn7GwZ7ae8NuEQzd1bqdd0jAbdaBZlHPcvPD2xI1iGzNVB4k20Un2AguYPw==

"@isaacs/cliui@^8.0.2":
  version "8.0.2"
  resolved "https://registry.npmjs.org/@isaacs/cliui/-/cliui-8.0.2.tgz"
  integrity sha512-O8jcjabXaleOG9DQ0+ARXWZBTfnP4WNAqzuiJK7ll44AmxGKv/J2M4TPjxjY3znBCfvBXFzucm1twdyFybFqEA==
  dependencies:
    string-width "^5.1.2"
    string-width-cjs "npm:string-width@^4.2.0"
    strip-ansi "^7.0.1"
    strip-ansi-cjs "npm:strip-ansi@^6.0.1"
    wrap-ansi "^8.1.0"
    wrap-ansi-cjs "npm:wrap-ansi@^7.0.0"

"@jridgewell/gen-mapping@^0.3.2", "@jridgewell/gen-mapping@^0.3.5":
  version "0.3.8"
  resolved "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.8.tgz"
  integrity sha512-imAbBGkb+ebQyxKgzv5Hu2nmROxoDOXHh80evxdoXNOrvAnVx7zimzc1Oo5h9RlfV4vPXaE2iM5pOFbvOCClWA==
  dependencies:
    "@jridgewell/set-array" "^1.2.1"
    "@jridgewell/sourcemap-codec" "^1.4.10"
    "@jridgewell/trace-mapping" "^0.3.24"

"@jridgewell/resolve-uri@^3.1.0":
  version "3.1.2"
  resolved "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz"
  integrity sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==

"@jridgewell/set-array@^1.2.1":
  version "1.2.1"
  resolved "https://registry.npmjs.org/@jridgewell/set-array/-/set-array-1.2.1.tgz"
  integrity sha512-R8gLRTZeyp03ymzP/6Lil/28tGeGEzhx1q2k703KGWRAI1VdvPIXdG70VJc2pAMw3NA6JKL5hhFu1sJX0Mnn/A==

"@jridgewell/sourcemap-codec@^1.4.10", "@jridgewell/sourcemap-codec@^1.4.14", "@jridgewell/sourcemap-codec@^1.5.0":
  version "1.5.0"
  resolved "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.0.tgz"
  integrity sha512-gv3ZRaISU3fjPAgNsriBRqGWQL6quFx04YMPW/zD8XMLsU32mhCCbfbO6KZFLjvYpCZ8zyDEgqsgf+PwPaM7GQ==

"@jridgewell/trace-mapping@^0.3.24", "@jridgewell/trace-mapping@^0.3.25":
  version "0.3.25"
  resolved "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.25.tgz"
  integrity sha512-vNk6aEwybGtawWmy/PzwnGDOjCkLWSD2wqvjGGAgOAwCGWySYXfYoxt00IJkTF+8Lb57DwOb3Aa0o9CApepiYQ==
  dependencies:
    "@jridgewell/resolve-uri" "^3.1.0"
    "@jridgewell/sourcemap-codec" "^1.4.14"

"@nodelib/fs.scandir@2.1.5":
  version "2.1.5"
  resolved "https://registry.npmjs.org/@nodelib/fs.scandir/-/fs.scandir-2.1.5.tgz"
  integrity sha512-vq24Bq3ym5HEQm2NKCr3yXDwjc7vTsEThRDnkp2DK9p1uqLR+DHurm/NOTo0KG7HYHU7eppKZj3MyqYuMBf62g==
  dependencies:
    "@nodelib/fs.stat" "2.0.5"
    run-parallel "^1.1.9"

"@nodelib/fs.stat@^2.0.2", "@nodelib/fs.stat@2.0.5":
  version "2.0.5"
  resolved "https://registry.npmjs.org/@nodelib/fs.stat/-/fs.stat-2.0.5.tgz"
  integrity sha512-RkhPPp2zrqDAQA/2jNhnztcPAlv64XdhIp7a7454A5ovI7Bukxgt7MX7udwAu3zg1DcpPU0rz3VV1SeaqvY4+A==

"@nodelib/fs.walk@^1.2.3":
  version "1.2.8"
  resolved "https://registry.npmjs.org/@nodelib/fs.walk/-/fs.walk-1.2.8.tgz"
  integrity sha512-oGB+UxlgWcgQkgwo8GcEGwemoTFt3FIO9ababBmaGwXIoBKZ+GTy0pP185beGg7Llih/NSHSV2XAs1lnznocSg==
  dependencies:
    "@nodelib/fs.scandir" "2.1.5"
    fastq "^1.6.0"

"@pkgjs/parseargs@^0.11.0":
  version "0.11.0"
  resolved "https://registry.npmjs.org/@pkgjs/parseargs/-/parseargs-0.11.0.tgz"
  integrity sha512-+1VkjdD0QBLPodGrJUeqarH8VAIvQODIbwh9XpP5Syisf7YoQgsJKPNFoqqLQlu+VQ/tVSshMR6loPMn8U+dPg==

"@polka/url@^1.0.0-next.24":
  version "1.0.0-next.28"
  resolved "https://registry.npmjs.org/@polka/url/-/url-1.0.0-next.28.tgz"
  integrity sha512-8LduaNlMZGwdZ6qWrKlfa+2M4gahzFkprZiAt2TF8uS0qQgBizKXpXURqvTJ4WtmupWxaLqjRb2UCTe72mu+Aw==

"@rollup/pluginutils@^5.1.3":
  version "5.1.4"
  resolved "https://registry.npmjs.org/@rollup/pluginutils/-/pluginutils-5.1.4.tgz"
  integrity sha512-USm05zrsFxYLPdWWq+K3STlWiT/3ELn3RcV5hJMghpeAIhxfsUIg6mt12CBJBInWMV4VneoV7SfGv8xIwo2qNQ==
  dependencies:
    "@types/estree" "^1.0.0"
    estree-walker "^2.0.2"
    picomatch "^4.0.2"

"@rollup/rollup-darwin-arm64@4.38.0":
  version "4.38.0"
  resolved "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.38.0.tgz"
  integrity sha512-buA17AYXlW9Rn091sWMq1xGUvWQFOH4N1rqUxGJtEQzhChxWjldGCCup7r/wUnaI6Au8sKXpoh0xg58a7cgcpg==

"@sec-ant/readable-stream@^0.4.1":
  version "0.4.1"
  resolved "https://registry.npmjs.org/@sec-ant/readable-stream/-/readable-stream-0.4.1.tgz"
  integrity sha512-831qok9r2t8AlxLko40y2ebgSDhenenCatLVeW/uBtnHPyhHOvG0C7TvfgecV+wHzIm5KUICgzmVpWS+IMEAeg==

"@sindresorhus/merge-streams@^4.0.0":
  version "4.0.0"
  resolved "https://registry.npmjs.org/@sindresorhus/merge-streams/-/merge-streams-4.0.0.tgz"
  integrity sha512-tlqY9xq5ukxTUZBmoOp+m61cqwQD5pHJtFY3Mn8CA8ps6yghLH/Hw8UPdqg4OLmFW3IFlcXnQNmo/dh8HzXYIQ==

"@tanstack/virtual-core@3.13.6":
  version "3.13.6"
  resolved "https://registry.npmjs.org/@tanstack/virtual-core/-/virtual-core-3.13.6.tgz"
  integrity sha512-cnQUeWnhNP8tJ4WsGcYiX24Gjkc9ALstLbHcBj1t3E7EimN6n6kHH+DPV4PpDnuw00NApQp+ViojMj1GRdwYQg==

"@tanstack/vue-virtual@^3.0.0-beta.60":
  version "3.13.6"
  resolved "https://registry.npmjs.org/@tanstack/vue-virtual/-/vue-virtual-3.13.6.tgz"
  integrity sha512-GYdZ3SJBQPzgxhuCE2fvpiH46qzHiVx5XzBSdtESgiqh4poj8UgckjGWYEhxaBbcVt1oLzh1m3Ql4TyH32TOzQ==
  dependencies:
    "@tanstack/virtual-core" "3.13.6"

"@tsconfig/node22@^22.0.0":
  version "22.0.1"
  resolved "https://registry.npmjs.org/@tsconfig/node22/-/node22-22.0.1.tgz"
  integrity sha512-VkgOa3n6jvs1p+r3DiwBqeEwGAwEvnVCg/hIjiANl5IEcqP3G0u5m8cBJspe1t9qjZRlZ7WFgqq5bJrGdgAKMg==

"@types/estree@^1.0.0", "@types/estree@1.0.7":
  version "1.0.7"
  resolved "https://registry.npmjs.org/@types/estree/-/estree-1.0.7.tgz"
  integrity sha512-w28IoSUCJpidD/TGviZwwMJckNESJZXFu7NBZ5YJ4mEUnNraUn9Pm8HSZm/jDF1pDWYKspWE7oVphigUPRakIQ==

"@types/node@^18.0.0 || ^20.0.0 || >=22.0.0", "@types/node@^22.13.9":
  version "22.13.14"
  resolved "https://registry.npmjs.org/@types/node/-/node-22.13.14.tgz"
  integrity sha512-Zs/Ollc1SJ8nKUAgc7ivOEdIBM8JAKgrqqUYi2J997JuKO7/tpQC+WCetQ1sypiKCQWHdvdg9wBNpUPEWZae7w==
  dependencies:
    undici-types "~6.20.0"

"@vitejs/plugin-vue@^5.2.1":
  version "5.2.3"
  resolved "https://registry.npmjs.org/@vitejs/plugin-vue/-/plugin-vue-5.2.3.tgz"
  integrity sha512-IYSLEQj4LgZZuoVpdSUCw3dIynTWQgPlaRP6iAvMle4My0HdYwr5g5wQAfwOeHQBmYwEkqF70nRpSilr6PoUDg==

"@volar/language-core@~2.4.11", "@volar/language-core@2.4.12":
  version "2.4.12"
  resolved "https://registry.npmjs.org/@volar/language-core/-/language-core-2.4.12.tgz"
  integrity sha512-RLrFdXEaQBWfSnYGVxvR2WrO6Bub0unkdHYIdC31HzIEqATIuuhRRzYu76iGPZ6OtA4Au1SnW0ZwIqPP217YhA==
  dependencies:
    "@volar/source-map" "2.4.12"

"@volar/source-map@2.4.12":
  version "2.4.12"
  resolved "https://registry.npmjs.org/@volar/source-map/-/source-map-2.4.12.tgz"
  integrity sha512-bUFIKvn2U0AWojOaqf63ER0N/iHIBYZPpNGogfLPQ68F5Eet6FnLlyho7BS0y2HJ1jFhSif7AcuTx1TqsCzRzw==

"@volar/typescript@~2.4.11":
  version "2.4.12"
  resolved "https://registry.npmjs.org/@volar/typescript/-/typescript-2.4.12.tgz"
  integrity sha512-HJB73OTJDgPc80K30wxi3if4fSsZZAOScbj2fcicMuOPoOkcf9NNAINb33o+DzhBdF9xTKC1gnPmIRDous5S0g==
  dependencies:
    "@volar/language-core" "2.4.12"
    path-browserify "^1.0.1"
    vscode-uri "^3.0.8"

"@vue/babel-helper-vue-transform-on@1.4.0":
  version "1.4.0"
  resolved "https://registry.npmjs.org/@vue/babel-helper-vue-transform-on/-/babel-helper-vue-transform-on-1.4.0.tgz"
  integrity sha512-mCokbouEQ/ocRce/FpKCRItGo+013tHg7tixg3DUNS+6bmIchPt66012kBMm476vyEIJPafrvOf4E5OYj3shSw==

"@vue/babel-plugin-jsx@^1.1.5":
  version "1.4.0"
  resolved "https://registry.npmjs.org/@vue/babel-plugin-jsx/-/babel-plugin-jsx-1.4.0.tgz"
  integrity sha512-9zAHmwgMWlaN6qRKdrg1uKsBKHvnUU+Py+MOCTuYZBoZsopa90Di10QRjB+YPnVss0BZbG/H5XFwJY1fTxJWhA==
  dependencies:
    "@babel/helper-module-imports" "^7.25.9"
    "@babel/helper-plugin-utils" "^7.26.5"
    "@babel/plugin-syntax-jsx" "^7.25.9"
    "@babel/template" "^7.26.9"
    "@babel/traverse" "^7.26.9"
    "@babel/types" "^7.26.9"
    "@vue/babel-helper-vue-transform-on" "1.4.0"
    "@vue/babel-plugin-resolve-type" "1.4.0"
    "@vue/shared" "^3.5.13"

"@vue/babel-plugin-resolve-type@1.4.0":
  version "1.4.0"
  resolved "https://registry.npmjs.org/@vue/babel-plugin-resolve-type/-/babel-plugin-resolve-type-1.4.0.tgz"
  integrity sha512-4xqDRRbQQEWHQyjlYSgZsWj44KfiF6D+ktCuXyZ8EnVDYV3pztmXJDf1HveAjUAXxAnR8daCQT51RneWWxtTyQ==
  dependencies:
    "@babel/code-frame" "^7.26.2"
    "@babel/helper-module-imports" "^7.25.9"
    "@babel/helper-plugin-utils" "^7.26.5"
    "@babel/parser" "^7.26.9"
    "@vue/compiler-sfc" "^3.5.13"

"@vue/compiler-core@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/compiler-core/-/compiler-core-3.5.13.tgz"
  integrity sha512-oOdAkwqUfW1WqpwSYJce06wvt6HljgY3fGeM9NcVA1HaYOij3mZG9Rkysn0OHuyUAGMbEbARIpsG+LPVlBJ5/Q==
  dependencies:
    "@babel/parser" "^7.25.3"
    "@vue/shared" "3.5.13"
    entities "^4.5.0"
    estree-walker "^2.0.2"
    source-map-js "^1.2.0"

"@vue/compiler-dom@^3.3.4", "@vue/compiler-dom@^3.5.0", "@vue/compiler-dom@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/compiler-dom/-/compiler-dom-3.5.13.tgz"
  integrity sha512-ZOJ46sMOKUjO3e94wPdCzQ6P1Lx/vhp2RSvfaab88Ajexs0AHeV0uasYhi99WPaogmBlRHNRuly8xV75cNTMDA==
  dependencies:
    "@vue/compiler-core" "3.5.13"
    "@vue/shared" "3.5.13"

"@vue/compiler-sfc@^3.5.13", "@vue/compiler-sfc@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/compiler-sfc/-/compiler-sfc-3.5.13.tgz"
  integrity sha512-6VdaljMpD82w6c2749Zhf5T9u5uLBWKnVue6XWxprDobftnletJ8+oel7sexFfM3qIxNmVE7LSFGTpv6obNyaQ==
  dependencies:
    "@babel/parser" "^7.25.3"
    "@vue/compiler-core" "3.5.13"
    "@vue/compiler-dom" "3.5.13"
    "@vue/compiler-ssr" "3.5.13"
    "@vue/shared" "3.5.13"
    estree-walker "^2.0.2"
    magic-string "^0.30.11"
    postcss "^8.4.48"
    source-map-js "^1.2.0"

"@vue/compiler-ssr@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/compiler-ssr/-/compiler-ssr-3.5.13.tgz"
  integrity sha512-wMH6vrYHxQl/IybKJagqbquvxpWCuVYpoUJfCqFZwa/JY1GdATAQ+TgVtgrwwMZ0D07QhA99rs/EAAWfvG6KpA==
  dependencies:
    "@vue/compiler-dom" "3.5.13"
    "@vue/shared" "3.5.13"

"@vue/compiler-vue2@^2.7.16":
  version "2.7.16"
  resolved "https://registry.npmjs.org/@vue/compiler-vue2/-/compiler-vue2-2.7.16.tgz"
  integrity sha512-qYC3Psj9S/mfu9uVi5WvNZIzq+xnXMhOwbTFKKDD7b1lhpnn71jXSFdTQ+WsIEk0ONCd7VV2IMm7ONl6tbQ86A==
  dependencies:
    de-indent "^1.0.2"
    he "^1.2.0"

"@vue/devtools-api@^7.7.2":
  version "7.7.2"
  resolved "https://registry.npmjs.org/@vue/devtools-api/-/devtools-api-7.7.2.tgz"
  integrity sha512-1syn558KhyN+chO5SjlZIwJ8bV/bQ1nOVTG66t2RbG66ZGekyiYNmRO7X9BJCXQqPsFHlnksqvPhce2qpzxFnA==
  dependencies:
    "@vue/devtools-kit" "^7.7.2"

"@vue/devtools-core@^7.7.2":
  version "7.7.2"
  resolved "https://registry.npmjs.org/@vue/devtools-core/-/devtools-core-7.7.2.tgz"
  integrity sha512-lexREWj1lKi91Tblr38ntSsy6CvI8ba7u+jmwh2yruib/ltLUcsIzEjCnrkh1yYGGIKXbAuYV2tOG10fGDB9OQ==
  dependencies:
    "@vue/devtools-kit" "^7.7.2"
    "@vue/devtools-shared" "^7.7.2"
    mitt "^3.0.1"
    nanoid "^5.0.9"
    pathe "^2.0.2"
    vite-hot-client "^0.2.4"

"@vue/devtools-kit@^7.7.2":
  version "7.7.2"
  resolved "https://registry.npmjs.org/@vue/devtools-kit/-/devtools-kit-7.7.2.tgz"
  integrity sha512-CY0I1JH3Z8PECbn6k3TqM1Bk9ASWxeMtTCvZr7vb+CHi+X/QwQm5F1/fPagraamKMAHVfuuCbdcnNg1A4CYVWQ==
  dependencies:
    "@vue/devtools-shared" "^7.7.2"
    birpc "^0.2.19"
    hookable "^5.5.3"
    mitt "^3.0.1"
    perfect-debounce "^1.0.0"
    speakingurl "^14.0.1"
    superjson "^2.2.1"

"@vue/devtools-shared@^7.7.2":
  version "7.7.2"
  resolved "https://registry.npmjs.org/@vue/devtools-shared/-/devtools-shared-7.7.2.tgz"
  integrity sha512-uBFxnp8gwW2vD6FrJB8JZLUzVb6PNRG0B0jBnHsOH8uKyva2qINY8PTF5Te4QlTbMDqU5K6qtJDr6cNsKWhbOA==
  dependencies:
    rfdc "^1.4.1"

"@vue/language-core@2.2.8":
  version "2.2.8"
  resolved "https://registry.npmjs.org/@vue/language-core/-/language-core-2.2.8.tgz"
  integrity sha512-rrzB0wPGBvcwaSNRriVWdNAbHQWSf0NlGqgKHK5mEkXpefjUlVRP62u03KvwZpvKVjRnBIQ/Lwre+Mx9N6juUQ==
  dependencies:
    "@volar/language-core" "~2.4.11"
    "@vue/compiler-dom" "^3.5.0"
    "@vue/compiler-vue2" "^2.7.16"
    "@vue/shared" "^3.5.0"
    alien-signals "^1.0.3"
    minimatch "^9.0.3"
    muggle-string "^0.4.1"
    path-browserify "^1.0.1"

"@vue/reactivity@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/reactivity/-/reactivity-3.5.13.tgz"
  integrity sha512-NaCwtw8o48B9I6L1zl2p41OHo/2Z4wqYGGIK1Khu5T7yxrn+ATOixn/Udn2m+6kZKB/J7cuT9DbWWhRxqixACg==
  dependencies:
    "@vue/shared" "3.5.13"

"@vue/runtime-core@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/runtime-core/-/runtime-core-3.5.13.tgz"
  integrity sha512-Fj4YRQ3Az0WTZw1sFe+QDb0aXCerigEpw418pw1HBUKFtnQHWzwojaukAs2X/c9DQz4MQ4bsXTGlcpGxU/RCIw==
  dependencies:
    "@vue/reactivity" "3.5.13"
    "@vue/shared" "3.5.13"

"@vue/runtime-dom@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/runtime-dom/-/runtime-dom-3.5.13.tgz"
  integrity sha512-dLaj94s93NYLqjLiyFzVs9X6dWhTdAlEAciC3Moq7gzAc13VJUdCnjjRurNM6uTLFATRHexHCTu/Xp3eW6yoog==
  dependencies:
    "@vue/reactivity" "3.5.13"
    "@vue/runtime-core" "3.5.13"
    "@vue/shared" "3.5.13"
    csstype "^3.1.3"

"@vue/server-renderer@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/server-renderer/-/server-renderer-3.5.13.tgz"
  integrity sha512-wAi4IRJV/2SAW3htkTlB+dHeRmpTiVIK1OGLWV1yeStVSebSQQOwGwIq0D3ZIoBj2C2qpgz5+vX9iEBkTdk5YA==
  dependencies:
    "@vue/compiler-ssr" "3.5.13"
    "@vue/shared" "3.5.13"

"@vue/shared@^3.5.0", "@vue/shared@^3.5.13", "@vue/shared@3.5.13":
  version "3.5.13"
  resolved "https://registry.npmjs.org/@vue/shared/-/shared-3.5.13.tgz"
  integrity sha512-/hnE/qP5ZoGpol0a5mDi45bOd7t3tjYJBjsgCsivow7D48cJeV5l05RD82lPqi7gRiphZM37rnhW1l6ZoCNNnQ==

"@vue/tsconfig@^0.7.0":
  version "0.7.0"
  resolved "https://registry.npmjs.org/@vue/tsconfig/-/tsconfig-0.7.0.tgz"
  integrity sha512-ku2uNz5MaZ9IerPPUyOHzyjhXoX2kVJaVf7hL315DC17vS6IiZRmmCPfggNbU16QTvM80+uYYy3eYJB59WCtvg==

alien-signals@^1.0.3:
  version "1.0.10"
  resolved "https://registry.npmjs.org/alien-signals/-/alien-signals-1.0.10.tgz"
  integrity sha512-pBrgovDvA/c55/aA+ar5pxNCvjQB5IlODtpOQXmUyrpclWIsHmUMsfIuCWsSU/l1iLU2O3ZhICdPaYTsuvGu8Q==

ansi-regex@^3.0.0:
  version "3.0.1"
  resolved "https://registry.npmjs.org/ansi-regex/-/ansi-regex-3.0.1.tgz"
  integrity sha512-+O9Jct8wf++lXxxFc4hc8LsjaSq0HFzzL7cVsw8pRDIPdjKD2mT4ytDZlLuSBZ4cLKZFXIrMGO7DbQCtMJJMKw==

ansi-regex@^5.0.1:
  version "5.0.1"
  resolved "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz"
  integrity sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==

ansi-regex@^6.0.1:
  version "6.1.0"
  resolved "https://registry.npmjs.org/ansi-regex/-/ansi-regex-6.1.0.tgz"
  integrity sha512-7HSX4QQb4CspciLpVFwyRe79O3xsIZDDLER21kERQ71oaPodF8jL725AgJMFAYbooIqolJoRLuM81SpeUkpkvA==

ansi-styles@^4.0.0:
  version "4.3.0"
  resolved "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz"
  integrity sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==
  dependencies:
    color-convert "^2.0.1"

ansi-styles@^6.1.0:
  version "6.2.1"
  resolved "https://registry.npmjs.org/ansi-styles/-/ansi-styles-6.2.1.tgz"
  integrity sha512-bN798gFfQX+viw3R7yrGWRqnrN2oRkEkUjjl4JNn4E8GxxbjtG3FbrEIIY3l8/hrwUwIeCZvi4QuOTP4MErVug==

ansi-styles@^6.2.1:
  version "6.2.1"
  resolved "https://registry.npmjs.org/ansi-styles/-/ansi-styles-6.2.1.tgz"
  integrity sha512-bN798gFfQX+viw3R7yrGWRqnrN2oRkEkUjjl4JNn4E8GxxbjtG3FbrEIIY3l8/hrwUwIeCZvi4QuOTP4MErVug==

any-promise@^1.0.0:
  version "1.3.0"
  resolved "https://registry.npmjs.org/any-promise/-/any-promise-1.3.0.tgz"
  integrity sha512-7UvmKalWRt1wgjL1RrGxoSJW/0QZFIegpeGvZG9kjp8vrRu55XTHbwnqq2GpXm9uLbcuhxm3IqX9OB4MZR1b2A==

anymatch@~3.1.2:
  version "3.1.3"
  resolved "https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz"
  integrity sha512-KMReFUr0B4t+D+OBkjR3KYqvocp2XaSzO55UcB6mgQMd3KbcE+mWTyvVV7D/zsdEbNnV6acZUutkiHQXvTr1Rw==
  dependencies:
    normalize-path "^3.0.0"
    picomatch "^2.0.4"

arg@^5.0.2:
  version "5.0.2"
  resolved "https://registry.npmjs.org/arg/-/arg-5.0.2.tgz"
  integrity sha512-PYjyFOLKQ9y57JvQ6QLo8dAgNqswh8M1RMJYdQduT6xbWSgK36P/Z/v+p888pM69jMMfS8Xd8F6I1kQ/I9HUGg==

asynckit@^0.4.0:
  version "0.4.0"
  resolved "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz"
  integrity sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==

autoprefixer@^10.4.21:
  version "10.4.21"
  resolved "https://registry.npmjs.org/autoprefixer/-/autoprefixer-10.4.21.tgz"
  integrity sha512-O+A6LWV5LDHSJD3LjHYoNi4VLsj/Whi7k6zG12xTYaU4cQ8oxQGckXNX8cRHK5yOZ/ppVHe0ZBXGzSV9jXdVbQ==
  dependencies:
    browserslist "^4.24.4"
    caniuse-lite "^1.0.30001702"
    fraction.js "^4.3.7"
    normalize-range "^0.1.2"
    picocolors "^1.1.1"
    postcss-value-parser "^4.2.0"

axios@^1.8.4:
  version "1.8.4"
  resolved "https://registry.npmjs.org/axios/-/axios-1.8.4.tgz"
  integrity sha512-eBSYY4Y68NNlHbHBMdeDmKNtDgXWhQsJcGqzO3iLUM0GraQFSS9cVgPX5I9b3lbdFKyYoAEGAZF1DwhTaljNAw==
  dependencies:
    follow-redirects "^1.15.6"
    form-data "^4.0.0"
    proxy-from-env "^1.1.0"

balanced-match@^1.0.0:
  version "1.0.2"
  resolved "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz"
  integrity sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==

binary-extensions@^2.0.0:
  version "2.3.0"
  resolved "https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.3.0.tgz"
  integrity sha512-Ceh+7ox5qe7LJuLHoY0feh3pHuUDHAcRUeyL2VYghZwfpkNIy/+8Ocg0a3UuSoYzavmylwuLWQOf3hl0jjMMIw==

birpc@^0.2.19:
  version "0.2.19"
  resolved "https://registry.npmjs.org/birpc/-/birpc-0.2.19.tgz"
  integrity sha512-5WeXXAvTmitV1RqJFppT5QtUiz2p1mRSYU000Jkft5ZUCLJIk4uQriYNO50HknxKwM6jd8utNc66K1qGIwwWBQ==

brace-expansion@^2.0.1:
  version "2.0.1"
  resolved "https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.1.tgz"
  integrity sha512-XnAIvQ8eM+kC6aULx6wuQiwVsnzsi9d3WxzV3FpWTGA19F621kwdbsAcFKXgKUHZWsy+mY6iL1sHTxWEFCytDA==
  dependencies:
    balanced-match "^1.0.0"

braces@^3.0.3, braces@~3.0.2:
  version "3.0.3"
  resolved "https://registry.npmjs.org/braces/-/braces-3.0.3.tgz"
  integrity sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==
  dependencies:
    fill-range "^7.1.1"

browserslist@^4.24.0, browserslist@^4.24.4, "browserslist@>= 4.21.0":
  version "4.24.4"
  resolved "https://registry.npmjs.org/browserslist/-/browserslist-4.24.4.tgz"
  integrity sha512-KDi1Ny1gSePi1vm0q4oxSF8b4DR44GF4BbmS2YdhPLOEqd8pDviZOGH/GsmRwoWJ2+5Lr085X7naowMwKHDG1A==
  dependencies:
    caniuse-lite "^1.0.30001688"
    electron-to-chromium "^1.5.73"
    node-releases "^2.0.19"
    update-browserslist-db "^1.1.1"

bundle-name@^4.1.0:
  version "4.1.0"
  resolved "https://registry.npmjs.org/bundle-name/-/bundle-name-4.1.0.tgz"
  integrity sha512-tjwM5exMg6BGRI+kNmTntNsvdZS1X8BFYS6tnJ2hdH0kVxM6/eVZ2xy+FqStSWvYmtfFMDLIxurorHwDKfDz5Q==
  dependencies:
    run-applescript "^7.0.0"

call-bind-apply-helpers@^1.0.1, call-bind-apply-helpers@^1.0.2:
  version "1.0.2"
  resolved "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz"
  integrity sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==
  dependencies:
    es-errors "^1.3.0"
    function-bind "^1.1.2"

camelcase-css@^2.0.1:
  version "2.0.1"
  resolved "https://registry.npmjs.org/camelcase-css/-/camelcase-css-2.0.1.tgz"
  integrity sha512-QOSvevhslijgYwRx6Rv7zKdMF8lbRmx+uQGx2+vDc+KI/eBnsy9kit5aj23AgGu3pa4t9AgwbnXWqS+iOY+2aA==

camelcase@^5.0.0:
  version "5.3.1"
  resolved "https://registry.npmjs.org/camelcase/-/camelcase-5.3.1.tgz"
  integrity sha512-L28STB170nwWS63UjtlEOE3dldQApaJXZkOI1uMFfzf3rRuPegHaHesyee+YxQ+W6SvRDQV6UrdOdRiR153wJg==

caniuse-lite@^1.0.30001688, caniuse-lite@^1.0.30001702:
  version "1.0.30001707"
  resolved "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001707.tgz"
  integrity sha512-3qtRjw/HQSMlDWf+X79N206fepf4SOOU6SQLMaq/0KkZLmSjPxAkBOQQ+FxbHKfHmYLZFfdWsO3KA90ceHPSnw==

chokidar@^3.6.0:
  version "3.6.0"
  resolved "https://registry.npmjs.org/chokidar/-/chokidar-3.6.0.tgz"
  integrity sha512-7VT13fmjotKpGipCW9JEQAusEPE+Ei8nl6/g4FBAmIm0GOOLMua9NDDo/DWp0ZAxCr3cPq5ZpBqmPAQgDda2Pw==
  dependencies:
    anymatch "~3.1.2"
    braces "~3.0.2"
    glob-parent "~5.1.2"
    is-binary-path "~2.1.0"
    is-glob "~4.0.1"
    normalize-path "~3.0.0"
    readdirp "~3.6.0"
  optionalDependencies:
    fsevents "~2.3.2"

cliui@^6.0.0:
  version "6.0.0"
  resolved "https://registry.npmjs.org/cliui/-/cliui-6.0.0.tgz"
  integrity sha512-t6wbgtoCXvAzst7QgXxJYqPt0usEfbgQdftEPbLL/cvv6HPE5VgvqCuAIDR0NgU52ds6rFwqrgakNLrHEjCbrQ==
  dependencies:
    string-width "^4.2.0"
    strip-ansi "^6.0.0"
    wrap-ansi "^6.2.0"

color-convert@^2.0.1:
  version "2.0.1"
  resolved "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz"
  integrity sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==
  dependencies:
    color-name "~1.1.4"

color-name@~1.1.4:
  version "1.1.4"
  resolved "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz"
  integrity sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==

combined-stream@^1.0.8:
  version "1.0.8"
  resolved "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz"
  integrity sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==
  dependencies:
    delayed-stream "~1.0.0"

commander@^4.0.0:
  version "4.1.1"
  resolved "https://registry.npmjs.org/commander/-/commander-4.1.1.tgz"
  integrity sha512-NOKm8xhkzAjzFx8B2v5OAHT+u5pRQc2UCa2Vq9jYL/31o2wi9mxBA7LIFs3sV5VSC49z6pEhfbMULvShKj26WA==

convert-source-map@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz"
  integrity sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==

copy-anything@^3.0.2:
  version "3.0.5"
  resolved "https://registry.npmjs.org/copy-anything/-/copy-anything-3.0.5.tgz"
  integrity sha512-yCEafptTtb4bk7GLEQoM8KVJpxAfdBJYaXyzQEgQQQgYrZiDp8SJmGKlYza6CYjEDNstAdNdKA3UuoULlEbS6w==
  dependencies:
    is-what "^4.1.8"

cowsay@^1.6.0:
  version "1.6.0"
  resolved "https://registry.npmjs.org/cowsay/-/cowsay-1.6.0.tgz"
  integrity sha512-8C4H1jdrgNusTQr3Yu4SCm+ZKsAlDFbpa0KS0Z3im8ueag+9pGOf3CrioruvmeaW/A5oqg9L0ar6qeftAh03jw==
  dependencies:
    get-stdin "8.0.0"
    string-width "~2.1.1"
    strip-final-newline "2.0.0"
    yargs "15.4.1"

cross-spawn@^7.0.3, cross-spawn@^7.0.6:
  version "7.0.6"
  resolved "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz"
  integrity sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==
  dependencies:
    path-key "^3.1.0"
    shebang-command "^2.0.0"
    which "^2.0.1"

cssesc@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/cssesc/-/cssesc-3.0.0.tgz"
  integrity sha512-/Tb/JcjK111nNScGob5MNtsntNM1aCNUDipB/TkwZFhyDrrE47SOx/18wF2bbjgc3ZzCSKW1T5nt5EbFoAz/Vg==

csstype@^3.1.3:
  version "3.1.3"
  resolved "https://registry.npmjs.org/csstype/-/csstype-3.1.3.tgz"
  integrity sha512-M1uQkMl8rQK/szD0LNhtqxIPLpimGm8sOBwU7lLnCpSbTyY3yeU1Vc7l4KT5zT4s/yOxHH5O7tIuuLOCnLADRw==

de-indent@^1.0.2:
  version "1.0.2"
  resolved "https://registry.npmjs.org/de-indent/-/de-indent-1.0.2.tgz"
  integrity sha512-e/1zu3xH5MQryN2zdVaF0OrdNLUbvWxzMbi+iNA6Bky7l1RoP8a2fIbRocyHclXt/arDrrR6lL3TqFD9pMQTsg==

debug@^4.1.0, debug@^4.3.1, debug@^4.3.7:
  version "4.4.0"
  resolved "https://registry.npmjs.org/debug/-/debug-4.4.0.tgz"
  integrity sha512-6WTZ/IxCY/T6BALoZHaE4ctp9xm+Z5kY/pzYaCHRFeyVhojxlrm+46y68HA6hr0TcwEssoxNiDEUJQjfPZ/RYA==
  dependencies:
    ms "^2.1.3"

decamelize@^1.2.0:
  version "1.2.0"
  resolved "https://registry.npmjs.org/decamelize/-/decamelize-1.2.0.tgz"
  integrity sha512-z2S+W9X73hAUUki+N+9Za2lBlun89zigOyGrsax+KUQ6wKW4ZoWpEYBkGhQjwAjjDCkWxhY0VKEhk8wzY7F5cA==

default-browser-id@^5.0.0:
  version "5.0.0"
  resolved "https://registry.npmjs.org/default-browser-id/-/default-browser-id-5.0.0.tgz"
  integrity sha512-A6p/pu/6fyBcA1TRz/GqWYPViplrftcW2gZC9q79ngNCKAeR/X3gcEdXQHl4KNXV+3wgIJ1CPkJQ3IHM6lcsyA==

default-browser@^5.2.1:
  version "5.2.1"
  resolved "https://registry.npmjs.org/default-browser/-/default-browser-5.2.1.tgz"
  integrity sha512-WY/3TUME0x3KPYdRRxEJJvXRHV4PyPoUsxtZa78lwItwRQRHhd2U9xOscaT/YTf8uCXIAjeJOFBVEh/7FtD8Xg==
  dependencies:
    bundle-name "^4.1.0"
    default-browser-id "^5.0.0"

define-lazy-prop@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/define-lazy-prop/-/define-lazy-prop-3.0.0.tgz"
  integrity sha512-N+MeXYoqr3pOgn8xfyRPREN7gHakLYjhsHhWGT3fWAiL4IkAt0iDw14QiiEm2bE30c5XX5q0FtAA3CK5f9/BUg==

delayed-stream@~1.0.0:
  version "1.0.0"
  resolved "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz"
  integrity sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==

didyoumean@^1.2.2:
  version "1.2.2"
  resolved "https://registry.npmjs.org/didyoumean/-/didyoumean-1.2.2.tgz"
  integrity sha512-gxtyfqMg7GKyhQmb056K7M3xszy/myH8w+B4RT+QXBQsvAOdc3XymqDDPHx1BgPgsdAA5SIifona89YtRATDzw==

dlv@^1.1.3:
  version "1.1.3"
  resolved "https://registry.npmjs.org/dlv/-/dlv-1.1.3.tgz"
  integrity sha512-+HlytyjlPKnIG8XuRG8WvmBP8xs8P71y+SKKS6ZXWoEgLuePxtDoUEiH7WkdePWrQ5JBpE6aoVqfZfJUQkjXwA==

dunder-proto@^1.0.1:
  version "1.0.1"
  resolved "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz"
  integrity sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==
  dependencies:
    call-bind-apply-helpers "^1.0.1"
    es-errors "^1.3.0"
    gopd "^1.2.0"

eastasianwidth@^0.2.0:
  version "0.2.0"
  resolved "https://registry.npmjs.org/eastasianwidth/-/eastasianwidth-0.2.0.tgz"
  integrity sha512-I88TYZWc9XiYHRQ4/3c5rjjfgkjhLyW2luGIheGERbNQ6OY7yTybanSpDXZa8y7VUP9YmDcYa+eyq4ca7iLqWA==

electron-to-chromium@^1.5.73:
  version "1.5.128"
  resolved "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.128.tgz"
  integrity sha512-bo1A4HH/NS522Ws0QNFIzyPcyUUNV/yyy70Ho1xqfGYzPUme2F/xr4tlEOuM6/A538U1vDA7a4XfCd1CKRegKQ==

emoji-regex@^8.0.0:
  version "8.0.0"
  resolved "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz"
  integrity sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==

emoji-regex@^9.2.2:
  version "9.2.2"
  resolved "https://registry.npmjs.org/emoji-regex/-/emoji-regex-9.2.2.tgz"
  integrity sha512-L18DaJsXSUk2+42pv8mLs5jJT2hqFkFE4j21wOmgbUqsZ2hL72NsUU785g9RXgo3s0ZNgVl42TiHp3ZtOv/Vyg==

entities@^4.5.0:
  version "4.5.0"
  resolved "https://registry.npmjs.org/entities/-/entities-4.5.0.tgz"
  integrity sha512-V0hjH4dGPh9Ao5p0MoRY6BVqtwCjhz6vI5LT8AJ55H+4g9/4vbHx1I54fS0XuclLhDHArPQCiMjDxjaL8fPxhw==

error-stack-parser-es@^0.1.5:
  version "0.1.5"
  resolved "https://registry.npmjs.org/error-stack-parser-es/-/error-stack-parser-es-0.1.5.tgz"
  integrity sha512-xHku1X40RO+fO8yJ8Wh2f2rZWVjqyhb1zgq1yZ8aZRQkv6OOKhKWRUaht3eSCUbAOBaKIgM+ykwFLE+QUxgGeg==

es-define-property@^1.0.1:
  version "1.0.1"
  resolved "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz"
  integrity sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==

es-errors@^1.3.0:
  version "1.3.0"
  resolved "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz"
  integrity sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==

es-object-atoms@^1.0.0, es-object-atoms@^1.1.1:
  version "1.1.1"
  resolved "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz"
  integrity sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==
  dependencies:
    es-errors "^1.3.0"

es-set-tostringtag@^2.1.0:
  version "2.1.0"
  resolved "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz"
  integrity sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==
  dependencies:
    es-errors "^1.3.0"
    get-intrinsic "^1.2.6"
    has-tostringtag "^1.0.2"
    hasown "^2.0.2"

esbuild@^0.25.0:
  version "0.25.1"
  resolved "https://registry.npmjs.org/esbuild/-/esbuild-0.25.1.tgz"
  integrity sha512-BGO5LtrGC7vxnqucAe/rmvKdJllfGaYWdyABvyMoXQlfYMb2bbRuReWR5tEGE//4LcNJj9XrkovTqNYRFZHAMQ==
  optionalDependencies:
    "@esbuild/aix-ppc64" "0.25.1"
    "@esbuild/android-arm" "0.25.1"
    "@esbuild/android-arm64" "0.25.1"
    "@esbuild/android-x64" "0.25.1"
    "@esbuild/darwin-arm64" "0.25.1"
    "@esbuild/darwin-x64" "0.25.1"
    "@esbuild/freebsd-arm64" "0.25.1"
    "@esbuild/freebsd-x64" "0.25.1"
    "@esbuild/linux-arm" "0.25.1"
    "@esbuild/linux-arm64" "0.25.1"
    "@esbuild/linux-ia32" "0.25.1"
    "@esbuild/linux-loong64" "0.25.1"
    "@esbuild/linux-mips64el" "0.25.1"
    "@esbuild/linux-ppc64" "0.25.1"
    "@esbuild/linux-riscv64" "0.25.1"
    "@esbuild/linux-s390x" "0.25.1"
    "@esbuild/linux-x64" "0.25.1"
    "@esbuild/netbsd-arm64" "0.25.1"
    "@esbuild/netbsd-x64" "0.25.1"
    "@esbuild/openbsd-arm64" "0.25.1"
    "@esbuild/openbsd-x64" "0.25.1"
    "@esbuild/sunos-x64" "0.25.1"
    "@esbuild/win32-arm64" "0.25.1"
    "@esbuild/win32-ia32" "0.25.1"
    "@esbuild/win32-x64" "0.25.1"

escalade@^3.2.0:
  version "3.2.0"
  resolved "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz"
  integrity sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==

estree-walker@^2.0.2:
  version "2.0.2"
  resolved "https://registry.npmjs.org/estree-walker/-/estree-walker-2.0.2.tgz"
  integrity sha512-Rfkk/Mp/DL7JVje3u18FxFujQlTNR2q6QfMSMB7AvCBx91NGj/ba3kCfza0f6dVDbw7YlRf/nDrn7pQrCCyQ/w==

execa@^9.5.1:
  version "9.5.2"
  resolved "https://registry.npmjs.org/execa/-/execa-9.5.2.tgz"
  integrity sha512-EHlpxMCpHWSAh1dgS6bVeoLAXGnJNdR93aabr4QCGbzOM73o5XmRfM/e5FUqsw3aagP8S8XEWUWFAxnRBnAF0Q==
  dependencies:
    "@sindresorhus/merge-streams" "^4.0.0"
    cross-spawn "^7.0.3"
    figures "^6.1.0"
    get-stream "^9.0.0"
    human-signals "^8.0.0"
    is-plain-obj "^4.1.0"
    is-stream "^4.0.1"
    npm-run-path "^6.0.0"
    pretty-ms "^9.0.0"
    signal-exit "^4.1.0"
    strip-final-newline "^4.0.0"
    yoctocolors "^2.0.0"

fast-glob@^3.3.2:
  version "3.3.3"
  resolved "https://registry.npmjs.org/fast-glob/-/fast-glob-3.3.3.tgz"
  integrity sha512-7MptL8U0cqcFdzIzwOTHoilX9x5BrNqye7Z/LuC7kCMRio1EMSyqRK3BEAUD7sXRq4iT4AzTVuZdhgQ2TCvYLg==
  dependencies:
    "@nodelib/fs.stat" "^2.0.2"
    "@nodelib/fs.walk" "^1.2.3"
    glob-parent "^5.1.2"
    merge2 "^1.3.0"
    micromatch "^4.0.8"

fastq@^1.6.0:
  version "1.19.1"
  resolved "https://registry.npmjs.org/fastq/-/fastq-1.19.1.tgz"
  integrity sha512-GwLTyxkCXjXbxqIhTsMI2Nui8huMPtnxg7krajPJAjnEG/iiOS7i+zCtWGZR9G0NBKbXKh6X9m9UIsYX/N6vvQ==
  dependencies:
    reusify "^1.0.4"

figures@^6.1.0:
  version "6.1.0"
  resolved "https://registry.npmjs.org/figures/-/figures-6.1.0.tgz"
  integrity sha512-d+l3qxjSesT4V7v2fh+QnmFnUWv9lSpjarhShNTgBOfA0ttejbQUAlHLitbjkoRiDulW0OPoQPYIGhIC8ohejg==
  dependencies:
    is-unicode-supported "^2.0.0"

fill-range@^7.1.1:
  version "7.1.1"
  resolved "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz"
  integrity sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==
  dependencies:
    to-regex-range "^5.0.1"

find-up@^4.1.0:
  version "4.1.0"
  resolved "https://registry.npmjs.org/find-up/-/find-up-4.1.0.tgz"
  integrity sha512-PpOwAdQ/YlXQ2vj8a3h8IipDuYRi3wceVQQGYWxNINccq40Anw7BlsEXCMbt1Zt+OLA6Fq9suIpIWD0OsnISlw==
  dependencies:
    locate-path "^5.0.0"
    path-exists "^4.0.0"

follow-redirects@^1.15.6:
  version "1.15.9"
  resolved "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.9.tgz"
  integrity sha512-gew4GsXizNgdoRyqmyfMHyAmXsZDk6mHkSxZFCzW9gwlbtOW44CDtYavM+y+72qD/Vq2l550kMF52DT8fOLJqQ==

foreground-child@^3.1.0:
  version "3.3.1"
  resolved "https://registry.npmjs.org/foreground-child/-/foreground-child-3.3.1.tgz"
  integrity sha512-gIXjKqtFuWEgzFRJA9WCQeSJLZDjgJUOMCMzxtvFq/37KojM1BFGufqsCy0r4qSQmYLsZYMeyRqzIWOMup03sw==
  dependencies:
    cross-spawn "^7.0.6"
    signal-exit "^4.0.1"

form-data@^4.0.0:
  version "4.0.2"
  resolved "https://registry.npmjs.org/form-data/-/form-data-4.0.2.tgz"
  integrity sha512-hGfm/slu0ZabnNt4oaRZ6uREyfCj6P4fT/n6A1rGV+Z0VdGXjfOhVUpkn6qVQONHGIFwmveGXyDs75+nr6FM8w==
  dependencies:
    asynckit "^0.4.0"
    combined-stream "^1.0.8"
    es-set-tostringtag "^2.1.0"
    mime-types "^2.1.12"

fraction.js@^4.3.7:
  version "4.3.7"
  resolved "https://registry.npmjs.org/fraction.js/-/fraction.js-4.3.7.tgz"
  integrity sha512-ZsDfxO51wGAXREY55a7la9LScWpwv9RxIrYABrlvOFBlH/ShPnrtsXeuUIfXKKOVicNxQ+o8JTbJvjS4M89yew==

fs-extra@^11.2.0:
  version "11.3.0"
  resolved "https://registry.npmjs.org/fs-extra/-/fs-extra-11.3.0.tgz"
  integrity sha512-Z4XaCL6dUDHfP/jT25jJKMmtxvuwbkrD1vNSMFlo9lNLY2c5FHYSQgHPRZUjAB26TpDEoW9HCOgplrdbaPV/ew==
  dependencies:
    graceful-fs "^4.2.0"
    jsonfile "^6.0.1"
    universalify "^2.0.0"

fsevents@~2.3.2, fsevents@~2.3.3:
  version "2.3.3"
  resolved "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz"
  integrity sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==

function-bind@^1.1.2:
  version "1.1.2"
  resolved "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz"
  integrity sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==

gensync@^1.0.0-beta.2:
  version "1.0.0-beta.2"
  resolved "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz"
  integrity sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==

get-caller-file@^2.0.1:
  version "2.0.5"
  resolved "https://registry.npmjs.org/get-caller-file/-/get-caller-file-2.0.5.tgz"
  integrity sha512-DyFP3BM/3YHTQOCUL/w0OZHR0lpKeGrxotcHWcqNEdnltqFwXVfhEBQ94eIo34AfQpo0rGki4cyIiftY06h2Fg==

get-intrinsic@^1.2.6:
  version "1.3.0"
  resolved "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz"
  integrity sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==
  dependencies:
    call-bind-apply-helpers "^1.0.2"
    es-define-property "^1.0.1"
    es-errors "^1.3.0"
    es-object-atoms "^1.1.1"
    function-bind "^1.1.2"
    get-proto "^1.0.1"
    gopd "^1.2.0"
    has-symbols "^1.1.0"
    hasown "^2.0.2"
    math-intrinsics "^1.1.0"

get-proto@^1.0.1:
  version "1.0.1"
  resolved "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz"
  integrity sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==
  dependencies:
    dunder-proto "^1.0.1"
    es-object-atoms "^1.0.0"

get-stdin@8.0.0:
  version "8.0.0"
  resolved "https://registry.npmjs.org/get-stdin/-/get-stdin-8.0.0.tgz"
  integrity sha512-sY22aA6xchAzprjyqmSEQv4UbAAzRN0L2dQB0NlN5acTTK9Don6nhoc3eAbUnpZiCANAMfd/+40kVdKfFygohg==

get-stream@^9.0.0:
  version "9.0.1"
  resolved "https://registry.npmjs.org/get-stream/-/get-stream-9.0.1.tgz"
  integrity sha512-kVCxPF3vQM/N0B1PmoqVUqgHP+EeVjmZSQn+1oCRPxd2P21P2F19lIgbR3HBosbB1PUhOAoctJnfEn2GbN2eZA==
  dependencies:
    "@sec-ant/readable-stream" "^0.4.1"
    is-stream "^4.0.1"

glob-parent@^5.1.2, glob-parent@~5.1.2:
  version "5.1.2"
  resolved "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz"
  integrity sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==
  dependencies:
    is-glob "^4.0.1"

glob-parent@^6.0.2:
  version "6.0.2"
  resolved "https://registry.npmjs.org/glob-parent/-/glob-parent-6.0.2.tgz"
  integrity sha512-XxwI8EOhVQgWp6iDL+3b0r86f4d6AX6zSU55HfB4ydCEuXLXc5FcYeOu+nnGftS4TEju/11rt4KJPTMgbfmv4A==
  dependencies:
    is-glob "^4.0.3"

glob@^10.3.10:
  version "10.4.5"
  resolved "https://registry.npmjs.org/glob/-/glob-10.4.5.tgz"
  integrity sha512-7Bv8RF0k6xjo7d4A/PxYLbUCfb6c+Vpd2/mB2yRDlew7Jb5hEXiCD9ibfO7wpk8i4sevK6DFny9h7EYbM3/sHg==
  dependencies:
    foreground-child "^3.1.0"
    jackspeak "^3.1.2"
    minimatch "^9.0.4"
    minipass "^7.1.2"
    package-json-from-dist "^1.0.0"
    path-scurry "^1.11.1"

globals@^11.1.0:
  version "11.12.0"
  resolved "https://registry.npmjs.org/globals/-/globals-11.12.0.tgz"
  integrity sha512-WOBp/EEGUiIsJSp7wcv/y6MO+lV9UoncWqxuFfm8eBwzWNgyfBd6Gz+IeKQ9jCmyhoH99g15M3T+QaVHFjizVA==

gopd@^1.2.0:
  version "1.2.0"
  resolved "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz"
  integrity sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==

graceful-fs@^4.1.6, graceful-fs@^4.2.0:
  version "4.2.11"
  resolved "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.11.tgz"
  integrity sha512-RbJ5/jmFcNNCcDV5o9eTnBLJ/HszWV0P73bc+Ff4nS/rJj+YaS6IGyiOL0VoBYX+l1Wrl3k63h/KrH+nhJ0XvQ==

has-symbols@^1.0.3, has-symbols@^1.1.0:
  version "1.1.0"
  resolved "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz"
  integrity sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==

has-tostringtag@^1.0.2:
  version "1.0.2"
  resolved "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz"
  integrity sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==
  dependencies:
    has-symbols "^1.0.3"

hasown@^2.0.2:
  version "2.0.2"
  resolved "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz"
  integrity sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==
  dependencies:
    function-bind "^1.1.2"

he@^1.2.0:
  version "1.2.0"
  resolved "https://registry.npmjs.org/he/-/he-1.2.0.tgz"
  integrity sha512-F/1DnUGPopORZi0ni+CvrCgHQ5FyEAHRLSApuYWMmrbSwoN2Mn/7k+Gl38gJnR7yyDZk6WLXwiGod1JOWNDKGw==

hookable@^5.5.3:
  version "5.5.3"
  resolved "https://registry.npmjs.org/hookable/-/hookable-5.5.3.tgz"
  integrity sha512-Yc+BQe8SvoXH1643Qez1zqLRmbA5rCL+sSmk6TVos0LWVfNIB7PGncdlId77WzLGSIB5KaWgTaNTs2lNVEI6VQ==

human-signals@^8.0.0:
  version "8.0.1"
  resolved "https://registry.npmjs.org/human-signals/-/human-signals-8.0.1.tgz"
  integrity sha512-eKCa6bwnJhvxj14kZk5NCPc6Hb6BdsU9DZcOnmQKSnO1VKrfV0zCvtttPZUsBvjmNDn8rpcJfpwSYnHBjc95MQ==

is-binary-path@~2.1.0:
  version "2.1.0"
  resolved "https://registry.npmjs.org/is-binary-path/-/is-binary-path-2.1.0.tgz"
  integrity sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==
  dependencies:
    binary-extensions "^2.0.0"

is-core-module@^2.16.0:
  version "2.16.1"
  resolved "https://registry.npmjs.org/is-core-module/-/is-core-module-2.16.1.tgz"
  integrity sha512-UfoeMA6fIJ8wTYFEUjelnaGI67v6+N7qXJEvQuIGa99l4xsCruSYOVSQ0uPANn4dAzm8lkYPaKLrrijLq7x23w==
  dependencies:
    hasown "^2.0.2"

is-docker@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/is-docker/-/is-docker-3.0.0.tgz"
  integrity sha512-eljcgEDlEns/7AXFosB5K/2nCM4P7FQPkGc/DWLy5rmFEWvZayGrik1d9/QIY5nJ4f9YsVvBkA6kJpHn9rISdQ==

is-extglob@^2.1.1:
  version "2.1.1"
  resolved "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz"
  integrity sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==

is-fullwidth-code-point@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-2.0.0.tgz"
  integrity sha512-VHskAKYM8RfSFXwee5t5cbN5PZeq1Wrh6qd5bkyiXIf6UQcN6w/A0eXM9r6t8d+GYOh+o6ZhiEnb88LN/Y8m2w==

is-fullwidth-code-point@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz"
  integrity sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==

is-glob@^4.0.1, is-glob@^4.0.3, is-glob@~4.0.1:
  version "4.0.3"
  resolved "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz"
  integrity sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==
  dependencies:
    is-extglob "^2.1.1"

is-inside-container@^1.0.0:
  version "1.0.0"
  resolved "https://registry.npmjs.org/is-inside-container/-/is-inside-container-1.0.0.tgz"
  integrity sha512-KIYLCCJghfHZxqjYBE7rEy0OBuTd5xCHS7tHVgvCLkx7StIoaxwNW3hCALgEUjFfeRk+MG/Qxmp/vtETEF3tRA==
  dependencies:
    is-docker "^3.0.0"

is-number@^7.0.0:
  version "7.0.0"
  resolved "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz"
  integrity sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==

is-plain-obj@^4.1.0:
  version "4.1.0"
  resolved "https://registry.npmjs.org/is-plain-obj/-/is-plain-obj-4.1.0.tgz"
  integrity sha512-+Pgi+vMuUNkJyExiMBt5IlFoMyKnr5zhJ4Uspz58WOhBF5QoIZkFyNHIbBAtHwzVAgk5RtndVNsDRN61/mmDqg==

is-stream@^4.0.1:
  version "4.0.1"
  resolved "https://registry.npmjs.org/is-stream/-/is-stream-4.0.1.tgz"
  integrity sha512-Dnz92NInDqYckGEUJv689RbRiTSEHCQ7wOVeALbkOz999YpqT46yMRIGtSNl2iCL1waAZSx40+h59NV/EwzV/A==

is-unicode-supported@^2.0.0:
  version "2.1.0"
  resolved "https://registry.npmjs.org/is-unicode-supported/-/is-unicode-supported-2.1.0.tgz"
  integrity sha512-mE00Gnza5EEB3Ds0HfMyllZzbBrmLOX3vfWoj9A9PEnTfratQ/BcaJOuMhnkhjXvb2+FkY3VuHqtAGpTPmglFQ==

is-what@^4.1.8:
  version "4.1.16"
  resolved "https://registry.npmjs.org/is-what/-/is-what-4.1.16.tgz"
  integrity sha512-ZhMwEosbFJkA0YhFnNDgTM4ZxDRsS6HqTo7qsZM08fehyRYIYa0yHu5R6mgo1n/8MgaPBXiPimPD77baVFYg+A==

is-wsl@^3.1.0:
  version "3.1.0"
  resolved "https://registry.npmjs.org/is-wsl/-/is-wsl-3.1.0.tgz"
  integrity sha512-UcVfVfaK4Sc4m7X3dUSoHoozQGBEFeDC+zVo06t98xe8CzHSZZBekNXH+tu0NalHolcJ/QAGqS46Hef7QXBIMw==
  dependencies:
    is-inside-container "^1.0.0"

isexe@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz"
  integrity sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==

isexe@^3.1.1:
  version "3.1.1"
  resolved "https://registry.npmjs.org/isexe/-/isexe-3.1.1.tgz"
  integrity sha512-LpB/54B+/2J5hqQ7imZHfdU31OlgQqx7ZicVlkm9kzg9/w8GKLEcFfJl/t7DCEDueOyBAD6zCCwTO6Fzs0NoEQ==

jackspeak@^3.1.2:
  version "3.4.3"
  resolved "https://registry.npmjs.org/jackspeak/-/jackspeak-3.4.3.tgz"
  integrity sha512-OGlZQpz2yfahA/Rd1Y8Cd9SIEsqvXkLVoSw/cgwhnhFMDbsQFeZYoJJ7bIZBS9BcamUW96asq/npPWugM+RQBw==
  dependencies:
    "@isaacs/cliui" "^8.0.2"
  optionalDependencies:
    "@pkgjs/parseargs" "^0.11.0"

jiti@^1.21.6, jiti@>=1.21.0:
  version "1.21.7"
  resolved "https://registry.npmjs.org/jiti/-/jiti-1.21.7.tgz"
  integrity sha512-/imKNG4EbWNrVjoNC/1H5/9GFy+tqjGBHCaSsN+P2RnPqjsLmv6UD3Ej+Kj8nBWaRAwyk7kK5ZUc+OEatnTR3A==

js-tokens@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz"
  integrity sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==

jsesc@^3.0.2:
  version "3.1.0"
  resolved "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz"
  integrity sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==

json-parse-even-better-errors@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/json-parse-even-better-errors/-/json-parse-even-better-errors-4.0.0.tgz"
  integrity sha512-lR4MXjGNgkJc7tkQ97kb2nuEMnNCyU//XYVH0MKTGcXEiSudQ5MKGKen3C5QubYy0vmq+JGitUg92uuywGEwIA==

json5@^2.2.3:
  version "2.2.3"
  resolved "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz"
  integrity sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==

jsonfile@^6.0.1:
  version "6.1.0"
  resolved "https://registry.npmjs.org/jsonfile/-/jsonfile-6.1.0.tgz"
  integrity sha512-5dgndWOriYSm5cnYaJNhalLNDKOqFwyDB/rr1E9ZsGciGvKPs8R2xYGCacuf3z6K1YKDz182fd+fY3cn3pMqXQ==
  dependencies:
    universalify "^2.0.0"
  optionalDependencies:
    graceful-fs "^4.1.6"

kolorist@^1.8.0:
  version "1.8.0"
  resolved "https://registry.npmjs.org/kolorist/-/kolorist-1.8.0.tgz"
  integrity sha512-Y+60/zizpJ3HRH8DCss+q95yr6145JXZo46OTpFvDZWLfRCE4qChOyk1b26nMaNpfHHgxagk9dXT5OP0Tfe+dQ==

lilconfig@^3.0.0, lilconfig@^3.1.3:
  version "3.1.3"
  resolved "https://registry.npmjs.org/lilconfig/-/lilconfig-3.1.3.tgz"
  integrity sha512-/vlFKAoH5Cgt3Ie+JLhRbwOsCQePABiU3tJ1egGvyQ+33R/vcwM2Zl2QR/LzjsBeItPt3oSVXapn+m4nQDvpzw==

lines-and-columns@^1.1.6:
  version "1.2.4"
  resolved "https://registry.npmjs.org/lines-and-columns/-/lines-and-columns-1.2.4.tgz"
  integrity sha512-7ylylesZQ/PV29jhEDl3Ufjo6ZX7gCqJr5F7PKrqc93v7fzSymt1BpwEU8nAUXs8qzzvqhbjhK5QZg6Mt/HkBg==

locate-path@^5.0.0:
  version "5.0.0"
  resolved "https://registry.npmjs.org/locate-path/-/locate-path-5.0.0.tgz"
  integrity sha512-t7hw9pI+WvuwNJXwk5zVHpyhIqzg2qTlklJOf0mVxGSbe3Fp2VieZcduNYjaLDoy6p9uGpQEGWG87WpMKlNq8g==
  dependencies:
    p-locate "^4.1.0"

lru-cache@^10.2.0:
  version "10.4.3"
  resolved "https://registry.npmjs.org/lru-cache/-/lru-cache-10.4.3.tgz"
  integrity sha512-JNAzZcXrCt42VGLuYz0zfAzDfAvJWW6AfYlDBQyDV5DClI2m5sAmK+OIO7s59XfsRsWHp02jAJrRadPRGTt6SQ==

lru-cache@^5.1.1:
  version "5.1.1"
  resolved "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz"
  integrity sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==
  dependencies:
    yallist "^3.0.2"

magic-string@^0.30.11, magic-string@^0.30.4:
  version "0.30.17"
  resolved "https://registry.npmjs.org/magic-string/-/magic-string-0.30.17.tgz"
  integrity sha512-sNPKHvyjVf7gyjwS4xGTaW/mCnF8wnjtifKBEhxfZ7E/S8tQ0rssrwGNn6q8JH/ohItJfSQp9mBtQYuTlH5QnA==
  dependencies:
    "@jridgewell/sourcemap-codec" "^1.5.0"

math-intrinsics@^1.1.0:
  version "1.1.0"
  resolved "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz"
  integrity sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==

memorystream@^0.3.1:
  version "0.3.1"
  resolved "https://registry.npmjs.org/memorystream/-/memorystream-0.3.1.tgz"
  integrity sha512-S3UwM3yj5mtUSEfP41UZmt/0SCoVYUcU1rkXv+BQ5Ig8ndL4sPoJNBUJERafdPb5jjHJGuMgytgKvKIf58XNBw==

merge2@^1.3.0:
  version "1.4.1"
  resolved "https://registry.npmjs.org/merge2/-/merge2-1.4.1.tgz"
  integrity sha512-8q7VEgMJW4J8tcfVPy8g09NcQwZdbwFEqhe/WZkoIzjn/3TGDwtOCYtXGxA3O8tPzpczCCDgv+P2P5y00ZJOOg==

micromatch@^4.0.8:
  version "4.0.8"
  resolved "https://registry.npmjs.org/micromatch/-/micromatch-4.0.8.tgz"
  integrity sha512-PXwfBhYu0hBCPw8Dn0E+WDYb7af3dSLVWKi3HGv84IdF4TyFoC0ysxFd0Goxw7nSv4T/PzEJQxsYsEiFCKo2BA==
  dependencies:
    braces "^3.0.3"
    picomatch "^2.3.1"

mime-db@1.52.0:
  version "1.52.0"
  resolved "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz"
  integrity sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==

mime-types@^2.1.12:
  version "2.1.35"
  resolved "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz"
  integrity sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==
  dependencies:
    mime-db "1.52.0"

minimatch@^9.0.0, minimatch@^9.0.3, minimatch@^9.0.4:
  version "9.0.5"
  resolved "https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz"
  integrity sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==
  dependencies:
    brace-expansion "^2.0.1"

"minipass@^5.0.0 || ^6.0.2 || ^7.0.0", minipass@^7.1.2:
  version "7.1.2"
  resolved "https://registry.npmjs.org/minipass/-/minipass-7.1.2.tgz"
  integrity sha512-qOOzS1cBTWYF4BH8fVePDBOO9iptMnGUEZwNc/cMWnTV2nVLZ7VoNWEPHkYczZA0pdoA7dl6e7FL659nX9S2aw==

mitt@^3.0.1:
  version "3.0.1"
  resolved "https://registry.npmjs.org/mitt/-/mitt-3.0.1.tgz"
  integrity sha512-vKivATfr97l2/QBCYAkXYDbrIWPM2IIKEl7YPhjCvKlG3kE2gm+uBo6nEXK3M5/Ffh/FLpKExzOQ3JJoJGFKBw==

mrmime@^2.0.0:
  version "2.0.1"
  resolved "https://registry.npmjs.org/mrmime/-/mrmime-2.0.1.tgz"
  integrity sha512-Y3wQdFg2Va6etvQ5I82yUhGdsKrcYox6p7FfL1LbK2J4V01F9TGlepTIhnK24t7koZibmg82KGglhA1XK5IsLQ==

ms@^2.1.3:
  version "2.1.3"
  resolved "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz"
  integrity sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==

muggle-string@^0.4.1:
  version "0.4.1"
  resolved "https://registry.npmjs.org/muggle-string/-/muggle-string-0.4.1.tgz"
  integrity sha512-VNTrAak/KhO2i8dqqnqnAHOa3cYBwXEZe9h+D5h/1ZqFSTEFHdM65lR7RoIqq3tBBYavsOXV84NoHXZ0AkPyqQ==

mz@^2.7.0:
  version "2.7.0"
  resolved "https://registry.npmjs.org/mz/-/mz-2.7.0.tgz"
  integrity sha512-z81GNO7nnYMEhrGh9LeymoE4+Yr0Wn5McHIZMK5cfQCl+NDX08sCZgUc9/6MHni9IWuFLm1Z3HTCXu2z9fN62Q==
  dependencies:
    any-promise "^1.0.0"
    object-assign "^4.0.1"
    thenify-all "^1.0.0"

nanoid@^3.3.8:
  version "3.3.11"
  resolved "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz"
  integrity sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==

nanoid@^5.0.9:
  version "5.1.5"
  resolved "https://registry.npmjs.org/nanoid/-/nanoid-5.1.5.tgz"
  integrity sha512-Ir/+ZpE9fDsNH0hQ3C68uyThDXzYcim2EqcZ8zn8Chtt1iylPT9xXJB0kPCnqzgcEGikO9RxSrh63MsmVCU7Fw==

node-releases@^2.0.19:
  version "2.0.19"
  resolved "https://registry.npmjs.org/node-releases/-/node-releases-2.0.19.tgz"
  integrity sha512-xxOWJsBKtzAq7DY0J+DTzuz58K8e7sJbdgwkbMWQe8UYB6ekmsQ45q0M/tJDsGaZmbC+l7n57UV8Hl5tHxO9uw==

normalize-path@^3.0.0, normalize-path@~3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz"
  integrity sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==

normalize-range@^0.1.2:
  version "0.1.2"
  resolved "https://registry.npmjs.org/normalize-range/-/normalize-range-0.1.2.tgz"
  integrity sha512-bdok/XvKII3nUpklnV6P2hxtMNrCboOjAcyBuQnWEhO665FwrSNRxU+AqpsyvO6LgGYPspN+lu5CLtw4jPRKNA==

npm-normalize-package-bin@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/npm-normalize-package-bin/-/npm-normalize-package-bin-4.0.0.tgz"
  integrity sha512-TZKxPvItzai9kN9H/TkmCtx/ZN/hvr3vUycjlfmH0ootY9yFBzNOpiXAdIn1Iteqsvk4lQn6B5PTrt+n6h8k/w==

npm-run-all2@^7.0.2:
  version "7.0.2"
  resolved "https://registry.npmjs.org/npm-run-all2/-/npm-run-all2-7.0.2.tgz"
  integrity sha512-7tXR+r9hzRNOPNTvXegM+QzCuMjzUIIq66VDunL6j60O4RrExx32XUhlrS7UK4VcdGw5/Wxzb3kfNcFix9JKDA==
  dependencies:
    ansi-styles "^6.2.1"
    cross-spawn "^7.0.6"
    memorystream "^0.3.1"
    minimatch "^9.0.0"
    pidtree "^0.6.0"
    read-package-json-fast "^4.0.0"
    shell-quote "^1.7.3"
    which "^5.0.0"

npm-run-path@^6.0.0:
  version "6.0.0"
  resolved "https://registry.npmjs.org/npm-run-path/-/npm-run-path-6.0.0.tgz"
  integrity sha512-9qny7Z9DsQU8Ou39ERsPU4OZQlSTP47ShQzuKZ6PRXpYLtIFgl/DEBYEXKlvcEa+9tHVcK8CF81Y2V72qaZhWA==
  dependencies:
    path-key "^4.0.0"
    unicorn-magic "^0.3.0"

object-assign@^4.0.1:
  version "4.1.1"
  resolved "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz"
  integrity sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==

object-hash@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/object-hash/-/object-hash-3.0.0.tgz"
  integrity sha512-RSn9F68PjH9HqtltsSnqYC1XXoWe9Bju5+213R98cNGttag9q9yAOTzdbsqvIa7aNm5WffBZFpWYr2aWrklWAw==

open@^10.1.0:
  version "10.1.0"
  resolved "https://registry.npmjs.org/open/-/open-10.1.0.tgz"
  integrity sha512-mnkeQ1qP5Ue2wd+aivTD3NHd/lZ96Lu0jgf0pwktLPtx6cTZiH7tyeGRRHs0zX0rbrahXPnXlUnbeXyaBBuIaw==
  dependencies:
    default-browser "^5.2.1"
    define-lazy-prop "^3.0.0"
    is-inside-container "^1.0.0"
    is-wsl "^3.1.0"

p-limit@^2.2.0:
  version "2.3.0"
  resolved "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz"
  integrity sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==
  dependencies:
    p-try "^2.0.0"

p-locate@^4.1.0:
  version "4.1.0"
  resolved "https://registry.npmjs.org/p-locate/-/p-locate-4.1.0.tgz"
  integrity sha512-R79ZZ/0wAxKGu3oYMlz8jy/kbhsNrS7SKZ7PxEHBgJ5+F2mtFW2fK2cOtBh1cHYkQsbzFV7I+EoRKe6Yt0oK7A==
  dependencies:
    p-limit "^2.2.0"

p-try@^2.0.0:
  version "2.2.0"
  resolved "https://registry.npmjs.org/p-try/-/p-try-2.2.0.tgz"
  integrity sha512-R4nPAVTAU0B9D35/Gk3uJf/7XYbQcyohSKdvAxIRSNghFl4e71hVoGnBNQz9cWaXxO2I10KTC+3jMdvvoKw6dQ==

package-json-from-dist@^1.0.0:
  version "1.0.1"
  resolved "https://registry.npmjs.org/package-json-from-dist/-/package-json-from-dist-1.0.1.tgz"
  integrity sha512-UEZIS3/by4OC8vL3P2dTXRETpebLI2NiI5vIrjaD/5UtrkFX/tNbwjTSRAGC/+7CAo2pIcBaRgWmcBBHcsaCIw==

parse-ms@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/parse-ms/-/parse-ms-4.0.0.tgz"
  integrity sha512-TXfryirbmq34y8QBwgqCVLi+8oA3oWx2eAnSn62ITyEhEYaWRlVZ2DvMM9eZbMs/RfxPu/PK/aBLyGj4IrqMHw==

path-browserify@^1.0.1:
  version "1.0.1"
  resolved "https://registry.npmjs.org/path-browserify/-/path-browserify-1.0.1.tgz"
  integrity sha512-b7uo2UCUOYZcnF/3ID0lulOJi/bafxa1xPe7ZPsammBSpjSWQkjNxlt635YGS2MiR9GjvuXCtz2emr3jbsz98g==

path-exists@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz"
  integrity sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==

path-key@^3.1.0:
  version "3.1.1"
  resolved "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz"
  integrity sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==

path-key@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/path-key/-/path-key-4.0.0.tgz"
  integrity sha512-haREypq7xkM7ErfgIyA0z+Bj4AGKlMSdlQE2jvJo6huWD1EdkKYV+G/T4nq0YEF2vgTT8kqMFKo1uHn950r4SQ==

path-parse@^1.0.7:
  version "1.0.7"
  resolved "https://registry.npmjs.org/path-parse/-/path-parse-1.0.7.tgz"
  integrity sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==

path-scurry@^1.11.1:
  version "1.11.1"
  resolved "https://registry.npmjs.org/path-scurry/-/path-scurry-1.11.1.tgz"
  integrity sha512-Xa4Nw17FS9ApQFJ9umLiJS4orGjm7ZzwUrwamcGQuHSzDyth9boKDaycYdDcZDuqYATXw4HFXgaqWTctW/v1HA==
  dependencies:
    lru-cache "^10.2.0"
    minipass "^5.0.0 || ^6.0.2 || ^7.0.0"

pathe@^2.0.2:
  version "2.0.3"
  resolved "https://registry.npmjs.org/pathe/-/pathe-2.0.3.tgz"
  integrity sha512-WUjGcAqP1gQacoQe+OBJsFA7Ld4DyXuUIjZ5cc75cLHvJ7dtNsTugphxIADwspS+AraAUePCKrSVtPLFj/F88w==

perfect-debounce@^1.0.0:
  version "1.0.0"
  resolved "https://registry.npmjs.org/perfect-debounce/-/perfect-debounce-1.0.0.tgz"
  integrity sha512-xCy9V055GLEqoFaHoC1SoLIaLmWctgCUaBaWxDZ7/Zx4CTyX7cJQLJOok/orfjZAh9kEYpjJa4d0KcJmCbctZA==

picocolors@^1.0.0, picocolors@^1.1.1:
  version "1.1.1"
  resolved "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz"
  integrity sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==

picomatch@^2.0.4, picomatch@^2.2.1, picomatch@^2.3.1:
  version "2.3.1"
  resolved "https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz"
  integrity sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==

picomatch@^4.0.2:
  version "4.0.2"
  resolved "https://registry.npmjs.org/picomatch/-/picomatch-4.0.2.tgz"
  integrity sha512-M7BAV6Rlcy5u+m6oPhAPFgJTzAioX/6B0DxyvDlo9l8+T3nLKbrczg2WLUyzd45L8RqfUMyGPzekbMvX2Ldkwg==

pidtree@^0.6.0:
  version "0.6.0"
  resolved "https://registry.npmjs.org/pidtree/-/pidtree-0.6.0.tgz"
  integrity sha512-eG2dWTVw5bzqGRztnHExczNxt5VGsE6OwTeCG3fdUf9KBsZzO3R5OIIIzWR+iZA0NtZ+RDVdaoE2dK1cn6jH4g==

pify@^2.3.0:
  version "2.3.0"
  resolved "https://registry.npmjs.org/pify/-/pify-2.3.0.tgz"
  integrity sha512-udgsAY+fTnvv7kI7aaxbqwWNb0AHiB0qBO89PZKPkoTmGOgdbrHDKD+0B2X4uTfJ/FT1R09r9gTsjUjNJotuog==

pinia@^3.0.1:
  version "3.0.1"
  resolved "https://registry.npmjs.org/pinia/-/pinia-3.0.1.tgz"
  integrity sha512-WXglsDzztOTH6IfcJ99ltYZin2mY8XZCXujkYWVIJlBjqsP6ST7zw+Aarh63E1cDVYeyUcPCxPHzJpEOmzB6Wg==
  dependencies:
    "@vue/devtools-api" "^7.7.2"

pirates@^4.0.1:
  version "4.0.7"
  resolved "https://registry.npmjs.org/pirates/-/pirates-4.0.7.tgz"
  integrity sha512-TfySrs/5nm8fQJDcBDuUng3VOUKsd7S+zqvbOTiGXHfxX4wK31ard+hoNuvkicM/2YFzlpDgABOevKSsB4G/FA==

postcss-import@^15.1.0:
  version "15.1.0"
  resolved "https://registry.npmjs.org/postcss-import/-/postcss-import-15.1.0.tgz"
  integrity sha512-hpr+J05B2FVYUAXHeK1YyI267J/dDDhMU6B6civm8hSY1jYJnBXxzKDKDswzJmtLHryrjhnDjqqp/49t8FALew==
  dependencies:
    postcss-value-parser "^4.0.0"
    read-cache "^1.0.0"
    resolve "^1.1.7"

postcss-js@^4.0.1:
  version "4.0.1"
  resolved "https://registry.npmjs.org/postcss-js/-/postcss-js-4.0.1.tgz"
  integrity sha512-dDLF8pEO191hJMtlHFPRa8xsizHaM82MLfNkUHdUtVEV3tgTp5oj+8qbEqYM57SLfc74KSbw//4SeJma2LRVIw==
  dependencies:
    camelcase-css "^2.0.1"

postcss-load-config@^4.0.2:
  version "4.0.2"
  resolved "https://registry.npmjs.org/postcss-load-config/-/postcss-load-config-4.0.2.tgz"
  integrity sha512-bSVhyJGL00wMVoPUzAVAnbEoWyqRxkjv64tUl427SKnPrENtq6hJwUojroMz2VB+Q1edmi4IfrAPpami5VVgMQ==
  dependencies:
    lilconfig "^3.0.0"
    yaml "^2.3.4"

postcss-nested@^6.2.0:
  version "6.2.0"
  resolved "https://registry.npmjs.org/postcss-nested/-/postcss-nested-6.2.0.tgz"
  integrity sha512-HQbt28KulC5AJzG+cZtj9kvKB93CFCdLvog1WFLf1D+xmMvPGlBstkpTEZfK5+AN9hfJocyBFCNiqyS48bpgzQ==
  dependencies:
    postcss-selector-parser "^6.1.1"

postcss-selector-parser@^6.1.1, postcss-selector-parser@^6.1.2:
  version "6.1.2"
  resolved "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-6.1.2.tgz"
  integrity sha512-Q8qQfPiZ+THO/3ZrOrO0cJJKfpYCagtMUkXbnEfmgUjwXg6z/WBeOyS9APBBPCTSiDV+s4SwQGu8yFsiMRIudg==
  dependencies:
    cssesc "^3.0.0"
    util-deprecate "^1.0.2"

postcss-value-parser@^4.0.0, postcss-value-parser@^4.2.0:
  version "4.2.0"
  resolved "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.2.0.tgz"
  integrity sha512-1NNCs6uurfkVbeXG4S8JFT9t19m45ICnif8zWLd5oPSZ50QnwMfK+H3jv408d4jw/7Bttv5axS5IiHoLaVNHeQ==

postcss@^8.0.0, postcss@^8.1.0, postcss@^8.2.14, postcss@^8.4.21, postcss@^8.4.47, postcss@^8.4.48, postcss@^8.5.3, postcss@>=8.0.9:
  version "8.5.3"
  resolved "https://registry.npmjs.org/postcss/-/postcss-8.5.3.tgz"
  integrity sha512-dle9A3yYxlBSrt8Fu+IpjGT8SY8hN0mlaA6GY8t0P5PjIOZemULz/E2Bnm/2dcUOena75OTNkHI76uZBNUUq3A==
  dependencies:
    nanoid "^3.3.8"
    picocolors "^1.1.1"
    source-map-js "^1.2.1"

pretty-ms@^9.0.0:
  version "9.2.0"
  resolved "https://registry.npmjs.org/pretty-ms/-/pretty-ms-9.2.0.tgz"
  integrity sha512-4yf0QO/sllf/1zbZWYnvWw3NxCQwLXKzIj0G849LSufP15BXKM0rbD2Z3wVnkMfjdn/CB0Dpp444gYAACdsplg==
  dependencies:
    parse-ms "^4.0.0"

proxy-from-env@^1.1.0:
  version "1.1.0"
  resolved "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz"
  integrity sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==

queue-microtask@^1.2.2:
  version "1.2.3"
  resolved "https://registry.npmjs.org/queue-microtask/-/queue-microtask-1.2.3.tgz"
  integrity sha512-NuaNSa6flKT5JaSYQzJok04JzTL1CA6aGhv5rfLW3PgqA+M2ChpZQnAC8h8i4ZFkBS8X5RqkDBHA7r4hej3K9A==

read-cache@^1.0.0:
  version "1.0.0"
  resolved "https://registry.npmjs.org/read-cache/-/read-cache-1.0.0.tgz"
  integrity sha512-Owdv/Ft7IjOgm/i0xvNDZ1LrRANRfew4b2prF3OWMQLxLfu3bS8FVhCsrSCMK4lR56Y9ya+AThoTpDCTxCmpRA==
  dependencies:
    pify "^2.3.0"

read-package-json-fast@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/read-package-json-fast/-/read-package-json-fast-4.0.0.tgz"
  integrity sha512-qpt8EwugBWDw2cgE2W+/3oxC+KTez2uSVR8JU9Q36TXPAGCaozfQUs59v4j4GFpWTaw0i6hAZSvOmu1J0uOEUg==
  dependencies:
    json-parse-even-better-errors "^4.0.0"
    npm-normalize-package-bin "^4.0.0"

readdirp@~3.6.0:
  version "3.6.0"
  resolved "https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz"
  integrity sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==
  dependencies:
    picomatch "^2.2.1"

require-directory@^2.1.1:
  version "2.1.1"
  resolved "https://registry.npmjs.org/require-directory/-/require-directory-2.1.1.tgz"
  integrity sha512-fGxEI7+wsG9xrvdjsrlmL22OMTTiHRwAMroiEeMgq8gzoLC/PQr7RsRDSTLUg/bZAZtF+TVIkHc6/4RIKrui+Q==

require-main-filename@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/require-main-filename/-/require-main-filename-2.0.0.tgz"
  integrity sha512-NKN5kMDylKuldxYLSUfrbo5Tuzh4hd+2E8NPPX02mZtn1VuREQToYe/ZdlJy+J3uCpfaiGF05e7B8W0iXbQHmg==

resolve@^1.1.7, resolve@^1.22.8:
  version "1.22.10"
  resolved "https://registry.npmjs.org/resolve/-/resolve-1.22.10.tgz"
  integrity sha512-NPRy+/ncIMeDlTAsuqwKIiferiawhefFJtkNSW0qZJEqMEb+qBt/77B/jGeeek+F0uOeN05CDa6HXbbIgtVX4w==
  dependencies:
    is-core-module "^2.16.0"
    path-parse "^1.0.7"
    supports-preserve-symlinks-flag "^1.0.0"

reusify@^1.0.4:
  version "1.1.0"
  resolved "https://registry.npmjs.org/reusify/-/reusify-1.1.0.tgz"
  integrity sha512-g6QUff04oZpHs0eG5p83rFLhHeV00ug/Yf9nZM6fLeUrPguBTkTQOdpAWWspMh55TZfVQDPaN3NQJfbVRAxdIw==

rfdc@^1.4.1:
  version "1.4.1"
  resolved "https://registry.npmjs.org/rfdc/-/rfdc-1.4.1.tgz"
  integrity sha512-q1b3N5QkRUWUl7iyylaaj3kOpIT0N2i9MqIEQXP73GVsN9cw3fdx8X63cEmWhJGi2PPCF23Ijp7ktmd39rawIA==

rollup@^1.20.0||^2.0.0||^3.0.0||^4.0.0, rollup@^4.30.1:
  version "4.38.0"
  resolved "https://registry.npmjs.org/rollup/-/rollup-4.38.0.tgz"
  integrity sha512-5SsIRtJy9bf1ErAOiFMFzl64Ex9X5V7bnJ+WlFMb+zmP459OSWCEG7b0ERZ+PEU7xPt4OG3RHbrp1LJlXxYTrw==
  dependencies:
    "@types/estree" "1.0.7"
  optionalDependencies:
    "@rollup/rollup-android-arm-eabi" "4.38.0"
    "@rollup/rollup-android-arm64" "4.38.0"
    "@rollup/rollup-darwin-arm64" "4.38.0"
    "@rollup/rollup-darwin-x64" "4.38.0"
    "@rollup/rollup-freebsd-arm64" "4.38.0"
    "@rollup/rollup-freebsd-x64" "4.38.0"
    "@rollup/rollup-linux-arm-gnueabihf" "4.38.0"
    "@rollup/rollup-linux-arm-musleabihf" "4.38.0"
    "@rollup/rollup-linux-arm64-gnu" "4.38.0"
    "@rollup/rollup-linux-arm64-musl" "4.38.0"
    "@rollup/rollup-linux-loongarch64-gnu" "4.38.0"
    "@rollup/rollup-linux-powerpc64le-gnu" "4.38.0"
    "@rollup/rollup-linux-riscv64-gnu" "4.38.0"
    "@rollup/rollup-linux-riscv64-musl" "4.38.0"
    "@rollup/rollup-linux-s390x-gnu" "4.38.0"
    "@rollup/rollup-linux-x64-gnu" "4.38.0"
    "@rollup/rollup-linux-x64-musl" "4.38.0"
    "@rollup/rollup-win32-arm64-msvc" "4.38.0"
    "@rollup/rollup-win32-ia32-msvc" "4.38.0"
    "@rollup/rollup-win32-x64-msvc" "4.38.0"
    fsevents "~2.3.2"

run-applescript@^7.0.0:
  version "7.0.0"
  resolved "https://registry.npmjs.org/run-applescript/-/run-applescript-7.0.0.tgz"
  integrity sha512-9by4Ij99JUr/MCFBUkDKLWK3G9HVXmabKz9U5MlIAIuvuzkiOicRYs8XJLxX+xahD+mLiiCYDqF9dKAgtzKP1A==

run-parallel@^1.1.9:
  version "1.2.0"
  resolved "https://registry.npmjs.org/run-parallel/-/run-parallel-1.2.0.tgz"
  integrity sha512-5l4VyZR86LZ/lDxZTR6jqL8AFE2S0IFLMP26AbjsLVADxHdhB/c0GUsH+y39UfCi3dzz8OlQuPmnaJOMoDHQBA==
  dependencies:
    queue-microtask "^1.2.2"

semver@^6.3.1:
  version "6.3.1"
  resolved "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz"
  integrity sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==

set-blocking@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/set-blocking/-/set-blocking-2.0.0.tgz"
  integrity sha512-KiKBS8AnWGEyLzofFfmvKwpdPzqiy16LvQfK3yv/fVH7Bj13/wl3JSR1J+rfgRE9q7xUJK4qvgS8raSOeLUehw==

shebang-command@^2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz"
  integrity sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==
  dependencies:
    shebang-regex "^3.0.0"

shebang-regex@^3.0.0:
  version "3.0.0"
  resolved "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz"
  integrity sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==

shell-quote@^1.7.3:
  version "1.8.2"
  resolved "https://registry.npmjs.org/shell-quote/-/shell-quote-1.8.2.tgz"
  integrity sha512-AzqKpGKjrj7EM6rKVQEPpB288oCfnrEIuyoT9cyF4nmGa7V8Zk6f7RRqYisX8X9m+Q7bd632aZW4ky7EhbQztA==

signal-exit@^4.0.1, signal-exit@^4.1.0:
  version "4.1.0"
  resolved "https://registry.npmjs.org/signal-exit/-/signal-exit-4.1.0.tgz"
  integrity sha512-bzyZ1e88w9O1iNJbKnOlvYTrWPDl46O1bG0D3XInv+9tkPrxrN8jUUTiFlDkkmKWgn1M6CfIA13SuGqOa9Korw==

sirv@^3.0.0:
  version "3.0.1"
  resolved "https://registry.npmjs.org/sirv/-/sirv-3.0.1.tgz"
  integrity sha512-FoqMu0NCGBLCcAkS1qA+XJIQTR6/JHfQXl+uGteNCQ76T91DMUjPa9xfmeqMY3z80nLSg9yQmNjK0Px6RWsH/A==
  dependencies:
    "@polka/url" "^1.0.0-next.24"
    mrmime "^2.0.0"
    totalist "^3.0.0"

source-map-js@^1.2.0, source-map-js@^1.2.1:
  version "1.2.1"
  resolved "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz"
  integrity sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==

speakingurl@^14.0.1:
  version "14.0.1"
  resolved "https://registry.npmjs.org/speakingurl/-/speakingurl-14.0.1.tgz"
  integrity sha512-1POYv7uv2gXoyGFpBCmpDVSNV74IfsWlDW216UPjbWufNf+bSU6GdbDsxdcxtfwb4xlI3yxzOTKClUosxARYrQ==

"string-width-cjs@npm:string-width@^4.2.0":
  version "4.2.3"
  resolved "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz"
  integrity sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==
  dependencies:
    emoji-regex "^8.0.0"
    is-fullwidth-code-point "^3.0.0"
    strip-ansi "^6.0.1"

string-width@^4.1.0, string-width@^4.2.0:
  version "4.2.3"
  resolved "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz"
  integrity sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==
  dependencies:
    emoji-regex "^8.0.0"
    is-fullwidth-code-point "^3.0.0"
    strip-ansi "^6.0.1"

string-width@^5.0.1, string-width@^5.1.2:
  version "5.1.2"
  resolved "https://registry.npmjs.org/string-width/-/string-width-5.1.2.tgz"
  integrity sha512-HnLOCR3vjcY8beoNLtcjZ5/nxn2afmME6lhrDrebokqMap+XbeW8n9TXpPDOqdGK5qcI3oT0GKTW6wC7EMiVqA==
  dependencies:
    eastasianwidth "^0.2.0"
    emoji-regex "^9.2.2"
    strip-ansi "^7.0.1"

string-width@~2.1.1:
  version "2.1.1"
  resolved "https://registry.npmjs.org/string-width/-/string-width-2.1.1.tgz"
  integrity sha512-nOqH59deCq9SRHlxq1Aw85Jnt4w6KvLKqWVik6oA9ZklXLNIOlqg4F2yrT1MVaTjAqvVwdfeZ7w7aCvJD7ugkw==
  dependencies:
    is-fullwidth-code-point "^2.0.0"
    strip-ansi "^4.0.0"

"strip-ansi-cjs@npm:strip-ansi@^6.0.1":
  version "6.0.1"
  resolved "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz"
  integrity sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==
  dependencies:
    ansi-regex "^5.0.1"

strip-ansi@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/strip-ansi/-/strip-ansi-4.0.0.tgz"
  integrity sha512-4XaJ2zQdCzROZDivEVIDPkcQn8LMFSa8kj8Gxb/Lnwzv9A8VctNZ+lfivC/sV3ivW8ElJTERXZoPBRrZKkNKow==
  dependencies:
    ansi-regex "^3.0.0"

strip-ansi@^6.0.0, strip-ansi@^6.0.1:
  version "6.0.1"
  resolved "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz"
  integrity sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==
  dependencies:
    ansi-regex "^5.0.1"

strip-ansi@^7.0.1:
  version "7.1.0"
  resolved "https://registry.npmjs.org/strip-ansi/-/strip-ansi-7.1.0.tgz"
  integrity sha512-iq6eVVI64nQQTRYq2KtEg2d2uU7LElhTJwsH4YzIHZshxlgZms/wIc4VoDQTlG/IvVIrBKG06CrZnp0qv7hkcQ==
  dependencies:
    ansi-regex "^6.0.1"

strip-final-newline@^4.0.0:
  version "4.0.0"
  resolved "https://registry.npmjs.org/strip-final-newline/-/strip-final-newline-4.0.0.tgz"
  integrity sha512-aulFJcD6YK8V1G7iRB5tigAP4TsHBZZrOV8pjV++zdUwmeV8uzbY7yn6h9MswN62adStNZFuCIx4haBnRuMDaw==

strip-final-newline@2.0.0:
  version "2.0.0"
  resolved "https://registry.npmjs.org/strip-final-newline/-/strip-final-newline-2.0.0.tgz"
  integrity sha512-BrpvfNAE3dcvq7ll3xVumzjKjZQ5tI1sEUIKr3Uoks0XUl45St3FlatVqef9prk4jRDzhW6WZg+3bk93y6pLjA==

sucrase@^3.35.0:
  version "3.35.0"
  resolved "https://registry.npmjs.org/sucrase/-/sucrase-3.35.0.tgz"
  integrity sha512-8EbVDiu9iN/nESwxeSxDKe0dunta1GOlHufmSSXxMD2z2/tMZpDMpvXQGsc+ajGo8y2uYUmixaSRUc/QPoQ0GA==
  dependencies:
    "@jridgewell/gen-mapping" "^0.3.2"
    commander "^4.0.0"
    glob "^10.3.10"
    lines-and-columns "^1.1.6"
    mz "^2.7.0"
    pirates "^4.0.1"
    ts-interface-checker "^0.1.9"

superjson@^2.2.1:
  version "2.2.2"
  resolved "https://registry.npmjs.org/superjson/-/superjson-2.2.2.tgz"
  integrity sha512-5JRxVqC8I8NuOUjzBbvVJAKNM8qoVuH0O77h4WInc/qC2q5IreqKxYwgkga3PfA22OayK2ikceb/B26dztPl+Q==
  dependencies:
    copy-anything "^3.0.2"

supports-preserve-symlinks-flag@^1.0.0:
  version "1.0.0"
  resolved "https://registry.npmjs.org/supports-preserve-symlinks-flag/-/supports-preserve-symlinks-flag-1.0.0.tgz"
  integrity sha512-ot0WnXS9fgdkgIcePe6RHNk1WA8+muPa6cSjeR3V8K27q9BB1rTE3R1p7Hv0z1ZyAc8s6Vvv8DIyWf681MAt0w==

tailwindcss@^3.4.0:
  version "3.4.17"
  resolved "https://registry.npmjs.org/tailwindcss/-/tailwindcss-3.4.17.tgz"
  integrity sha512-w33E2aCvSDP0tW9RZuNXadXlkHXqFzSkQew/aIa2i/Sj8fThxwovwlXHSPXTbAHwEIhBFXAedUhP2tueAKP8Og==
  dependencies:
    "@alloc/quick-lru" "^5.2.0"
    arg "^5.0.2"
    chokidar "^3.6.0"
    didyoumean "^1.2.2"
    dlv "^1.1.3"
    fast-glob "^3.3.2"
    glob-parent "^6.0.2"
    is-glob "^4.0.3"
    jiti "^1.21.6"
    lilconfig "^3.1.3"
    micromatch "^4.0.8"
    normalize-path "^3.0.0"
    object-hash "^3.0.0"
    picocolors "^1.1.1"
    postcss "^8.4.47"
    postcss-import "^15.1.0"
    postcss-js "^4.0.1"
    postcss-load-config "^4.0.2"
    postcss-nested "^6.2.0"
    postcss-selector-parser "^6.1.2"
    resolve "^1.22.8"
    sucrase "^3.35.0"

thenify-all@^1.0.0:
  version "1.6.0"
  resolved "https://registry.npmjs.org/thenify-all/-/thenify-all-1.6.0.tgz"
  integrity sha512-RNxQH/qI8/t3thXJDwcstUO4zeqo64+Uy/+sNVRBx4Xn2OX+OZ9oP+iJnNFqplFra2ZUVeKCSa2oVWi3T4uVmA==
  dependencies:
    thenify ">= 3.1.0 < 4"

"thenify@>= 3.1.0 < 4":
  version "3.3.1"
  resolved "https://registry.npmjs.org/thenify/-/thenify-3.3.1.tgz"
  integrity sha512-RVZSIV5IG10Hk3enotrhvz0T9em6cyHBLkH/YAZuKqd8hRkKhSfCGIcP2KUY0EPxndzANBmNllzWPwak+bheSw==
  dependencies:
    any-promise "^1.0.0"

to-regex-range@^5.0.1:
  version "5.0.1"
  resolved "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz"
  integrity sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==
  dependencies:
    is-number "^7.0.0"

totalist@^3.0.0:
  version "3.0.1"
  resolved "https://registry.npmjs.org/totalist/-/totalist-3.0.1.tgz"
  integrity sha512-sf4i37nQ2LBx4m3wB74y+ubopq6W/dIzXg0FDGjsYnZHVa1Da8FH853wlL2gtUhg+xJXjfk3kUZS3BRoQeoQBQ==

ts-interface-checker@^0.1.9:
  version "0.1.13"
  resolved "https://registry.npmjs.org/ts-interface-checker/-/ts-interface-checker-0.1.13.tgz"
  integrity sha512-Y/arvbn+rrz3JCKl9C4kVNfTfSm2/mEp5FSz5EsZSANGPSlQrpRI5M4PKF+mJnE52jOO90PnPSc3Ur3bTQw0gA==

typescript@*, typescript@>=4.4.4, typescript@>=5.0.0, typescript@~5.8.0, typescript@5.x:
  version "5.8.2"
  resolved "https://registry.npmjs.org/typescript/-/typescript-5.8.2.tgz"
  integrity sha512-aJn6wq13/afZp/jT9QZmwEjDqqvSGp1VT5GVg+f/t6/oVyrgXM6BY1h9BRh/O5p3PlUPAe+WuiEZOmb/49RqoQ==

undici-types@~6.20.0:
  version "6.20.0"
  resolved "https://registry.npmjs.org/undici-types/-/undici-types-6.20.0.tgz"
  integrity sha512-Ny6QZ2Nju20vw1SRHe3d9jVu6gJ+4e3+MMpqu7pqE5HT6WsTSlce++GQmK5UXS8mzV8DSYHrQH+Xrf2jVcuKNg==

unicorn-magic@^0.3.0:
  version "0.3.0"
  resolved "https://registry.npmjs.org/unicorn-magic/-/unicorn-magic-0.3.0.tgz"
  integrity sha512-+QBBXBCvifc56fsbuxZQ6Sic3wqqc3WWaqxs58gvJrcOuN83HGTCwz3oS5phzU9LthRNE9VrJCFCLUgHeeFnfA==

universalify@^2.0.0:
  version "2.0.1"
  resolved "https://registry.npmjs.org/universalify/-/universalify-2.0.1.tgz"
  integrity sha512-gptHNQghINnc/vTGIk0SOFGFNXw7JVrlRUtConJRlvaw6DuX0wO5Jeko9sWrMBhh+PsYAZ7oXAiOnf/UKogyiw==

update-browserslist-db@^1.1.1:
  version "1.1.3"
  resolved "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.1.3.tgz"
  integrity sha512-UxhIZQ+QInVdunkDAaiazvvT/+fXL5Osr0JZlJulepYu6Jd7qJtDZjlur0emRlT71EN3ScPoE7gvsuIKKNavKw==
  dependencies:
    escalade "^3.2.0"
    picocolors "^1.1.1"

util-deprecate@^1.0.2:
  version "1.0.2"
  resolved "https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz"
  integrity sha512-EPD5q1uXyFxJpCrLnCc1nHnq3gOa6DZBocAIiI2TaSCA7VCJ1UJDMagCzIkXNsUYfD1daK//LTEQ8xiIbrHtcw==

vite-hot-client@^0.2.4:
  version "0.2.4"
  resolved "https://registry.npmjs.org/vite-hot-client/-/vite-hot-client-0.2.4.tgz"
  integrity sha512-a1nzURqO7DDmnXqabFOliz908FRmIppkBKsJthS8rbe8hBEXwEwe4C3Pp33Z1JoFCYfVL4kTOMLKk0ZZxREIeA==

vite-plugin-inspect@0.8.9:
  version "0.8.9"
  resolved "https://registry.npmjs.org/vite-plugin-inspect/-/vite-plugin-inspect-0.8.9.tgz"
  integrity sha512-22/8qn+LYonzibb1VeFZmISdVao5kC22jmEKm24vfFE8siEn47EpVcCLYMv6iKOYMJfjSvSJfueOwcFCkUnV3A==
  dependencies:
    "@antfu/utils" "^0.7.10"
    "@rollup/pluginutils" "^5.1.3"
    debug "^4.3.7"
    error-stack-parser-es "^0.1.5"
    fs-extra "^11.2.0"
    open "^10.1.0"
    perfect-debounce "^1.0.0"
    picocolors "^1.1.1"
    sirv "^3.0.0"

vite-plugin-vue-devtools@^7.7.2:
  version "7.7.2"
  resolved "https://registry.npmjs.org/vite-plugin-vue-devtools/-/vite-plugin-vue-devtools-7.7.2.tgz"
  integrity sha512-5V0UijQWiSBj32blkyPEqIbzc6HO9c1bwnBhx+ay2dzU0FakH+qMdNUT8nF9BvDE+i6I1U8CqCuJiO20vKEdQw==
  dependencies:
    "@vue/devtools-core" "^7.7.2"
    "@vue/devtools-kit" "^7.7.2"
    "@vue/devtools-shared" "^7.7.2"
    execa "^9.5.1"
    sirv "^3.0.0"
    vite-plugin-inspect "0.8.9"
    vite-plugin-vue-inspector "^5.3.1"

vite-plugin-vue-inspector@^5.3.1:
  version "5.3.1"
  resolved "https://registry.npmjs.org/vite-plugin-vue-inspector/-/vite-plugin-vue-inspector-5.3.1.tgz"
  integrity sha512-cBk172kZKTdvGpJuzCCLg8lJ909wopwsu3Ve9FsL1XsnLBiRT9U3MePcqrgGHgCX2ZgkqZmAGR8taxw+TV6s7A==
  dependencies:
    "@babel/core" "^7.23.0"
    "@babel/plugin-proposal-decorators" "^7.23.0"
    "@babel/plugin-syntax-import-attributes" "^7.22.5"
    "@babel/plugin-syntax-import-meta" "^7.10.4"
    "@babel/plugin-transform-typescript" "^7.22.15"
    "@vue/babel-plugin-jsx" "^1.1.5"
    "@vue/compiler-dom" "^3.3.4"
    kolorist "^1.8.0"
    magic-string "^0.30.4"

"vite@^2.6.0 || ^3.0.0 || ^4.0.0 || ^5.0.0-0 || ^6.0.0-0", "vite@^3.0.0-0 || ^4.0.0-0 || ^5.0.0-0 || ^6.0.0-0", "vite@^3.1.0 || ^4.0.0 || ^5.0.0-0 || ^6.0.1", "vite@^3.1.0 || ^4.0.0-0 || ^5.0.0-0 || ^6.0.0-0", "vite@^5.0.0 || ^6.0.0", vite@^6.2.1:
  version "6.2.4"
  resolved "https://registry.npmjs.org/vite/-/vite-6.2.4.tgz"
  integrity sha512-veHMSew8CcRzhL5o8ONjy8gkfmFJAd5Ac16oxBUjlwgX3Gq2Wqr+qNC3TjPIpy7TPV/KporLga5GT9HqdrCizw==
  dependencies:
    esbuild "^0.25.0"
    postcss "^8.5.3"
    rollup "^4.30.1"
  optionalDependencies:
    fsevents "~2.3.3"

vscode-uri@^3.0.8:
  version "3.1.0"
  resolved "https://registry.npmjs.org/vscode-uri/-/vscode-uri-3.1.0.tgz"
  integrity sha512-/BpdSx+yCQGnCvecbyXdxHDkuk55/G3xwnC0GqY4gmQ3j+A+g8kzzgB4Nk/SINjqn6+waqw3EgbVF2QKExkRxQ==

vue-tsc@^2.2.8:
  version "2.2.8"
  resolved "https://registry.npmjs.org/vue-tsc/-/vue-tsc-2.2.8.tgz"
  integrity sha512-jBYKBNFADTN+L+MdesNX/TB3XuDSyaWynKMDgR+yCSln0GQ9Tfb7JS2lr46s2LiFUT1WsmfWsSvIElyxzOPqcQ==
  dependencies:
    "@volar/typescript" "~2.4.11"
    "@vue/language-core" "2.2.8"

"vue@^2.7.0 || ^3.0.0", "vue@^2.7.0 || ^3.5.11", vue@^3.0.0, vue@^3.2.0, vue@^3.2.25, vue@^3.4.0, vue@^3.5.13, "vue@>= 3", vue@3.5.13:
  version "3.5.13"
  resolved "https://registry.npmjs.org/vue/-/vue-3.5.13.tgz"
  integrity sha512-wmeiSMxkZCSc+PM2w2VRsOYAZC8GdipNFRTsLSfodVqI9mbejKeXEGr8SckuLnrQPGe3oJN5c3K0vpoU9q/wCQ==
  dependencies:
    "@vue/compiler-dom" "3.5.13"
    "@vue/compiler-sfc" "3.5.13"
    "@vue/runtime-dom" "3.5.13"
    "@vue/server-renderer" "3.5.13"
    "@vue/shared" "3.5.13"

which-module@^2.0.0:
  version "2.0.1"
  resolved "https://registry.npmjs.org/which-module/-/which-module-2.0.1.tgz"
  integrity sha512-iBdZ57RDvnOR9AGBhML2vFZf7h8vmBjhoaZqODJBFWHVtKkDmKuHai3cx5PgVMrX5YDNp27AofYbAwctSS+vhQ==

which@^2.0.1:
  version "2.0.2"
  resolved "https://registry.npmjs.org/which/-/which-2.0.2.tgz"
  integrity sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==
  dependencies:
    isexe "^2.0.0"

which@^5.0.0:
  version "5.0.0"
  resolved "https://registry.npmjs.org/which/-/which-5.0.0.tgz"
  integrity sha512-JEdGzHwwkrbWoGOlIHqQ5gtprKGOenpDHpxE9zVR1bWbOtYRyPPHMe9FaP6x61CmNaTThSkb0DAJte5jD+DmzQ==
  dependencies:
    isexe "^3.1.1"

"wrap-ansi-cjs@npm:wrap-ansi@^7.0.0":
  version "7.0.0"
  resolved "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz"
  integrity sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==
  dependencies:
    ansi-styles "^4.0.0"
    string-width "^4.1.0"
    strip-ansi "^6.0.0"

wrap-ansi@^6.2.0:
  version "6.2.0"
  resolved "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-6.2.0.tgz"
  integrity sha512-r6lPcBGxZXlIcymEu7InxDMhdW0KDxpLgoFLcguasxCaJ/SOIZwINatK9KY/tf+ZrlywOKU0UDj3ATXUBfxJXA==
  dependencies:
    ansi-styles "^4.0.0"
    string-width "^4.1.0"
    strip-ansi "^6.0.0"

wrap-ansi@^8.1.0:
  version "8.1.0"
  resolved "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-8.1.0.tgz"
  integrity sha512-si7QWI6zUMq56bESFvagtmzMdGOtoxfR+Sez11Mobfc7tm+VkUckk9bW2UeffTGVUbOksxmSw0AA2gs8g71NCQ==
  dependencies:
    ansi-styles "^6.1.0"
    string-width "^5.0.1"
    strip-ansi "^7.0.1"

y18n@^4.0.0:
  version "4.0.3"
  resolved "https://registry.npmjs.org/y18n/-/y18n-4.0.3.tgz"
  integrity sha512-JKhqTOwSrqNA1NY5lSztJ1GrBiUodLMmIZuLiDaMRJ+itFd+ABVE8XBjOvIWL+rSqNDC74LCSFmlb/U4UZ4hJQ==

yallist@^3.0.2:
  version "3.1.1"
  resolved "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz"
  integrity sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==

yaml@^2.3.4, yaml@^2.4.2:
  version "2.7.1"
  resolved "https://registry.npmjs.org/yaml/-/yaml-2.7.1.tgz"
  integrity sha512-10ULxpnOCQXxJvBgxsn9ptjq6uviG/htZKk9veJGhlqn3w/DxQ631zFF+nlQXLwmImeS5amR2dl2U8sg6U9jsQ==

yargs-parser@^18.1.2:
  version "18.1.3"
  resolved "https://registry.npmjs.org/yargs-parser/-/yargs-parser-18.1.3.tgz"
  integrity sha512-o50j0JeToy/4K6OZcaQmW6lyXXKhq7csREXcDwk2omFPJEwUNOVtJKvmDr9EI1fAJZUyZcRF7kxGBWmRXudrCQ==
  dependencies:
    camelcase "^5.0.0"
    decamelize "^1.2.0"

yargs@15.4.1:
  version "15.4.1"
  resolved "https://registry.npmjs.org/yargs/-/yargs-15.4.1.tgz"
  integrity sha512-aePbxDmcYW++PaqBsJ+HYUFwCdv4LVvdnhBy78E57PIor8/OVvhMrADFFEDh8DHDFRv/O9i3lPhsENjO7QX0+A==
  dependencies:
    cliui "^6.0.0"
    decamelize "^1.2.0"
    find-up "^4.1.0"
    get-caller-file "^2.0.1"
    require-directory "^2.1.1"
    require-main-filename "^2.0.0"
    set-blocking "^2.0.0"
    string-width "^4.2.0"
    which-module "^2.0.0"
    y18n "^4.0.0"
    yargs-parser "^18.1.2"

yoctocolors@^2.0.0:
  version "2.1.1"
  resolved "https://registry.npmjs.org/yoctocolors/-/yoctocolors-2.1.1.tgz"
  integrity sha512-GQHQqAopRhwU8Kt1DDM8NjibDXHC8eoh1erhGAJPEyveY9qqVeXvVikNKrDz69sHowPMorbPUrH/mx8c50eiBQ==

</file>

<file path='requirements.txt'>
# Web Framework
fastapi==0.103.1
uvicorn==0.23.2
pydantic==2.8.1  # <-- UPGRADED from 2.3.0 to match pydantic-settings requirement
python-multipart==0.0.6
email-validator==2.0.0

# Database
sqlalchemy==2.0.20
alembic==1.12.0
psycopg2-binary==2.9.7

# Task Queue
celery==5.3.4
redis==4.6.0

# File Processing
numpy==1.24.4  # Pinning NumPy version that's compatible with pandas 2.1.0
pandas==2.1.0
openpyxl==3.1.2
xlsxwriter==3.1.2

# API Clients
openai==0.28.0
httpx==0.24.1

# Security
python-jose==3.3.0
passlib==1.7.4
bcrypt==4.0.1

# Testing
pytest==7.4.2
pytest-asyncio==0.21.1

# Utilities
python-dotenv==1.0.0
tenacity==8.2.3
loguru==0.7.0
pydantic-settings==2.8.1 # Keep this newer version
</file>

<file path='run_local_V2.sh'>
#!/usr/bin/env bash

# ./run_local_V2.sh [PORT]
# Run this script to setup and initialize the local development environment
# for the NAICS vendor classification system.
# WARNING: THIS VERSION *REMOVES* THE DATABASE VOLUME ON EACH RUN,
#          CLEARING ALL PREVIOUS DATA.
# Optional parameter:
#   PORT - The host port to use (default: 8001)

set -e

echo "Starting vendor classification setup (DATABASE WILL BE RESET)..." # Updated message

# Check if a port was provided as an argument, otherwise use default
WEB_PORT=${1:-8001}
echo "Using host port $WEB_PORT for the web service"

# ----- DOCKER CLEANUP SECTION (MODIFIED TO REMOVE DB VOLUME) -----
echo "Cleaning up Docker resources (Containers, Networks, and DB Volume)..."
# Use docker compose command based on version (v1 or v2+)
if docker compose version >/dev/null 2>&1; then
    COMPOSE_CMD="docker compose"
    NETWORK_NAME="vendor_classification_app-network" # Default network name for v2+
    VOLUME_NAME="vendor_classification_postgres_data" # Default volume name for v2+
elif docker-compose version >/dev/null 2>&1; then
    COMPOSE_CMD="docker-compose"
    PROJECT_NAME=$(basename "$PWD" | sed 's/[^a-zA-Z0-9]//g') # Simple project name from dir
    NETWORK_NAME="${PROJECT_NAME}_app-network" # Common pattern for v1
    VOLUME_NAME="${PROJECT_NAME}_postgres_data" # Common pattern for v1
else
    echo "ERROR: Neither 'docker compose' (v2+) nor 'docker-compose' (v1) found. Please install Docker Compose."
    exit 1
fi
echo "Using compose command: '$COMPOSE_CMD'"
echo "Database volume name expected: '$VOLUME_NAME' (will be REMOVED)"

# Bring down containers and networks, AND remove volumes (-v flag ADDED)
$COMPOSE_CMD down --remove-orphans -v || echo "Warning: docker compose down failed, continuing cleanup..."

# Force remove network if it persists (compose down should handle this, but just in case)
if docker network inspect $NETWORK_NAME >/dev/null 2>&1; then
    echo "Network '$NETWORK_NAME' still exists, attempting force removal..."
    docker network rm -f $NETWORK_NAME || echo "Warning: Failed to force remove network '$NETWORK_NAME'"
else
    echo "Network '$NETWORK_NAME' does not exist or was removed."
fi

# Explicitly remove the database volume (compose down -v should handle this, but belt-and-suspenders)
echo "Attempting explicit removal of volume '$VOLUME_NAME'..."
docker volume rm -f $VOLUME_NAME || echo "Warning: Failed to remove volume '$VOLUME_NAME' or it didn't exist."
echo "Docker container/network/volume cleanup attempt finished."
# ----- END DOCKER CLEANUP -----

# Create necessary directories
echo "Creating data directories..."
mkdir -p data/input data/output data/taxonomy data/logs # Ensures structure exists

# --- ADDED: Clear logs directory ---
echo "Clearing previous contents of data/logs/ ..."
# Remove the directory and its contents, then recreate it to ensure it's empty
# Using rm -rf followed by mkdir -p is robust
rm -rf data/logs && mkdir -p data/logs || { echo "ERROR: Failed to clear and recreate data/logs directory. Check permissions."; exit 1; }
# --- END: Clear logs directory ---

# Set permissions for log directory (needs to happen AFTER recreation)
echo "Setting permissions for log directory..."
chmod -R 777 data/logs || echo "Warning: Could not set permissions on data/logs. Logging might fail if user IDs mismatch."

# Export the port as an environment variable
export WEB_PORT

# --- ADDED LOGGING ---
echo "Checking for frontend build files locally before Docker build:"
ls -l frontend/vue_frontend/package.json || { echo "ERROR: frontend/vue_frontend/package.json not found locally! Cannot build frontend."; exit 1; }
ls -l frontend/vue_frontend/vite.config.js || ls -l frontend/vue_frontend/vite.config.ts || { echo "WARNING: vite.config file not found locally! Frontend build might fail."; }
# --- END ADDED LOGGING ---

echo "Building Docker images (this will include the Vue frontend build)..."
# Use the detected compose command
$COMPOSE_CMD build --no-cache web # Rebuild web to ensure latest code
$COMPOSE_CMD build worker db redis

# Check if build was successful
if [ $? -ne 0 ]; then
    echo "Docker build failed. Please check the error messages above."
    exit 1
fi
echo "Docker build completed successfully."

echo "Starting containers in the background..."
# Use the detected compose command
$COMPOSE_CMD up -d

# Check if containers started successfully
if [ $? -ne 0 ]; then
  echo "There was an error starting the containers. Please check the Docker logs."
  exit 1
fi

WAIT_SECONDS=15 # Keep wait time as DB init might take a moment
echo "Waiting $WAIT_SECONDS seconds for containers to start and DB to initialize..."
sleep $WAIT_SECONDS

echo "===> Checking container statuses:"
$COMPOSE_CMD ps

# Check if web container is running
WEB_CONTAINER_ID=$($COMPOSE_CMD ps -q web)
if [ -z "$WEB_CONTAINER_ID" ]; then
    echo "Web container failed to start! Check logs with: $COMPOSE_CMD logs web"
    exit 1
else
    echo "Web container ($WEB_CONTAINER_ID) appears to be running."
fi

echo "===> Checking web container logs (last 30 lines):"
$COMPOSE_CMD logs web --tail 30

echo "===> Testing web service connectivity (Health Check):"
retry_count=0
max_retries=5
until curl -f -s -o /dev/null "http://localhost:$WEB_PORT/health"; do
    retry_count=$((retry_count+1))
    if [ $retry_count -ge $max_retries ]; then
        echo "Warning: Could not connect to web service health endpoint (http://localhost:$WEB_PORT/health) after $max_retries attempts! Check logs."
        break
    fi
    echo "Health check failed, retrying in 5 seconds... ($retry_count/$max_retries)"
    sleep 5
done
if [ $retry_count -lt $max_retries ]; then
    echo "Health check successful!"
fi


echo ""
echo "===> Setup completed (Database RESET, Logs Cleared)." # Updated message
echo "Access the web interface (built Vue app) at: http://localhost:$WEB_PORT"
echo "Login with username: admin, password: password"
echo "PostgreSQL is available on host port 5433"
echo "Database data in volume '$VOLUME_NAME' was REMOVED and recreated." # Updated message
echo "Log directory 'data/logs' was cleared before this run."
echo ""
echo "*** Frontend Development Note ***"
echo "The frontend served by this container is the *built* version."
echo "For frontend development, run the Vue dev server separately:"
echo "  cd frontend/vue_frontend"
echo "  npm install  # If needed"
echo "  npm run dev"
echo "Then access the dev server (usually http://localhost:5173 or similar)."
echo "The dev server should proxy API requests to http://localhost:$WEB_PORT (configure in frontend/vue_frontend/vite.config.js/ts if needed)."
echo "*******************************"
echo ""
echo "Press Enter to show continuous logs, or Ctrl+C to exit."
read -r
$COMPOSE_CMD logs -f
</file>

</Project Source Code>

--- Identified Files (Provide comma-separated list below) ---


